I have an absolutely positioned div containing several children, one of which is a relatively positioned div. When I use a percentage-based width on the child div, it collapses to '0' width on Internet&nbsp;Explorer&nbsp;7, but not on Firefox or Safari.

If I use pixel width, it works. If the parent is relatively positioned, the percentage width on the child works.


Is there something I'm missing here?
Is there an easy fix for this besides the pixel-based width on the
child?
Is there an area of the CSS specification that covers this?



How do you expose a LINQ query as an ASMX web service? Usually, from the business tier, I can return a typed DataSet or DataTable which can be serialized for transport over ASMX.

How can I do the same for a LINQ query? Is there a way to populate a typed DataSet or DataTable via a LINQ query?: 

public static MyDataTable CallMySproc()    
{    
    string conn = ...;

    MyDatabaseDataContext db = new MyDatabaseDataContext(conn);    
    MyDataTable dt = new MyDataTable();

    // execute a sproc via LINQ
    var query = from dr in db.MySproc().AsEnumerable
    select dr;

    // copy LINQ query resultset into a DataTable -this does not work !    
    dt = query.CopyToDataTable();

    return dt;
}


How can I get the resultset of a LINQ query into a DataSet or DataTable? Alternatively, is the LINQ query serializeable so that I can expose it as an ASMX web service?


If I have a trigger before the update on a table, how can I throw an error that prevents the update on that table?


Let's say I have a DataTable with a Name column. I want to have a collection of the unique names ordered alphabetically. The following query ignores the order by clause.

var names =
    (from DataRow dr in dataTable.Rows
    orderby (string)dr["Name"]
    select (string)dr["Name"]).Distinct();


Why does the orderby not get enforced?


How do you page through a collection in LINQ given that you have a startIndex and a count?


The version of Subclipse (1.2.4) currently available through Aptana's automatic Plugins Manager does not work with the newest version of Subversion.

I see on the Subclipse website however that they have 1.4.2 out for Eclipse. So I added a new remote update site to my Update manager. When I tried to install it, it told me I needed Mylyn 3.0.0. So after much searching I found Mylyn 3.0.0 and added another new remote update site to my update manager. Then when I tried to install that, it told me I needed org.eclipse.ui 3.3.0 or equivalent.

Looking at the configuration details for Aptana, it looks like it is built against eclipse 3.2.2.

Does anyone know if there is a way to upgrade the version of Eclipse Aptana that is built against to 3.3.0? Or if there is some other way to get Subclipse to work with the very newest version of Subversion?

I know this isn't necessarily a "programming" question, but I hope it's ok since it's highly relevant to the programming experience.


I've been banging my head against SQL Server 2005 trying to get a lot of data out.  I've been given a database with nearly 300 tables in it and I need to turn this into a MySQL database.  My first call was to use bcp but unfortunately it doesn't produce valid CSV - strings aren't encapsulated, so you can't deal with any row that has a string with a comma in it (or whatever you use as a delimiter) and I would still have to hand write all of the create table statements, as obviously CSV doesn't tell you anything about the data types.

What would be better is if there was some tool that could connect to both SQL Server and MySQL, then do a copy. You lose views, stored procedures, trigger, etc, but it isn't hard to copy a table that only uses base types from one DB to another... is it?

Does anybody know of such a tool?  I don't mind how many assumptions it makes or what simplifications occur, as long as it supports integer, float, datetime and string. I have to do a lot of pruning, normalising, etc. anyway so I don't care about keeping keys, relationships or anything like that, but I need the initial set of data in fast!


Stack Overflow has a subversion version number at the bottom:


  svn revision: 679


I want to use such automatic versioning with my .NET Web Site/Application, Windows Forms, WPD projects/solutions.

How do I implement this?


I want to print HTML from a C# web service.  The Web Browser control is overkill, and does not function well in a service environment, nor does it function well on a system with very tight security constraints.  Is there any sort of free .NET library that will support the printing of a basic HTML page?  Here is the code I have so far, which does not run properly.

public void PrintThing(string document)
{
    if (Thread.CurrentThread.GetApartmentState() != ApartmentState.STA)
    {
        Thread thread =
            new Thread((ThreadStart) delegate { PrintDocument(document); });
        thread.SetApartmentState(ApartmentState.STA);
        thread.Start();
    }
    else
    {
        PrintDocument(document);
    }
}

protected void PrintDocument(string document)
{
    WebBrowser browser = new WebBrowser();
    browser.DocumentText = document;
    while (browser.ReadyState != WebBrowserReadyState.Complete)
    {
        Application.DoEvents();
    }
    browser.Print();
}


This works fine when called from UI-type threads, but nothing happens when called from a service-type thread.  Changing Print() to ShowPrintPreviewDialog() yields the following IE script error:


  Error: 'dialogArguments.___IE_PrintType' is null or not an object
  URL: res://ieframe.dll/preview.dlg


And a small empty print preview dialog appears.


One of the fun parts of multi-cultural programming is number formats.


Americans use 10,000.50
Germans use 10.000,50
French use 10 000,50


My first approach would be to take the string, parse it backwards, until I encounter a separator and use this as my decimal separator. There is an obvious flaw with that: 10.000 would be interpreted as 10.

Another approach: if the string contains 2 different non-numeric characters, use the last one as the decimal separator and discard the others. If I only have one, check if it occurs more than once and discard it if it does. If it only appears once, check if it has 3 digits after it. If yes, discard it, otherwise use it as decimal separator.

The obvious "best solution" would be to detect the User's culture or Browser, but that does not work if you have a Frenchman using an en-US Windows/Browser.

Does the .net Framework contain some mythical black magic floating point parser that is better than Double.(Try)Parse() in trying to auto-detect the number format?


Yes, I know.  The existence of a running copy of SQL Server 6.5 in 2008 is absurd.

That stipulated, what is the best way to migrate from 6.5 to 2005?  Is there any direct path?  Most of the documentation I've found deals with upgrading 6.5 to 7.

Should I forget about the native SQL Server upgrade utilities, script out all of the objects and data, and try to recreate from scratch?

I was going to attempt the upgrade this weekend, but server issues pushed it back till next.  So, any ideas would be welcomed during the course of the week.

Update.  This is how I ended up doing it:


Back up the database in question and Master on 6.5.
Execute SQL Server 2000's instcat.sql against 6.5's Master.  This allows SQL Server 2000's OLEDB provider to connect to 6.5.
Use SQL Server 2000's standalone "Import and Export Data" to create a DTS package, using OLEDB to connect to 6.5.  This successfully copied all 6.5's tables to a new 2005 database (also using OLEDB).
Use 6.5's Enterprise Manager to script out all of the database's indexes and triggers to a .sql file.
Execute that .sql file against the new copy of the database, in 2005's Management Studio.
Use 6.5's Enterprise Manager to script out all of the stored procedures.
Execute that .sql file against the 2005 database.  Several dozen sprocs had issues making them incompatible with 2005.  Mainly non-ANSI joins and quoted identifier issues.
Corrected all of those issues and re-executed the .sql file.
Recreated the 6.5's logins in 2005 and gave them appropriate permissions.


There was a bit of rinse/repeat when correcting the stored procedures (there were hundreds of them to correct), but the upgrade went great otherwise.

Being able to use Management Studio instead of Query Analyzer and Enterprise Manager 6.5 is such an amazing difference.  A few report queries that took 20-30 seconds on the 6.5 database are now running in 1-2 seconds, without any modification, new indexes, or anything.  I didn't expect that kind of immediate improvement.


MySQL has this incredibly useful yet properitary REPLACE INTO SQL Command. 

I wonder: Can this easily be emulated in SQL Server 2005?

Starting a new Transaction, doing a Select() and then either UPDATE or INSERT and COMMIT is always a little bit of a pain, especially when doing it in the application and therefore always keeping 2 versions of the statement.

I wonder if there is an easy and universal way to implement such a function into SQL Server 2005?



What's the optimal level of concurrency that the C++ implementation of BerkeleyDB can reasonably support?
How many threads can I have hammering away at the DB before throughput starts to suffer because of resource contention?


I've read the manual and know how to set the number of locks, lockers, database page size, etc., but I'd just like some advice from someone who has real-world experience with BDB concurrency.

My application is pretty simple, I'll be doing gets and puts of records that are about 1KB each. No cursors, no deleting.


What are the best practices for checking in BIN directories in a collaborative development environment using SVN?  Should project level references be excluded from checkin?  Is it easier to just add all bin directories?

I develop a lot of DotNetNuke sites and it seems that in a multi-developer environment, it's always a huge task to get the environment setup correctly.

The ultimate goal (of course) is to have a new developer checkout the trunk from SVN, restore the DNN database and have it all just 'work'...


When is it appropriate to use an unsigned variable over a signed one? What about in a for loop?

I hear a lot of opinions about this and I wanted to see if there was anything resembling a consensus. 

for (unsigned int i = 0; i &lt; someThing.length(); i++) {  
    SomeThing var = someThing.at(i);  
    // You get the idea.  
}


I know Java doesn't have unsigned values, and that must have been a concious decision on Sun Microsystems' part. 


I am using the Photoshop's javascript API to find the fonts in a given PSD.

Given a font name returned by the API, I want to find the actual physical font file that that font name corresponds to on the disc.

This is all happening in a python program running on OSX so I guess I'm looking for one of:


Some Photoshop javascript
A Python function
An OSX API that I can call from python



I have a Ruby on Rails Website that makes HTTP calls to an external Web Service.

About once a day I get a SystemExit (stacktrace below) error email where a call to the service has failed.  If I then try the exact same query on my site moments later it works fine.
It's been happening since the site went live and I've had no luck tracking down what causes it.

Ruby is version 1.8.6 and rails is version 1.2.6.

Anyone else have this problem?

This is the error and stacktrace.


A SystemExit occurred
/usr/local/lib/ruby/gems/1.8/gems/rails-1.2.6/lib/fcgi_handler.rb:116:in `exit'
/usr/local/lib/ruby/gems/1.8/gems/rails-1.2.6/lib/fcgi_handler.rb:116:in `exit_now_handler'
/usr/local/lib/ruby/gems/1.8/gems/activesupport-1.4.4/lib/active_support/inflector.rb:250:in `to_proc'
/usr/local/lib/ruby/1.8/net/protocol.rb:133:in `call'
/usr/local/lib/ruby/1.8/net/protocol.rb:133:in `sysread'
/usr/local/lib/ruby/1.8/net/protocol.rb:133:in `rbuf_fill'
/usr/local/lib/ruby/1.8/timeout.rb:56:in `timeout'
/usr/local/lib/ruby/1.8/timeout.rb:76:in `timeout'
/usr/local/lib/ruby/1.8/net/protocol.rb:132:in `rbuf_fill'
/usr/local/lib/ruby/1.8/net/protocol.rb:116:in `readuntil'
/usr/local/lib/ruby/1.8/net/protocol.rb:126:in `readline'
/usr/local/lib/ruby/1.8/net/http.rb:2017:in `read_status_line'
/usr/local/lib/ruby/1.8/net/http.rb:2006:in `read_new'
/usr/local/lib/ruby/1.8/net/http.rb:1047:in `request'
/usr/local/lib/ruby/1.8/net/http.rb:945:in `request_get'
/usr/local/lib/ruby/1.8/net/http.rb:380:in `get_response'
/usr/local/lib/ruby/1.8/net/http.rb:543:in `start'
/usr/local/lib/ruby/1.8/net/http.rb:379:in `get_response'



I'm starting work on a hobby project with a python codebase and would like to set up some form of continuous integration (i.e. running a battery of test-cases each time a check-in is made and sending nag e-mails to responsible persons when the tests fail) similar to CruiseControl or TeamCity.

I realize I could do this with hooks in most VCSes, but that requires that the tests run on the same machine as the version control server, which isn't as elegant as I would like. Does anyone have any suggestions for a small, user-friendly, open-source continuous integration system suitable for a Python codebase?


I would like to test a function with a tuple from a set of fringe cases and normal values. For example, while testing a function which returns true whenever given three lengths that form a valid triangle, I would have specific cases, negative / small / large numbers, values close-to being overflowed, etc.; what is more, main aim is to generate combinations of these values, with or without repetition, in order to get a set of test data.

(inf,0,-1), (5,10,1000), (10,5,5), (0,-1,5), (1000,inf,inf),
...


As a note: I actually know the answer to this, but it might be helpful for others, and a challenge for people here! --will post my answer later on.


I'm looking for a performant, reasonably robust RNG using no special hardware. It can use mathematical methods (Mersenne Twister, etc), it can "collect entropy" from the machine, whatever. On Linux/etc we have a drand48() which generates 48 random bits. I'd like a similar function/class for C++ or C# which can generate more than 32 bits of randomness and which low-order bits are equally as random as high-order bits.

It doesn't have to be cryptographically secure but it must not use or be based on the C-language rand() or .NET System.Random.

Any source code, links to source, etc. would be appreciated! Failing that, what TYPE of RNG should I be looking for?


An MFC application that I'm trying to migrate uses afxext.h, which causes _AFXDLL to get set, which causes this error if I set /MT:


  Please use the /MD switch for _AFXDLL builds


My research to date indicates that it is impossible to build an application for execution on Windows NT 4.0 using Visual Studio (C++, in this case) 2005.

Is this really true? Are there any workaround available?


I would like the version property of my application to be incremented for each build but I'm not sure on how to enable this functionality in Visual Studio (2005/2008). I have tried to specify the AssemblyVersion as 1.0.* but it doesn't get me exactly what I want. 

I'm also using a settings file and in earlier attempts when the assembly version changed my settings got reset to the default since the application looked for the settings file in another directory. 

I would like to be able to display a version number in the form of 1.1.38 so when a user finds a problem I can log the version they are using as well as tell them to upgrade if they have an old release.

A short explanation of how the versioning works would also be appreciated. When does the build and revision number get incremented?

What is the fastest, yet secure way to encrypt passwords in (PHP preferred), and for which ever method you choose is it portable?

In other words if I later migrate my website to a different server will my passwords continue to work?

The method I am using now as I was told is dependent on the exact versions of the libraries installed on the server.


What is the best way to localise a date format descriptor?

As anyone from a culture which does not use the mm/dd/yyyy format knows, it is annoying to have to enter dates in this format. The .NET framework provides some very good localisation support, so it's trivial to parse dates according to the users culture, but you often want to also display a helpful hint as to the format required (especially to distinguish between yy and yyyy which is interchangeable in most cultures).

What is the best way to do this in a way that make sense to most users (e.g. dd/M/yyy is confusing because of the change in case and the switching between one and two letters).


I can get Python to work with Postgresql but I cannot get it to work with MySQL. The main problem is that on the shared hosting account I have I do not have the ability to install things such as Django or PySQL, I generally fail when installing them on my computer so maybe it's good I can't install on the host.

I found bpgsql really good because it does not require an install, it's a single file that I can look at, read and then call the functions of. Does anybody know of something like this for MySQL?

This is ASP classic, not .Net.  We have to get a way to SFTP into a server to upload and download a couple of files, kicked off by a user.

What have other people used to do SFTP in ASP classic?  Not necessarily opposed to purchasing a control.


I'm trying to maintain a Setup Project in Visual Studio 2003 (yes, it's a legacy application). The problem we have at the moment is that we need to write registry entries to HKCU for every user on the computer. They need to be in the HKCU rather than HKLM because they are the default user settings, and they do change per user. My feeling is that


This isn't possible
This isn't something the installer should be doing, but something the application should be doing (after all what happens when a user profile is created after the install?).


With that in mind, I still want to change as little as possible in the application, so my question is, is it possible to add registry entries for every user in a Visual Studio 2003 setup project? 

And, at the moment the project lists five registry root keys (HKEY_CLASSES_ROOT, HKEY_CURRENT_USER, HKEY_LOCAL_MACHINE, HKEY_USERS, and User/Machine Hive). I don't really know anything about the Users root key, and haven't seen User/Machine Hive. Can anyone enlighten me on what they are? Perhaps they could solve my problem above.


SQL:

SELECT
   u.id,
   u.name,
   isnull(MAX(h.dateCol), '1900-01-01') dateColWithDefault
FROM universe u
LEFT JOIN history h 
   ON u.id=h.id 
   AND h.dateCol&lt;GETDATE()-1
GROUP BY u.Id, u.name



Is there an easy way to produce MSDN-style documentation from the Visual Studio XML output?
I'm not patient enough to set up a good xslt for it because I know I'm not the first person to cross this bridge.  

Also, I tried setting up sandcastle recently, but it really made my eyes cross.  Either I was missing something important in the process or it is just way too involved.

I know somebody out there has a really nice dead-simple solution.

I'm reiterating here because I think my formatting made that paragraph non-inviting to read:


  I gave sandcastle a try but had a really hard time getting it set up.
  What I really have in mind is something much simpler.


That is, unless I just don't understand the sandcastle process.  It seemed like an awful lot of extra baggage to me just to produce something nice for the testers to work with.


What's the simplest way to connect and query a database for a set of records in C#?


Attempting to insert an escape character into a table results in a warning. 

For example:

create table EscapeTest (text varchar(50));

insert into EscapeTest (text) values ('This is the first part \n And this is the second');


Produces the warning:

WARNING:  nonstandard use of escape in a string literal


(Using PSQL 8.2)

Anyone know how to get around this?


I'm maintaining a .NET 1.1 application, and one of the things I've been tasked with is making sure the user doesn't see any unfriendly error notifications.

I've added handlers to Application.ThreadException and AppDomain.CurrentDomain.UnhandledException, which do get called. My problem is that the standard CLR error dialog is still displayed (before the exception handler is called).

Jeff talks about this problem on his blog here and here. But there's no solution. So what is the standard way in .NET 1.1 to handle uncaught exceptions and display a friendly dialog box?

Edit: Jeff's response was marked as the correct answer, because the link he provided has the most complete information on how to do what's required.


I'm looking for a way to delete a file which is locked by another process using C#. I suspect the method must be able to find which process is locking the file (perhaps by tracking the handles, although I'm not sure how to do this in C#) then close that process before being able to complete the file delete using File.Delete().


What is the correct way to get the process size on Solaris, HP-UX and AIX? Should we use top or ps -o vsz or something else?


I was wondering if there is any good and clean oo implementation of bayesian filtering for spam and text classification? For learning purposes.

Exceptions in C++ don't need to be caught (no compile time errors) by the calling function. So it's up to developer's judgment whether to catch it using try/catch (unlike in Java). 

Is there a way one can ensure that the exceptions thrown are always caught using try/catch by the calling function?


Given that indexing is so important as your data set increases in size, can someone explain how does indexing works at a database agnostic level?

For information on queries to index a field, check out How do I index a database column


Hopefully, I can get answers for each database server.

For an outline of how indexing works check out: http://stackoverflow.com/questions/1108/how-does-database-indexing-work

I need to be able to manipulate a large (10^7 nodes) graph in python. The data corresponding to each node/edge is minimal, say, a small number of strings. What is the most efficient, in terms of memory and speed, way of doing this? 

A dict of dicts is more flexible and simpler to implement, but I intuitively expect a list of lists to be faster. The list option would also require that I keep the data separate from the structure, while dicts would allow for something of the sort:

graph[I][J]["Property"]="value"


What would you suggest?



Yes, I should have been a bit clearer on what I mean by efficiency. In this particular case I mean it in terms of random access retrieval.

Loading the data in to memory isn't a huge problem. That's done once and for all. The time consuming part is visiting the nodes so I can extract the information and measure the metrics I'm interested in.

I hadn't considered making each node a class (properties are the same for all nodes) but it seems like that would add an extra layer of overhead? I was hoping someone would have some direct experience with a similar case that they could share. After all, graphs are one of the most common abstractions in CS.


I have just recently started to study Ruby, and in lieu of Jeff's advice over the weekend...


Stop theorizing.
Write lots of software.
Learn from your mistakes. 


...I was interested in honing my skills while helping out the Open Source Community the process so I thought I'd ask if anyone have any suggestions for cool/interesting Open Source Projects written in Ruby that you know of or are involved in.

I am looking to allow users to control of subdomain of an app I am toying with, much like Basecamp where it is customusername.seework.com.

What is required on the DNS end to allow these to be created dynamically and be available instantly. 

And how do you recommend dealing with this in the logic of the site? Htaccess rule to  lookup the subdomain in the DB?


I have a client-server app where the client is on a Windows Mobile 6 device, written in C++ and the server is on full Windows and written in C#. 

Originally, I only needed it to send messages from the client to the server, with the server only ever sending back an acknowledgment that it received the message.  Now, I would like to update it so that the server can actually send a message to the client to request data.  As I currently have it set up so the client is only in receive mode after it sends data to the server, this doesn't allow for the server to send a request at any time.  I would have to wait for client data.  My first thought would be to create another thread on the client with a separate open socket, listening for server requests...just like the server already has in respect the the client.  Is there a way, within the same thread and using the same socket, to all the server to send requests at any time?

Can you use something to the affect of WaitForMultipleObjects() and pass it a receive buffer and an event that tells it there is data to be sent?


I have a Queue&lt;T&gt; object that I have initialised to a capacity of 2, but obviously that is just the capacity and it keeps expanding as I add items.  Is there already an object that automatically dequeues an item when the limit is reached, or is the best solution to create my own inherited class?

I am using MSBuild to build my stuff. I want to use CruiseControl.net as by Build Server.

Now, CCNET refers nAnt a lot, but it looks as if ccnet can do most of the stuff nant could do through the project configuration and msbuild. Also, nAnt seems a bit unsupported, with a Beta release that is almost a year old now.

In short: I am actually quite happy with MSBuild (especially since it's the "official" compiler front end) and a bit uncomfortable with nAnt, but I do not want to judge prematurely.

What would be reasons to use nAnt over MSBuild? Especially with ccnet, which seems to overlap a bit with nant in terms of features (and adding the automated build related stuff)

I'd like to create a progress bar to indicate the status of an a batch job in Ruby. 

I've read some tutorials / libraries on using (n)curses, none of which were particularly helpful in explaining how to create an "animated" progress bar in the terminal or using curses with Ruby. 

I'm already aware of using a separate thread to monitor the progress of a given job, I'm just not sure how to proceed with drawing a progress bar. 



Update  

ProgressBar class was incredibly straight-forward, perfectly solved my problem.


I need the name of the current logged in user in my Air/Flex application. The application will only be deployed on Windows machines. I think I could attain this by regexing the User directory, but am open to other ways.

This is an open-ended question.  What approaches should I consider?

I'm setting up a dedicated SQL Server 2005 box on Windows Server 2008 this week, and would like to pare it down to be as barebones as possible while still being fully functional.

To that end, the "Server Core" option sounds appealing, but I'm not clear about whether or not I can run SQL Server on that SKU.  Several services are addressed on the Microsoft website, but I don't see any indication about SQL Server.

Does anyone know definitively?


I have a custom validation function in JavaScript in a user control on a .Net 2.0 web site which checks to see that the fee paid is not in excess of the fee amount due. 

I've placed the validator code in the ascx file, and I have also tried using Page.ClientScript.RegisterClientScriptBlock() and in both cases the validation fires, but cannot find the JavaScript function.

The output in Firefox's error console is "feeAmountCheck is not defined". Here is the function (this was taken directly from firefox->view source)

&lt;script type="text/javascript"&gt;
    function feeAmountCheck(source, arguments)
    {
        var amountDue = document.getElementById('ctl00_footerContentHolder_Fees1_FeeDue');
        var amountPaid = document.getElementById('ctl00_footerContentHolder_Fees1_FeePaid');

        if (amountDue.value &gt; 0 &amp;&amp; amountDue &gt;= amountPaid)
        {
            arguments.IsValid = true;
        }
        else
        {
            arguments.IsValid = false;
        }

        return arguments;
    }
&lt;/script&gt;


Any ideas as to why the function isn't being found? How can I remedy this without having to add the function to my master page or consuming page?


Is it possible to configure xampp to serve up a file outside of the htdocs directory?

For instance, say I have a file located as follows:

C:\projects\transitCalculator\trunk\TransitCalculator.php

and my xampp files are normally served out from:

C:\xampp\htdocs\

(because that's the default configuration) Is there some way to make Apache recognize and serve up my TransitCalculator.php file without moving it under htdocs? Preferably I'd like Apache to serve up/have access to the entire contents of the projects directory, and I don't want to move the projects directory under htdocs.

edit: edited to add Apache to the question title to make Q/A more "searchable"


I often encounter the following scenario where I need to offer many different types of permissions. I primarily use ASP.NET / VB.NET with SQL Server 2000.

Scenario

I want to offer a dynamic permission system that can work on different parameters. Let's say that I want to give either a department or just a specific person access to an application. And pretend that we have a number of applications that keeps growing.

In the past, I have chosen one of the following two ways that I know to do this.

1) Use a single permission table with special columns that are used for determining a how to apply the parameters. The special columns in this example are TypeID and TypeAuxID. The SQL would look something like this.

SELECT COUNT(PermissionID)
FROM application_permissions
WHERE
(TypeID = 1 AND TypeAuxID = @UserID) OR
(TypeID = 2 AND TypeAuxID = @DepartmentID)
AND ApplicationID = 1


2) Use a mapping table for each type of permission, then joining them all together.

SELECT COUNT(perm.PermissionID)
FROM application_permissions perm
LEFT JOIN application_UserPermissions emp
ON perm.ApplicationID = emp.ApplicationID
LEFT JOIN application_DepartmentPermissions dept
ON perm.ApplicationID = dept.ApplicationID
WHERE q.SectionID=@SectionID
  AND (emp.UserID=@UserID OR dept.DeptID=@DeptID OR
 (emp.UserID IS NULL AND dept.DeptID IS NULL)) AND ApplicationID = 1
ORDER BY q.QID ASC


My Thoughts

I hope that the examples make sense. I cobbled them together.

The first example requires less work, but neither of them feel like the best answer. Is there a better way to handle this?


I'm trying to do this (which produces an unexpected T_VARIABLE error):

public function createShipment($startZip, $endZip, $weight = $this-&gt;getDefaultWeight()){}


I don't want to put a magic number in there for weight, since the object I am using has a "defaultWeight" parameter that all new shipments get if you don't specify a weight. I can't put the defaultWeight in the shipment itself, because it changes from shipment group to shipment group. Is there a better way to do it than the following?

public function createShipment($startZip, $endZip, weight = 0){
    if($weight &lt;= 0){
        $weight = $this-&gt;getDefaultWeight();
    }
}



How do you express an integer as a binary number with Python literals?

I was easily able to find the answer for hex:

    &gt;&gt;&gt; 0x12AF
    4783
    &gt;&gt;&gt; 0x100
    256


and octal:

    &gt;&gt;&gt; 01267
    695
    &gt;&gt;&gt; 0100
    64


How do you use literals to express binary in Python?



Summary of Answers


Python 2.5 and earlier: can express binary using int('01010101111',2) but not with a literal.
Python 2.5 and earlier: there is no way to express binary literals.
Python 2.6 beta: You can do like so: 0b1100111 or 0B1100111.
Python 2.6 beta: will also allow 0o27 or 0O27 (second character is the letter O) to represent an octal.
Python 3.0 beta: Same as 2.6, but will no longer allow the older 027 syntax for octals.



How do I set the icon that appears on the iPhone for the web sites I create?

My office has a central Source Safe 2005 install that we use for source control. I can't change what the office uses on the server. 

I develop on a laptop and would like to have a different local source control repository that can sync with the central server (when available) regardless of the what that central provider is. The reason for the request is so I can maintain a local stable branch/build for client presentations while continuing to develop without having to jump through flaming hoops. Also, as a consultant, my clients may request that I use their source control provider and flexibility here would make life easier.

Can any of the existing distributed source control clients handle that?

I'm looking for some way to effectively hide inherited members. I have a library of classes which inherit from common base classes.  Some of the more recent descendant classes inherit dependency properties which have become vestigial and can be a little confusing when using IntelliSense or using the classes in a visual designer.

These classes are all controls that are written to be compiled for either WPF or Silverlight 2.0.  I know about ICustomTypeDescriptor and ICustomPropertyProvider, but I'm pretty certain those can't be used in Silverlight.  

It's not as much a functional issue as a usability issue.  What should I do?

Update

Some of the properties that I would really like to hide come from ancestors that are not my own and because of a specific tool I'm designing for, I can't do member hiding with the new operator.  (I know, it's ridiculous)


I've never been completely happy with the way exception handling works, there's a lot exceptions and try/catch brings to the table (stack unwinding, etc.), but it seems to break a lot of the OO model in the process.

Anyway, here's the problem:

Let's say you have some class which wraps or includes networked file IO operations (e.g. reading and writing to some file at some particular UNC path somewhere). For various reasons you don't want those IO operations to fail, so if you detect that they fail you retry them and you keep retrying them until they succeed or you reach a timeout. I already have a convenient RetryTimer class which I can instantiate and use to sleep the current thread between retries and determine when the timeout period has elapsed, etc.

The problem is that you have a bunch of IO operations in several methods of this class, and you need to wrap each of them in try-catch / retry logic.

Here's an example code snippet:

RetryTimer fileIORetryTimer = new RetryTimer(TimeSpan.FromHours(10));bool success = false;while (!success){    try    {        // do some file IO which may succeed or fail        success = true;    }    catch (IOException e)    {        if (fileIORetryTimer.HasExceededRetryTimeout)        {            throw e;        }        fileIORetryTimer.SleepUntilNextRetry();    }}

So, how do you avoid duplicating most of this code for every file IO operation throughout the class? My solution was to use anonymous delegate blocks and a single method in the class which executed the delegate block passed to it. This allowed me to do things like this in other methods:

this.RetryFileIO( delegate()    {        // some code block    } );

I like this somewhat, but it leaves a lot to be desired. I'd like to hear how other people would solve this sort of problem.

As a LAMP developer considering moving to a .Net IIS platform, one of my concerns is the loss of productivity due to lack of shell... Has anyone else had this experience?  Is there possibly a Linux shell equivalent for Windows?


I always create a new empty database, after that backup and restore of the existing database into it, but is this really the best way? As it seems very error prone and over complicated for me.


When spliting a solution in to logical layers, when is it best to use a separate project over just grouping by a folder?

I'm writing an app to help facilitate some research, and part of this involves doing some statistical calculations. Right now, the researchers are using a program called SPSS. Part of the output that they care about looks like this:



They're really only concerned about the F and Sig. values. My problem is that I have no background in statistics, and I can't figure out what the tests are called, or how to calculate them.

I thought the F value might be the result of the F-test, but after following the steps given on Wikipedia, I got a result that was different from what SPSS gives.


I am looking for guidance regarding the best practice around the use of the Profile feature in ASP.NET.

How do you decide what should be kept in the built-in user Profile, or if you should create your own DB Table and add a column for the desired fields? For example, a user has a ZIP code, should I save the ZIP code in my own table, or should I add it to the web.config xml profile and access it via the user profile ASP.NET mechanize? 

The pros/cons I can think of are that since I don't know the profile very well (it is a bit of a Matrix right now), I probably can do whatever I want if I go the table route (e.g., SQL to get all the users in the same ZIP code as the current user); I don't know if I can do the same if I use the ASP.NET profile.


I was just looking through some information about Google's protocol buffers data interchange format.  Has anyone played around with the code or even created a project around it?

I'm currently using XML in a Python project for structured content created by hand in a text editor, and I was wondering what the general opinion was on Protocol Buffers as a user-facing input format.  The speed and brevity benefits definitely seem to be there, but there are so many factors when it comes to actually generating and processing the data.

I am trying to grab the capital letters of a couple of words and wrap them in span tags. I am using preg_replace for extract and wrapping purposes, but it's not outputting anything.

preg_replace("/[A-Z]/", "&lt;span class=\"initial\"&gt;$1&lt;/span&gt;", $str)



I am trying to set a flag to show or hide a page element, but it always displays even when the expression is false. 

$canMerge = ($condition1 &amp;&amp; $condition2) ? 'true' : 'false';...&lt;?php if ($canMerge) { ?&gt;Stuff&lt;?php } ?&gt;

What's up?

Is it possible to create "federated" Subversion servers?
As in one server at location A and another at location B that sync up their local versions of the repository automatically.  That way when someone at either location interacts with the repository they are accessing their respective local server and therefore has faster response times.

I've got a menu in Python. That part was easy. I'm using raw_input() to get the selection from the user. 

The problem is that raw_input (and input) require the user to press Enter after they make a selection. Is there any way to make the program act immediately upon a keystroke? Here's what I've got so far:

import sys
print """Menu
1) Say Foo
2) Say Bar"""
answer = raw_input("Make a selection&gt; ")

if "1" in answer: print "foo"
elif "2" in answer: print "bar"


It would be great to have something like

print menu
while lastKey = "":
    lastKey = check_for_recent_keystrokes()
if "1" in lastKey: #do stuff...



OK. This is a bit of a vanity app, but I had a situation today at work where I was in a training class and the machine was set to lock every 10 minutes.  Well, if the trainers got excited about talking - as opposed to changing slides - the machine would lock up.

I'd like to write a teeny app that has nothing but a taskbar icon that does nothing but move the mouse by 1 pixel every 4 minutes.  

I can do that in 3 ways with Delphi (my strong language) but I'm moving to C# for work and I'd like to know the path of least resistance there.

I am currently working on a project and my goal is to locate text in an image. OCR'ing the text is not my intention as of yet. I want to basically obtain the bounds of text within an image. I am using the AForge.Net imaging component for manipulation. Any assistance in some sense or another?

Update 2/5/09:
I've since went along another route in my project. However I did attempt to obtain text using MODI (Microsoft Office Document Imaging). It allows you to OCR an image and pull text from it with some ease.


On the project that I am working on I have a couple of databases. Each table and each column in the database has a description set (as an extended property in SQL 2005). As a part of the documentation going to the client we need to produce a data dictionary showing all of the tables and columns along with a collection of meta data (data-type, optionality, constraints).
Is anyone using a tool to automatically create this kind of document? If so, which tools do you use? I have used Data Dictionary Creator which is awesome but it doesn't seem to do data types or optionality (unless you want to add in custom fields and fill them in yourself).

How do I delimit a Javascript databound string parameter in an anchor OnClick event?


I have an anchor tag in an ASP.NET Repeater control.  
The OnClick event of the anchor contains a call to a Javascript function.  
The Javascript function takes a string for its input parameter.  
The string parameter is populated with a databound value from the Repeater.


I need the "double quotes" for the Container.DataItem.
I need the 'single quotes' for the OnClick.

And I still need one more delimiter (triple quotes?) for the input string parameter of the Javascript function call.

Since I can't use 'single quotes' again, how do I ensure the Javascript function knows the input parameter is a string and not an integer?

Without the extra quotes around the input string parameter, the Javascript function thinks I'm passing in an integer.

Cheers in advance for any knowledge you can drop.

The anchor:

&lt;a id="aShowHide" onclick='ToggleDisplay(&lt;%# DataBinder.Eval(Container.DataItem, "JobCode") %&gt;);' &gt;Show/Hide&lt;/a&gt;    


and here is the Javascript:

&lt;script language="JavaScript" type="text/javascript"&gt;
/* Shows/Hides the Jobs Div */
  function ToggleDisplay(jobCode)
  {
  /* Each div has it's ID set dynamically ('d' plus the JobCode) */
    var elem = document.getElementById('d' + jobCode);

    if (elem) 
    {
      if (elem.style.display != 'block') 
      {
        elem.style.display = 'block';
        elem.style.visibility = 'visible';
      } 
      else
      {
        elem.style.display = 'none';
        elem.style.visibility = 'hidden';
      }
    }
  }
&lt;/script&gt;



I have a bunch of latitude/longitude pairs that map to known x/y coordinates on a (geographically distorted) map.

Then I have one more latitude/longitude pair. I want to plot it on the map as best is possible. How do I go about doing this?

At first I decided to create a system of linear equations for the three nearest lat/long points and compute a transformation from these, but this doesn't work well at all. Since that's a linear system, I can't use more nearby points either.

You can't assume North is up: all you have is the existing lat/long->x/y mappings.

EDIT: it's not a Mercator projection, or anything like that. It's arbitrarily distorted for readability (think subway map). I want to use only the nearest 5 to 10 mappings so that distortion on other parts of the map doesn't affect the mapping I'm trying to compute.

Further, the entire map is in a very small geographical area so there's no need to worry about the globe--flat-earth assumptions are good enough.

Using ASP.NET MVC there are situations (such as form submission) that may require a RedirectToAction.  

One such situation is when you encounter validation errors after a form submission and need to redirect back to the form, but would like the URL to reflect the URL of the form, not the action page it submits to.

As I require the form to contain the originally POSTed data, for user convenience, as well as validation purposes, how can I pass the data through the RedirectToAction()?  If I use the viewData parameter, my POST parameters will be changed to GET parameters.


In order to fully use LinqToSql in an ASP.net 3.5 application, it is necessary to create DataContext classes (which is usually done using the designer in VS 2008). From the UI perspective, the DataContext is a design of the sections of your database that you would like to expose to through LinqToSql and is integral in setting up the ORM features of LinqToSql.

My question is: I am setting up a project that uses a large database where all tables are interconnected in some way through Foreign Keys. My first inclination is to make one huge DataContext class that models the entire database. That way I could in theory (though I don't know if this would be needed in practice) use the Foreign Key connections that are generated through LinqToSql to easily go between related objects in my code, insert related objects, etc.

However, after giving it some thought, I am now thinking that it may make more sense to create multiple DataContext classes, each one relating to a specific namespace or logical interrelated section within my database. My main concern is that instantiating and disposing one huge DataContext class all the time for individual operations that relate to specific areas of the Database would be impose an unnecessary imposition on application resources. Additionally, it is easier to create and manage smaller DataContext files than one big one. The thing that I would lose is that there would be some distant sections of the database that would not be navigable through LinqToSql (even though a chain of relationships connects them in the actual database). Additionally, there would be some table classes that would exist in more than one DataContext.

Any thoughts or experience on whether multiple DataContexts (corresponding to DB namespaces) are appropriate in place of (or in addition to) one very large DataContext class (corresponding to the whole DB)?

I was just wondering if there is an elegant way to set the maximum CPU load for a particular thread doing intensive calculations. 
Right now I have located the most time consuming loop in the thread (it does only compression) and use GetTickCount() and Sleep() with hardcoded values. It makes sure that the loop continues for a certain period of time and than sleeps for a certain minimal time. It more or less does the job i.e. guarantees that the thread will not use more than 50% of CPU. However behavior is dependent on the number of CPU cores (huge disadvantage) and simply ugly (smaller disadvantage :)). Any ideas?


When you data bind in C#, the thread that changes the data causes the control to change too. But if this thread is not the one on which the control was created, you'll get the above exception.

I surfed the net and found no good answer.

Anyone?


I have values stored as strings in a DataTable where each value could really represent an int, double, or string (they were all converted to strings during an import process from an external data source). I need to test and see what type each value really is.

What is more efficient for the application (or is there no practical difference)?


Try to convert to int (and then double). If conversion works, the return true. If an exception is thrown, return false.
Regular expressions designed to match the pattern of an int or double
Some other method?



Looking for books or other references that discuss actually "how" to write a code coverage tool in Java; some of the various techniques or tricks - source vs. byte code instrumentation. This is for a scripting language that generates java byte code under the hood.


I want to get the MD5 Hash of a string value in SQL Server 2005. I do this with the following command:

SELECT HashBytes('MD5', 'HelloWorld')


However, this returns a VarBinary instead of a VarChar value. If I attempt to convert 0x68E109F0F40CA72A15E05CC22786F8E6 into a VarChar I get há ðô§*à\Â'†øæ instead of 68E109F0F40CA72A15E05CC22786F8E6.

Is there any SQL-based solution?

Yes


Instead of writing my ASP.NET C# applications in Visual Studio, I used my favorite text editor UltraEdit32.

Is there anyway I can implement MVC without the use of VS?


The web applications I develop often require co-dependant configuration settings and there are also settings that have to change as we move between each of our environments.  

All our settings are currently simple key value pairs but it would be useful to create custom config sections so that it is obvious when two values need to change together or when the settings need to change for an environment.

What's the best way to create custom config sections and are there any special considerations to make when retrieving the values?

Is there a way to create a JButton with your own button graphic and not just with an image inside the button? 

If not, is there another way to create a custom button in java?


I'm currently working on an application with a frontend written in Adobe Flex 3. I'm aware of FlexUnit but what I'd really like is a unit test runner for Ant/NAnt and a runner that integrates with the Flex Builder IDE (AKA Eclipse). Does one exist? 

Also, are there any other resources on how to do Flex development "the right way" besides the Cairngorm microarchitecture example?


I currently use a DataTable to get results from a database which I can use in my code.

However, many example on the web show using a DataSet instead and accessing the table(s) through the collections method.

Is there any advantage, performance wise or otherwise, of using DataSets or DataTables as a storage method for SQL results?


Is there a way of mapping data collected on a stream or array to a data structure or vice-versa?
In C++ this would simply be a matter of casting a pointer to the stream as a data type I want to use (or vice-versa for the reverse)
eg: in C++

Mystruct * pMyStrct = (Mystruct*)&amp;SomeDataStream;
pMyStrct-&gt;Item1 = 25;

int iReadData = pMyStrct-&gt;Item2;


obviously the C++ way is pretty unsafe unless you are sure of the quality of the stream data when reading incoming data, but for outgoing data is super quick and easy.


How do I rewrite URL's in ASP.NET?

I would like users to be able to goto http://www.website.com/users/smith instead of http://www.website.com/?user=smith


I want to be able to do:For Each thing In things
End For

CLASSIC ASP - NOT .NET!

I see in the Stack Overflow footer that the SVN Revision number is displayed. Is this automated and if so, how does one implement it in ASP.NET?

(Solutions in other languages are acceptable)

I have a problem with maintaining state in an ASP.NET AJAX page. Short version: I need some way to update the page ViewState after an async callback has been made, to reflect any state changes the server made during the async call. 

This seems to be a common problem, but I will describe my scenario to help explain:

I have a grid-like control which has some JavaScript enhancements - namely, the ability to drag and drop columns and rows. When a column or row is dropped into a new position, an AJAX method is invoked to notify the control server-side and fire a corresponding server-side event ("OnColumnMoved" or "OnRowMoved").

ASP.NET AJAX calls, by default, send the entire page as the request. That way the page goes through a complete lifecycle, viewstate is persisted and the state of the control is restored before the RaiseCallbackEvent method is invoked.

However, since the AJAX call does not update the page, the ViewState reflects the original state of the control, even after the column or row has been moved. So the second time a client-side action occurs, the AJAX request goes to the server and the page &amp; control are built back up again to reflect the first state of the control, not the state after the first column or row was moved.

This problem extends to many implications. For example if we have a client-side/AJAX action to add a new item to the grid, and then a row is dragged, the grid is built server-side with one less item than on the client-side.

And finally &amp; most seriously for my specific example, the actual data source object we are acting upon is stored in the page ViewState. That was a design decision to allow keeping a stateful copy of the manipulated data which can either be committed to DB after many manipulations or discarded if the user backs out. That is very difficult to change.

So, again, I need a way for the page ViewState to be updated on callback after the AJAX method is fired.


In the code below

For i = LBound(arr) To UBound(arr)


What is the point in asking using LBound? Surely that is always 0.


In Windows, in any windows form or web browser, you can use the tab button to switch focus through all of the form fields. 

It will stop on textboxes, radiobuttons, checkboxes, dropdown menus, etc. 

However, in Mac OSX, tab skips dropdown menus. Is there anyway to change this behavior, or access the above items mentioned, without using a mouse?


I'm trying to read binary data using C#. I have all information about the layout of the data in the files I want to read. I'm able to read the data "chunk by chunk", i.e. getting the first 40 bytes of data converting it to a string, get the next 40 bytes, ...
Since there are at least three slighlty different version of the data, I would like to read the data directly into a struct. It just feels so much more right than by reading it "line by line".
I have tried the following approach but to no avail:StructType aStruct;
int count = Marshal.SizeOf(typeof(StructType));
byte[] readBuffer = new byte[count];
BinaryReader reader = new BinaryReader(stream);
readBuffer = reader.ReadBytes(count);
GCHandle handle = GCHandle.Alloc(readBuffer, GCHandleType.Pinned);
aStruct = (StructType) Marshal.PtrToStructure(handle.AddrOfPinnedObject(), typeof(StructType));
handle.Free();

The stream is an opened FileStream from which I have began to read from. I get an AccessViolationException when using Marshal.PtrToStructure.
The stream contains more information than I'm trying to read since I'm not interested in data at the end of the file.
The struct is defined like:[StructLayout(LayoutKind.Explicit)]
struct StructType
{
    [FieldOffset(0)]
    public string FileDate;
    [FieldOffset(8)]
    public string FileTime;
    [FieldOffset(16)]
    public int Id1;
    [FieldOffset(20)]
    public string Id2;
}

The examples code is changed from original to make this question shorter.
How would I read binary data from a file into a struct?

Regarding Agile development.

What are the best practices for testing security per release?  

If it is a monthly release, are there shops doing pen-tests every month?


What is the best way to record statistics on the number of visitors visiting my site that have set their browser to block ads?


I've been able to find details on several self-balancing BSTs through several sources, but I haven't found any good descriptions detailing which one is best to use in different situations (or if it really doesn't matter).  

I want a BST that is optimal for storing in excess of ten million nodes. The order of insertion of the nodes is basically random, and I will never need to delete nodes, so insertion time is the only thing that would need to be optimized.  

I intend to use it to store previously visited game states in a puzzle game, so that I can quickly check if a previous configuration has already been encountered.


I have 2 SQLite databases, one downloaded from a server (server.db), and one used as storage on the client (client.db). I need to perform various sync queries on the client database, using data from the server database.

For example, I want to delete all rows in the client.db tRole table, and repopulate with all rows in the server.db tRole table.

Another example, I want to delete all rows in the client.db tFile table where the fileID is not in the server.db tFile table.

In SQL Server you can just prefix the table with the name of the database. Is there anyway to do this in SQLite using Adobe Air?

How can I find out which node in a tree list the context menu has been activated? For instance right-clicking a node and selecting an option from the menu. 

I can't use the TreeViews' SelectedNode property because the node is only been right-clicked and not selected.


What are good libraries for C with datastructures like vectors, deques, stacks, hashmaps, treemaps, sets, etc.? Plain C, please, and platform-independent.

A quick glance at the present-day internet would seem to indicate that Adobe Flash is the obvious choice for embedding video in a web page.  Is this accurate, or are they other effective choices?  Does the choice of ASP.NET as a platform influence this decision?

Any good recommendations for a platform agnostic (i.e. Javascript) grid control/plugin that will accept pasted Excel data and can emit Excel-compliant clipboard data during a Copy?

I believe Excel data is formatted as CSV during "normal" clipboard operations.



dhtmlxGrid looks promising, but the online demo's don't actually copy contents to my clipboard!


The parent div needs to have a defined width, either in pixels or as a percentage. In Internet&nbsp;Explorer&nbsp;7, the parent div needs a defined width for child percentage divs to work correctly.


Here is one hack that may work. It isn't clean, but it looks like it might work:

Essentially, you just try to update a column that doesn't exist.


The problem is that the Distinct
 operator does not grant that it will
 maintain the original order of
 values.

So your query will need to work like this

var names = (from DataRow dr in dataTable.Rows
             select (string)dr["Name"]).Distinct().OrderBy( name =&gt; name );



It is very simple with the Skip and Take extension methods.

var query = from i in ideas
            select i;

var paggedCollection = query.Skip(startIndex).Take(count);



If you use a return type of IEnumerable, you can return your query variable directly.


I've had problems with JavaHL in Eclipse Ganymede, when it worked fine in Eclipse Europa.  I'm not sure how Aptana is different, but try either upgrading JavaHL or switching to the pure-java SVNKit implementation within the Subclipse config.

Rolling your own PHP solution will certainly work though I'm not sure if there is a good way to automatically duplicate the schema from one DB to the other (maybe this was your question).

If you are just copying data, and/or you need custom code anyway to convert between modified schemas between the two DB's, I would recommend using PHP 5.2+ and the PDO libraries.  You'll be able to connect using PDO ODBC (and use MSSQL drivers).  I had a lot of problems getting large text fields and multi-byte characters from MSSQL into PHP using other libraries.

Another tool to try would be the SQLMaestro suite - http://www.sqlmaestro.com It is a little tricky nailing down the precise tool, but they have a variety of tools, both free and for purchase that handle a wide variety of tasks for multiple database platforms. I'd suggest trying the Data Wizard tool first for MySQL, since I believe that will have the proper "import" tool you need.

The best way that I have found is the MySQL Migration Toolkit provided by MySQL.  I have used it successfully for some large migration projects.

You can do it by adding the following anywhere in your code 

$Id:$


So for example @Jeff did:

&lt;div id="svnrevision"&gt;svn revision: $Id:$&lt;/div&gt;


and when checked in the server replaced $Id:$ with the current revision number.  I also found this reference.

There is also $Date:$, $Rev:$, $Revision:$


Looks like Jeff is using CruiseControl.NET based on some leafing through the podcast transcripts.  This seems to have automated deployment capabilities from source control to production. Might this be where the insertion is happening?


Hey, I'm still stuck in that camp too. The third party application we have to support is FINALLY going to 2K5, so we're almost out of the wood. But I feel your pain 8^D

That said, from everything I heard from our DBA, the key is to convert the database to 8.0 format first, and then go to 2005. I believe they used the built in migration/upgrade tools for this. There are some big steps between 6.5 and 8.0 that are better solved there than going from 6.5 to 2005 directly.

Your BIGGEST pain, if you didn't know already, is that DTS is gone in favor of SSIS. There is a shell type module that will run your existing DTS packages, but you're going to want to manually recreate them all in SSIS. Ease of this will depend on the complexity of the packages themselves, but I've done a few at work so far and they've been pretty smooth.

I don't know the ASP.NET side of the problem but .NET has a pretty powerful class: System.Globalization.CultureInfo. You can use the following code to parse a string containing a double value:

double d = double.Parse("100.20", CultureInfo.CurrentCulture);//  -- OR --double d = double.Parse("100.20", CultureInfo.CurrentUICulture);

If ASP.NET somehow (i.e. using HTTP Request headers) passes current user's CultureInfo to either CultureInfo.CurrentCulture or CultureInfo.CurrentUICulture, these will work fine.

You can't please everyone.  If I enter ten as 10.000, and someone enters ten thousand as 10.000, you cannot handle that without some knowledge of the culture of the input.  Detect the culture somehow (browser, system setting - what is the use case?  ASP?  Internal app, or open to the world?), or provide an example of the expected formatting, and use the most lenient parser you can.  Probably something like:

double d = Double.Parse("5,000.00", NumberStyles.Any, CultureInfo.InvariantCulture);

The functionality you're looking for is traditionally called an UPSERT. Atleast knowing what it's called might help you find what you're looking for.

I don't think SQL Server 2005 has any great ways of doing this. 2008 introduces the MERGE statement that can be used to accomplish this as shown in: http://www.databasejournal.com/features/mssql/article.php/3739131 or http://blogs.conchango.com/davidportas/archive/2007/11/14/SQL-Server-2008-MERGE.aspx

Merge was available in the beta of 2005, but they removed it out in the final release.

What the upsert/merge is doing is something to the effect of...

IF EXISTS (SELECT * FROM [Table] WHERE Id = X)
   UPDATE [Table] SET...
ELSE
   INSERT INTO [Table]


So hopefully the combination of those articles and this pseudo code can get things moving.

I think the best you can do in this case is to take their input and then show them what you think they meant. If they disagree, show them the format you're expecting and get them to enter it again.

Any assemblies that are expected to be in the GAC should stay in the GAC. This includes System.web.dll or any other 3rd party dll that you'll deploy to the GAC in production. This means a new developer would have to install these assemblies.

All other 3rd party assemblies should be references through a relative path. My typical structure is:

-Project--Project.sln--References---StructureMap.dll---NUnit.dll---System.Web.Mvc.dll--Project.Web---Project.Web.Proj---Project.Web.Proj files--Project---Project.Proj---Project.Proj files

Project.Web and Project reference the assemblies in the root/References folder relatively. These .dlls are checked into subversion.

Aside from that, */bin  */bin/* obj  should be in your global ignore path.

With this setup, all references to assemblies are either through the GAC (so should work across all computers), or relative to each project within your solution.

Is this a .Net specific question?

Generally the best practice is to not check in anything which is built automatically from files that are already in SCM.  All of that is ideally created as part of your automatic build process.

If the bin directory you're referring to contains third-party binaries, rather than a build of your project, ignore (downvote?) this advice.

I know that Visual Studio itself (at least in 2003 version) references the IE dll directly to render the "Design View".

It may be worth looking into that.

Otherwise, I can't think of anything beyond the Web Browser control.

I was glad to find a good conversation on this subject, as I hadn't really given it much thought before.

In summary, signed is a good general choice - even when you're dead sure all the numbers are positive - if you're going to do arithmetic on the variable (like in a typical for loop case).  

If you're going to do bitwise things like masks, unsigned starts to make more sense.  Or, if you're desperate to get that extra positive range by taking advantage of the sign bit.

Personally, I like signed because I don't trust myself to stay consistent and avoid mixing the two types (like the article warns against).

In your example above, when 'i' will always be positive and a higher range would be beneficial, unsigned would be useful. Like if you're using 'declare' statements, such as: 

#declare BIT1 (unsigned int 1)#declare BIT32 (unsigned int reallybignumber)

Especially when these values will never change.

However, if you're doing an accounting program where the people are irresponsible with their money and are constantly in the red, you will most definitely want to use 'signed'.

I do agree with saint though that a good rule of thumb is to use signed, which C actually defaults to, so you're covered.

size_t is often a good choice for this, or size_type if you're using an STL class.


The difference between 12.345 in French and English is a factor of 1000. If you supply an expected range where max &lt; 1000*min, you can easily guess. 

Take for example the height of a person (including babies and children) in mm.

By using a range of 200-3000, an input of 1.800 or 1,800 can unambiguously be interpreted as 1 meter and 80 centimeters, whereas an input of 912.300 or 912,300 can unambiguously be interpreted as 91 centimeters and 2.3 millimeters.

open up a terminal (Applications-&gt;Utilities-&gt;Terminal) and type this in:

locate InsertFontHere

This will spit out every file that has the name you want.

Warning: there may be alot to wade through.

I haven't been able to find anything that does this directly.  I think you'll have to iterate through the various font folders on the system: /System/Library/Fonts, /Library/Fonts, and there can probably be a user-level directory as well ~/Library/Fonts.


Using fcgi with Ruby is known to be very buggy. 

Practically everybody has moved to Mongrel for this reason, and I recommend you do the same.

Doesn't this depend on the hardware as well as number of threads and stuff?

I would make a simple test and run it with increasing amounts of threads hammering and see what seems best.

Maven helps quite a lot with this problem when I'm coding java. We commit the pom.xml to the scs and the maven repository contains all our dependencies.
For me that seems like a nice way to do it.

One possibility is Hudson.  It's written in Java, but there's integration with Python projects:


  Hudson embraces Python


I've never tried it myself, however.

(Update, Sept. 2011: After a trademark dispute Hudson has been renamed to Jenkins.)


We run Buildbot - Trac at work, I haven't used it too much since my code base isn't part of the release cycle yet. But we run the tests on different environments (OSX/Linux/Win) and it sends emails --and it's written in python.

Interesting question!

I would do this by picking combinations, something like the following in python.  The hardest part is probably first pass verification, i.e. if f(1,2,3) returns true, is that a correct result?  Once you have verified that, then this is a good basis for regression testing.

Probably it's a good idea to make a set of test cases that you know will be all true (e.g. 3,4,5 for this triangle case), and a set of test cases that you know will be all false (e.g. 0,1,inf).  Then you can more easily verify the tests are correct.


# xpermutations from http://code.activestate.com/recipes/190465
from xpermutations import *

lengths=[-1,0,1,5,10,0,1000,'inf']
for c in xselections(lengths,3):        # or xuniqueselections
    print c



(-1,-1,-1);
(-1,-1,0);
(-1,-1,1);
(-1,-1,5);
(-1,-1,10);
(-1,-1,0);
(-1,-1,1000);
(-1,-1,inf);
(-1,0,-1);
(-1,0,0);
...



For C++, Boost.Random is probably what you're looking for. It has support for MT (among many other algorithms), and can collect entropy via the nondet_random class. Check it out! :-)

The Gnu Scientific Library (GSL) has a pretty extensive set of RN generators, test harness, etc.  If you're on linux, it's probably already available on your system.

It's been awhile since I used FCGI but I think a FCGI process could throw a SystemExit if the thread was taking too long.  This could be the web service not responding or even a slow DNS query.  Some google results show a similar error with Python and FCGI so moving to mongrel would be a good idea.  This post is my reference I used to setup mongrel and I still refer back to it.


With the "Built in" stuff, you can't, as using 1.0.* or 1.0.0.* will replace the revision and build numbers with a coded date/timestamp, which is usually also a good way. 

For more info, see the Assembly Linker Documentation in the /v tag.

As for automatically incrementing numbers, use the AssemblyInfo Task:

AssemblyInfo Task

This can be configured to automatically increment the build number.

There are 2 Gotchas:


Each of the 4 numbers in the Version string is limited to 65535. This is a Windows Limitation and unlikely to get fixed.

Why are build numbers limited to 65535?

Using with with Subversion requires a small change:

Using MSBuild to generate assembly version info at build time (including SubVersion fix)



Retrieving the Version number is then quite easy:

Version v = Assembly.GetExecutingAssembly().GetName().Version;
string About = string.Format(CultureInfo.InvariantCulture, @"YourApp Version {0}.{1}.{2} (r{3})", v.Major, v.Minor, v.Build, v.Revision);




And, to clarify: In .net or at least in C#, the build is actually the THIRD number, not the fourth one as some people (for example Delphi Developers who are used to Major.Minor.Release.Build) might expect.

In .net, it's Major.Minor.Build.Revision.


Second the Buildbot - Trac integration. You can find more information about the integration on the Buildbot website. At my previous job, we wrote and used the plugin they mention (tracbb).
What the plugin does is rewriting all of the Buildbot urls so you can use Buildbot from within Trac. (http://example.com/tracbb).

The really nice thing about Buildbot is that the configuration is written in Python. You can integrate your own Python code directly to the configuration. It's also very easy to write your own BuildSteps to execute specific tasks.

We used BuildSteps to get the source from SVN, pull the dependencies, publish test results to WebDAV, etcetera.

I wrote an X10 interface so we could send signals with build results. When the build failed, we switched on a red lava lamp. When the build succeeded, a green lava lamp switched on. Good times :-)

It depends on what kind of application you are building. Create a representative test scenario, and start hammering away. Then you will know the definitive answer.

Besides your use case, it also depends on CPU, memory, front-side bus, operating system, cache settings, etcetera.

Seriously, just test your own scenario.

If you need some numbers (that actually may mean nothing in your scenario):


Oracle Berkeley DB: 
Performance Metrics and 
Benchmarks
Performance Metrics 
&amp; Benchmarks: 
Berkeley DB


If you are choosing an encryption method for your login system then speed is not your friend, Jeff had a to-and-frow with Thomas Ptacek about passwords and the conclusion was that you should use the slowest, most secure encryption method you can afford to. 


  From Thomas Ptacek's blog:
  Speed is exactly what you don’t want in a password hash function.
  
  Modern password schemes are attacked with incremental password crackers.
  
  Incremental crackers don’t precalculate all possible cracked passwords. They consider each password hash individually, and they feed their dictionary through the password hash function the same way your PHP login page would. Rainbow table crackers like Ophcrack use space to attack passwords; incremental crackers like John the Ripper, Crack, and LC5 work with time: statistics and compute.
  
  The password attack game is scored in time taken to crack password X. With rainbow tables, that time depends on how big your table needs to be and how fast you can search it. With incremental crackers, the time depends on how fast you can make the password hash function run.
  
  The better you can optimize your password hash function, the faster your password hash function gets, the weaker your scheme is. MD5 and SHA1, even conventional block ciphers like DES, are designed to be fast. MD5, SHA1, and DES are weak password hashes. On modern CPUs, raw crypto building blocks like DES and MD5 can be bitsliced, vectorized, and parallelized to make password searches lightning fast. Game-over FPGA implementations cost only hundreds of dollars. 



I'm with Peter. Developer don't seem to understand passwords. We all pick (and I'm guilty of this too) MD5 or SHA1 because they are fast. Thinking about it ('cuz someone recently pointed it out to me) that doesn't make any sense. We should be picking a hashing algorithm that's stupid slow. I mean, on the scale of things, a busy site will hash passwords what? every 1/2 minute? Who cares if it take 0.8 seconds vs 0.03 seconds server wise? But that extra slowness is huge to prevent all types of common brute-forcish attacks.

From my reading, bcrypt is specifically designed for secure password hashing. It's based on blowfish, and there are many implementation.

For PHP, check out PHPPass http://www.openwall.com/phpass/

For anyone doing .NET, check out BCrypt.NET http://derekslager.com/blog/posts/2007/10/bcrypt-dotnet-strong-password-hashing-for-dotnet-and-mono.ashx

No, there are many applications built with VS2005 that have to support Windows XP, 2000, NT, the whole stack. The issue is that (by default) VS2005 wants to use libraries/exports not present on NT.
See this thread for some background.
Then start limiting your dependencies via preprocessor macros, and avoiding APIs which aren't supported on NT.

Here is my current method. Any suggestions?

Regex singleMToDoubleRegex = new Regex("(?&lt;!m)m(?!m)");Regex singleDToDoubleRegex = new Regex("(?&lt;!d)d(?!d)");CultureInfo currentCulture = CultureInfo.CurrentUICulture;// If the culture is netural there is no date pattern to use, so use the default.if (currentCulture.IsNeutralCulture){    currentCulture = CultureInfo.InvariantCulture;}// Massage the format into a more general user friendly form.string shortDatePattern = CultureInfo.CurrentUICulture.DateTimeFormat.ShortDatePattern.ToLower();shortDatePattern = singleMToDoubleRegex.Replace(shortDatePattern, "mm");shortDatePattern = singleDToDoubleRegex.Replace(shortDatePattern, "dd");

How about giving the format (mm/dd/yyyy or dd/mm/yyyy) followed by a printout of today's date in the user's culture. MSDN has an article on formatting a DateTime for the person's culture, using the CultureInfo object that might be helpful in doing this. A combination of the format (which most people are familiar with) combined with the current date represented in that format should be enough of a clue to the person on how they should enter the date. (Also include a calendar control for those who still cant figure it out).

You can print from the command line using the following:


  rundll32.exe
  %WINDIR%\System32\mshtml.dll,PrintHTML
  "%1"


Where %1 is the file path of the html file to be printed.

If you don't need to print from memory (or can afford to write to the disk in a temp file) you can use:

using (Process printProcess = new Process()){    string systemPath = Environment.GetFolderPath(Environment.SpecialFolder.System);    printProcess.StartInfo.FileName = systemPath + @"\rundll32.exe";    printProcess.StartInfo.Arguments = systemPath + @"\mshtml.dll,PrintHTML """ + fileToPrint + @"""";    printProcess.Start();}

N.B. This only works on Windows 2000 and above I think.

Just use ISO-8601. It's an international standard.

Date and time (current at page generation) expressed according to ISO 8601:
Date:                           2014-07-05
Combined date and time in UTC:  2014-07-05T04:00:25+00:00
                                2014-07-05T04:00:25Z
Week:                           2014-W27
Date with week number:          2014-W27-6
Ordinal date:                   2014-186



I don't have any experience with http://www.SiteGround.com as a web host personally.  

This is just a guess, but it's common for a shared host to support Python and MySQL with the MySQLdb module (e.g., GoDaddy does this).  Try the following CGI script to see if MySQLdb is installed.

#!/usr/bin/pythonmodule_name = 'MySQLdb'head = '''Content-Type: text/html%s is ''' % module_nametry:    __import__(module_name)    print head + 'installed'except ImportError:    print head + 'not installed'

What source control system are you using? 

Almost all of them have some form of $ Id $ tag that gets expanded when the file is checked in.

I usually use some form of hackery to display this as the version number.

The other alternative is use to use the date as the build number: 080803-1448

Absolutely, especially dealing with lots of these permutations/combinations I can definitely see that the first pass would be an issue.

Interesting implementation in python, though I wrote a nice one in C and Ocaml based on "Algorithm 515" (see below). He wrote his in Fortran as it was common back then for all the "Algorithm XX" papers, well, that assembly or c. I had to re-write it and make some small improvements to work with arrays not ranges of numbers. This one does random access, I'm still working on getting some nice implementations of the ones mentioned in Knuth 4th volume fascicle 2. I'll an explanation of how this works to the reader. Though if someone is curious, I wouldn't object to writing something up.

/** [combination c n p x] * get the [x]th lexicographically ordered set of [p] elements in [n] * output is in [c], and should be sizeof(int)*[p] */void combination(int* c,int n,int p, int x){    int i,r,k = 0;    for(i=0;i&lt;p-1;i++){        c[i] = (i != 0) ? c[i-1] : 0;        do {            c[i]++;            r = choose(n-c[i],p-(i+1));            k = k + r;        } while(k &lt; x);        k = k - r;    }    c[p-1] = c[p-2] + x - k;}

~"Algorithm 515: Generation of a Vector from the Lexicographical Index"; Buckles, B. P., and Lybanon, M. ACM Transactions on Mathematical Software, Vol. 3, No. 2, June 1977.

I uploaded it and got an internal error

Premature end of script headers

After much playing around, I found that if I had

import cgiimport cgitb; cgitb.enable()import MySQLdb

It would give me a much more useful answer and say that it was not installed, you can see it yourself -> http://woarl.com/db.py

Oddly enough, this would produce an error

import MySQLdbimport cgiimport cgitb; cgitb.enable()

I looked at some of the other files I had up there and it seems that library was one of the ones I had already tried.

I'm partway to my solution with this entry on MSDN (don't know how I couldn't find it before).

User/Machine Hive
Subkeys and values entered under this hive will be installed under the HKEY_CURRENT_USER hive when a user chooses "Just Me" or the HKEY_USERS hive or when a user chooses "Everyone" during installation.

Registry Editor


First: Yes, this is something that belongs in the Application for the exact reson you specified: What happens after new user profiles are created? Sure, if you're using a domain it's possible to have some stuff put in the registry on creation, but this is not really a use case. The Application should check if there are seetings and use the default settings if not.

That being said, it IS possible to change other users Keys through the HKEY_USERS Hive.

I have no experience with the Visual Studio 2003 Setup Project, so here is a bit of (totally unrelated) VBScript code that might just give you an idea where to look:

const HKEY_USERS = &amp;H80000003strComputer = "."Set objReg=GetObject("winmgmts:{impersonationLevel=impersonate}!\\" &amp; strComputer &amp; "\root\default:StdRegProv")strKeyPath = ""objReg.EnumKey HKEY_USERS, strKeyPath, arrSubKeysstrKeyPath = "\Software\Microsoft\Windows\CurrentVersion\WinTrust\Trust Providers\Software Publishing"For Each subkey In arrSubKeys    objReg.SetDWORDValue HKEY_USERS, subkey &amp; strKeyPath, "State", 146944Next

(Code Courtesy of Jeroen Ritmeijer)

I'm guessing that because you want to set it for all users, that you're on some kind of shared computer, which is probably running under a domain?

HERE BE DRAGONS

Let's say Joe and Jane regularly log onto the computer, then they will each have 'registries'.

You'll then install your app, and the installer will employ giant hacks and disgusting things to set items under HKCU for them.

THEN, bob will come along and log on (he, and 500 other people have accounts in the domain and so can do this). He's never used this computer before, so he has no registry. The first time he logs in, windows creates him one, but he won't have your setting. 

Your app then falls over or behaves incorrectly, and bob complains loudly about those crappy products from raynixon incorporated.

The correct answer is to just have some default settings in your app, which can write them to the registry if it doesn't find them. It's general good practice that your app should never depend on the registry, and should create things as needed, for any registry entry, not just HKCU, anyway

A solution, albeit one that defers handling of the null value to the code, could be:


  DateTime yesterday = DateTime.Now.Date.AddDays(-1);


var collection=
    from u in db.Universe
    select new
    {
        u.id,
        u.name,
        MaxDate =(DateTime?)
       (
           from h in db.History
           where u.Id == h.Id
           &amp;&amp; h.dateCol &lt; yesterday
           select h.dateCol 
       ).Max()
    };


This does not produce exactly the same SQL, but does provide the same logical result. Translating "complex" SQL queries to LINQ is not always straightforward.


You're looking for Sandcastle

Project Page: Sandcastle Releases

Blog: Sandcastle Blog

NDoc Code Documentation Generator for .NET used to be the tool of choice, but support has all but stopped.


Have a look at Sandcastle, which does exactly that. It's also one of the more simpler solutions out there, and it's more or less the tool of choice, so in the long run, maybe we could help you to set up Sandcastle if you specify what issues you encountered during setup?

Despite what the MSDN article  says about User/Machine Hive, it doesn't write to HKEY_USERS. Rather it writes to HKCU if you select Just Me and HKLM if you select Everyone.

So my solution is going to be to use the User/Machine Hive, and then in the application it checks if the registry entries are in HKCU and if not, copies them from HKLM. I know this probably isn't the most ideal way of doing it, but it has the least amount of changes.

If you have the ability to use WScript.Shell then you can just execute pscp.exe from the Putty package. Obviously this is less then ideal but it will get the job done and let you use SCP/SFTP in classic ASP.

I am by no means authoritative, but I believe the only supported path is from 6.5 to 7. Certainly that would be the most sane route, then I believe you can migrate from 7 directly to 2005 pretty painlessly.

As for scripting out all the objects - I would advise against it as you will inevitably miss something (unless you database is truly trivial).

If you can find a professional or some other super-enterprise version of Visual Studio 6.0 - it came with a copy of MSDE (Basically the predecessor to SQL Express). I believe MSDE 2000 is still available as a free download from Microsoft, but I don't know if you can migrate directly from 6.5 to 2000.

I think in concept, you won't likely face any danger. Years of practice however tell me that you will always miss some object, permission, or other database item that won't manifest itself immediately. If you can script out the entire dump, the better as you will be less likely to miss something - and if you do miss something, it can be easily added to the script and fixed. I would avoid any manual steps (other than hitting the enter key once) like the plague.

Does doubling the \ work?

insert into EscapeTest (text) values ('This will be inserted \\n This will not be');

Partially. The text is inserted, but the warning is still generated.

I found a discussion that indicated the text needed to be preceded with 'E', as such:

insert into EscapeTest (text) values (E'This is the first part \n And this is the second');


This suppressed the warning, but the text was still not being returned correctly. When I added the additional slash as Michael suggested, it worked.

As such:

insert into EscapeTest (text) values (E'This is the first part \\n And this is the second');



Very roughly and from memory since I don't have code on this laptop:

using (OleDBConnection conn = new OleDbConnection())
{
  conn.ConnectionString = "Whatever connection string";

  using (OleDbCommand cmd = new OleDbCommand())
  {
    cmd.Connection = conn;
    cmd.CommandText = "Select * from CoolTable";

    using (OleDbDataReader dr = cmd.ExecuteReader())
    {
      while (dr.Read())
      {
        // do something like Console.WriteLine(dr["column name"] as String);
      }
    }
  }
}



Cool.

I also found the documentation regarding the E:

http://www.postgresql.org/docs/8.3/interactive/sql-syntax-lexical.html#SQL-SYNTAX-STRINGS


  PostgreSQL also accepts "escape" string constants, which are an extension to the SQL standard. An escape string constant is specified by writing the letter E (upper or lower case) just before the opening single quote, e.g. E'foo'. (When continuing an escape string constant across lines, write E only before the first opening quote.) Within an escape string, a backslash character (\) begins a C-like backslash escape sequence, in which the combination of backslash and following character(s) represents a special byte value. \b is a backspace, \f is a form feed, \n is a newline, \r is a carriage return, \t is a tab. Also supported are \digits, where digits represents an octal byte value, and \xhexdigits, where hexdigits represents a hexadecimal byte value. (It is your responsibility that the byte sequences you create are valid characters in the server character set encoding.) Any other character following a backslash is taken literally. Thus, to include a backslash character, write two backslashes (\\). Also, a single quote can be included in an escape string by writing \', in addition to the normal way of ''.


You can upgrade 6.5 to SQL Server 2000.  You may have an easier time getting a hold of SQL Server or the 2000 version of the MSDE.  Microsoft has a page on going from 6.5 to 2000.  Once you have the database in 2000 format, SQL Server 2005 will have no trouble upgrading it to the 2005 format.  

If you don't have SQL Server 2000, you can download the MSDE 2000 version directly from Microsoft.

@Goyuix -- that's excellent for something written from memory.
tested it here -- found the connection wasn't opened. Otherwise very nice.

using System.Data.OleDb;
...

using (OleDbConnection conn = new OleDbConnection())
{
    conn.ConnectionString = "Provider=sqloledb;Data Source=yourServername\\yourInstance;Initial Catalog=databaseName;Integrated Security=SSPI;";

    using (OleDbCommand cmd = new OleDbCommand())
    {
        conn.Open();
        cmd.Connection = conn;
        cmd.CommandText = "Select * from yourTable";

        using (OleDbDataReader dr = cmd.ExecuteReader())
        {
            while (dr.Read())
            {
                Console.WriteLine(dr["columnName"]);
            }
        }
    }
}



is this a console app or a winforms app? If it's a .NET 1.1 console app this is, sadly, by design -- it's confirmed by a MSFT dev in the second blog post you referenced:


  BTW, on my 1.1 machine the example from MSDN does have the expected output; it's just that the second line doesn't show up until after you've attached a debugger (or not). In v2 we've flipped things around so that the UnhandledException event fires before the debugger attaches, which seems to be what most people expect.


Sounds like .NET 2.0 does this better (thank goodness), but honestly, I never had time to go back and check.

It's a WinForms app. The exceptions that are caught by Application.ThreadException work fine, and I don't get the ugly .NET exception box (OK to terminate, cancel to debug? who came up with that??).

I was getting some exceptions that weren't being caught by that and ended up going to the AppDomain.UnhandledException event that were causing problems. I think I've caught most of those exceptions and I am displaying them in our nice error box now.

So I'll just have to hope there are not some other circumstances that would cause exceptions to not be caught by the Application.ThreadException handler.

Oh, in WinForms you definitely should be able to get it to work. The only thing you have to watch out for is things happening on different threads.

I have an old CodeProject article here which should help:

http://www.codeproject.com/KB/exception/ExceptionHandling.aspx

That's definitely a good way to do it.  But you if you happen to be using a database that supports LINQ to SQL, it can be a lot more fun.  It can look something like this:

MyDB db = new MyDB("Data Source=...");var q = from db.MyTable        select c;foreach (var c in q)  Console.WriteLine(c.MyField.ToString());

You can use this program, Handle, to find which process has the lock on your file. It's a command-line tool, so I guess you use the output from that... I'm not sure about finding it programmatically.

If deleting the file can wait, you could specify it for deletion when your computer next starts up:


Start REGEDT32 (W2K) or REGEDIT (WXP) and navigate to:

HKEY_LOCAL_MACHINE\System\CurrentControlSet\Control\Session Manager

W2K and WXP


W2K:EditAdd Value...Data Type: REG_MULTI_SZValue Name: PendingFileRenameOperationsOK
WXP:EditNewMulti-String Valueenter
PendingFileRenameOperations

In the Data area, enter "\??\" + filename to be deleted. LFNs may
be entered without being embedded in quotes. To delete C:\Long Directory Name\Long File Name.exe, enter the following data:

\??\C:\Long Directory Name\Long File Name.exe


Then press OK.
The "destination file name" is a null (zero) string. It is entered
as follows:


W2K:EditBinaryselect Data Format: Hexclick at the end of the hex stringenter 0000 (four zeros)OK
WXP:Right-click the valuechoose "Modify Binary Data"click at the end of the hex stringenter 0000 (four zeros)OK

Close REGEDT32/REGEDIT and reboot to delete the file.


(Shamelessly stolen from some random forum, for posterity's sake.)


Killing other processes is not a healthy thing to do. If your scenario involves something like uninstallation, you could use the MoveFileEx API function to mark the file for deletion upon next reboot.

If it appears that you really need to delete a file in use by another process, I'd recommend re-considering the actual problem before considering any solutions.


If you want to do it programatically. I'm not sure... and I'd really recommend against it.
If you're just troubleshooting stuff on your own machine, SysInternals Process Explorer can help you

Run it, use the Find Handle command (I think it's either in the find or handle menu), and search for the name of your file. Once the handle(s) is found, you can forcibly close them.

You can then delete the file and so on.

Beware, doing this may cause the program which owns the handles to behave strangely, as you've just pulled the proverbial rug out from under it, but it works well when you are debugging your own errant code, or when visual studio / windows explorer is being crap and not releasing file handles even though you told them to close the file ages ago... sigh :-)

Oh, one big hack I employed years ago, is that Windows won't let you delete files, but it does let you move them.

Pseudo-sort-of-code:

mv %WINDIR%\System32\mfc42.dll %WINDIR\System32\mfc42.dll.old
Install new mfc42.dll
Tell user to save work and restart applications


When the applications restarted (note we didn't need to reboot the machine), they loaded the new mfc42.dll, and all was well. That, coupled with PendingFileOperations to delete the old one the next time the whole system restarted, worked pretty well.


The typical method is as follows. You've said you want to do this in C# so here goes...

If you don't know which process has the file locked, you'll need to examine each process's handle list, and query each handle to determine if it identifies the locked file. Doing this in C# will likely require P/Invoke or an intermediary C++/CLI to call the native APIs you'll need.
Once you've figured out which process(es) have the file locked, you'll need to safely inject a small native DLL into the process (you can also inject a managed DLL, but this is messier, as you then have to start or attach to the .NET runtime).
That bootstrap DLL then closes the handle using CloseHandle etc.
Essentially: the way to unlock a "locked" file is to inject a DLL into the offending process's address space and close it yourself. You can do this using native or managed code. No matter what, you're going to need a small amount of native code or at least P/Invoke into the same.
Helpful links:

http://www.codeproject.com/KB/threads/winspy.aspx
http://damianblog.com/2008/07/02/net-code-injection/
Good luck!

What I did when working against a database of unknown performance was to measure turnaround time on my queries.  I kept upping the thread count until turn-around time dropped, and dropping the thread count until turn-around time improved (well, it was processes in my environment, but whatever).  

There were moving averages and all sorts of metrics involved, but the take-away lesson was: just adapt to how things are working at the moment.  You never know when the DBAs will improve performance or hardware will be upgraded, or perhaps another process will come along to load down the system while you're running.  So adapt.

Oh, and another thing: avoid process switches if you can - batch things up.



Oh, I should make this clear: this all happened at run time, not during development.


The trouble with international standards is that pretty much noone uses them. I try where I can, but I am forced to use dd/mm/yyyy almost everywhere in real life, which means I am so used to it it's always a conscious process to use ISO-8601. For the majority of people who don't even try to use ISO-8601 it's even worse. If you can internationalize where you can, I think it's a great advantage.

Yes, you are right to lock at the VSZ.

ps u will give you the VSZ and RSS, which are the virtual memory size and resident set size.  The RSS is how much physical memory has been allocated to the process, and the VSZ is the virtual memory size of the process.  If you have several copies of a program running, a lot of the memory in the VSZ will be shared between those processes.


Maybe https://ci-bayes.dev.java.net/ or http://www.cs.cmu.edu/~javabayes/Home/node2.html?

I never played with it either.


Here is an implementation of Bayesian filtering in C#: A Naive Bayesian Spam Filter for C# (hosted on CodeProject).

No.

See A Pragmatic Look at Exception Specifications for reasons why not. 

The only way you can "help" this is to document the exceptions your function can throw, say as a comment in the header file declaring it. This is not enforced by the compiler or anything. Use code reviews for that purpose.


AppDomain.UnhandledException is an event, not a global exception handler. This means, by the time it is raised, your application is already on its way down the drain, and there is nothing you can do about it, except for doing cleanup and error logging.

What happened behind the scenes is this: The framework detected the exception, walked up the call stack to the very top, found no handlers that would recover from the error, so was unable to determine if it was safe to continue execution. So, it started the shutdown sequence, and fired up this event as a courtesy to you so you can pay your respects to your already-doomed process. This happens when an exception is left unhandled in the main thread.

There is no single-point solution to this kind of error. You need to put a real exception handler (a catch block) upstream of all places where this error occurs and forward it to (for example) a global handler method/class that will determine if it is safe to simply report and continue, based on exception type and/or content.

Edit: It is possible to disable (=hack) the error-reporting mechanism built into Windows so the mandatory "crash and burn" dialog does not get displayed when your app goes down. However, this becomes effective for all the applications in the system, not just your own.

I've just set up Sandcastle again. Try installing it (the May 2008 release) and search for SandcastleGui.exe or something similar (it's in the examples folder or so).

Click Add Assembly and add your Assembly or Assemblies, add any .xml Documentation files (the ones generated by the compiler if you enabled that option) and then Build.

It will take some time, but the result will be worth the effort. It will actually look up stuff from MSDN, so your resulting documentation will also have the Class Inheritance all the way down to System.Object with links to MSDN and stuff.

Sandcastle seems a bit complicated at first, especially when you want to use it in an automated build, but I am absolutely sure it will be worth the effort.

Also have a look at Sandcastle Help File Builder, this is a somewhat more advanced GUI for it.


Follow this simple 5 step article and you are pretty much done. As a bonus you can use H2Viewer to view Html Help 2.x files.

Why is it needed?

When data is stored on disk based storage devices, it is stored as blocks of data. These blocks are accessed in their entirety, making them the atomic disk access operation. Disk blocks are structured in much the same way as linked lists; both contain a section for data, a pointer to the location of the next node (or block), and both need not be stored contiguously.

Due to the fact that a number of records can only be sorted on one field, we can state that searching on a field that isn’t sorted requires a Linear Search which requires N/2 block accesses (on average), where N is the number of blocks that the table spans. If that field is a non-key field (i.e. doesn’t contain unique entries) then the entire table space must be searched at N block accesses.

Whereas with a sorted field, a Binary Search may be used, this has log2 N block accesses. Also since the data is sorted given a non-key field, the rest of the table doesn’t need to be searched for duplicate values, once a higher value is found. Thus the performance increase is substantial.

What is indexing?

Indexing is a way of sorting a number of records on multiple fields. Creating an index on a field in a table creates another data structure which holds the field value, and pointer to the record it relates to. This index structure is then sorted, allowing Binary Searches to be performed on it.

The downside to indexing is that these indexes require additional space on the disk, since the indexes are stored together in a table using the MyISAM engine, this file can quickly reach the size limits of the underlying file system if many fields within the same table are indexed.

How does it work?

Firstly, let’s outline a sample database table schema; 


Field name       Data type      Size on disk
id (Primary key) Unsigned INT   4 bytes
firstName        Char(50)       50 bytes
lastName         Char(50)       50 bytes
emailAddress     Char(100)      100 bytes


Note: char was used in place of varchar to allow for an accurate size on disk value. 
This sample database contains five million rows, and is unindexed. The performance of several queries will now be analyzed. These are a query using the id (a sorted key field) and one using the firstName (a non-key unsorted field).

Example 1

Given our sample database of r = 5,000,000 records of a fixed size giving a record length of R = 204 bytes and they are stored in a table using the MyISAM engine which is using the default block size B = 1,024 bytes. The blocking factor of the table would be bfr = (B/R) = 1024/204 = 5 records per disk block. The total number of blocks required to hold the table is N = (r/bfr) = 5000000/5 = 1,000,000 blocks. 

A linear search on the id field would require an average of N/2 = 500,000 block accesses to find a value given that the id field is a key field. But since the id field is also sorted a binary search can be conducted requiring an average of log2 1000000 = 19.93 = 20 block accesses. Instantly we can see this is a drastic improvement.

Now the firstName field is neither sorted nor a key field, so a binary search is impossible, nor are the values unique, and thus the table will require searching to the end for an exact N = 1,000,000 block accesses. It is this situation that indexing aims to correct.

Given that an index record contains only the indexed field and a pointer to the original record, it stands to reason that it will be smaller than the multi-field record that it points to. So the index itself requires fewer disk blocks than the original table, which therefore requires fewer block accesses to iterate through. The schema for an index on the firstName field is outlined below; 


Field name       Data type      Size on disk
firstName        Char(50)       50 bytes
(record pointer) Special        4 bytes


Note: Pointers in MySQL are 2, 3, 4 or 5 bytes in length depending on the size of the table.

Example 2

Given our sample database of r = 5,000,000 records with an index record length of R = 54 bytes and using the default block size B = 1,024 bytes. The blocking factor of the index would be bfr = (B/R) = 1024/54 = 18 records per disk block. The total number of blocks required to hold the table is N = (r/bfr) = 5000000/18 = 277,778 blocks. 

Now a search using the firstName field can utilise the index to increase performance. This allows for a binary search of the index with an average of log2 277778 = 18.08 = 19 block accesses. To find the address of the actual record, which requires a further block access to read, bringing the total to 19 + 1 = 20 block accesses, a far cry from the 277,778 block accesses required by the non-indexed table.

When should it be used?

Given that creating an index requires additional disk space (277,778 blocks extra from the above example), and that too many indexes can cause issues arising from the file systems size limits, careful thought must be used to select the correct fields to index.

Since indexes are only used to speed up the searching for a matching field within the records, it stands to reason that indexing fields used only for output would be simply a waste of disk space and processing time when doing an insert or delete operation, and thus should be avoided. Also given the nature of a binary search, the cardinality or uniqueness of the data is important. Indexing on a field with a cardinality of 2 would split the data in half, whereas a cardinality of 1,000 would return approximately 1,000 records. With such a low cardinality the effectiveness is reduced to a linear sort, and the query optimizer will avoid using the index if the cardinality is less than 30% of the record number, effectively making the index a waste of space.


In French, but you should be able to find the download link :)
PHP Naive Bayesian Filter

The following is SQL92 standard so should be supported by the majority of RDMBS that use SQL:

CREATE INDEX [index name] ON [table name] ( [column name] )

A dictionary may also contain overhead, depending on the actual implementation. A hashtable usually contain some prime number of available nodes to begin with, even though you might only use a couple of the nodes.

Judging by your example, "Property", would you be better of with a class approach for the final level and real properties? Or is the names of the properties changing a lot from node to node?

I'd say that what "efficient" means depends on a lot of things, like:


speed of updates (insert, update, delete)
speed of random access retrieval
speed of sequential retrieval
memory used


I think that you'll find that a data structure that is speedy will generally consume more memory than one that is slow. This isn't always the case, but most data structures seems to follow this.

A dictionary might be easy to use, and give you relatively uniformly fast access, it will most likely use more memory than, as you suggest, lists. Lists, however, generally tend to contain more overhead when you insert data into it, unless they preallocate X nodes, in which they will again use more memory.

My suggestion, in general, would be to just use the method that seems the most natural to you, and then do a "stress test" of the system, adding a substantial amount of data to it and see if it becomes a problem.

You might also consider adding a layer of abstraction to your system, so that you don't have to change the programming interface if you later on need to change the internal data structure.

The trick to that is to use URL rewriting so that name.domain.com transparently maps to something like domain.com/users/name on your server.  Once you start down that path, it's fairly trivial to implement.

Well, you didn't specify Rails, so I'm going to throw Shoes out there. First, building shoes apps is probably the best way to learn Ruby (Rails is great, but I find mastering Ruby far more fun/useful). Secondly, while I certainly don't think building crossplatform UI components is trivial, shoes is relatively new, and relatively small. There are no doubt countless additions that could be made.


Don't worry about DNS and URL rewriting

Your DNS record will be static, something like:

*.YOURDOMAIN.COM A 123.123.123.123

Ask your DNS provider to do it for you (if it's not done already) or do it by yourself if you have control over your DNS records. This will automatically point all your subdomains (current and future ones) into the same HTTP server.

Once it's done, you will only need to parse HOST header on every single http request to detect what hostname was used to access your server-side scripts on your http server.

Assuming you're using ASP.NET, this is kind of silly example I came up with but works and demonstrates simplicity of this approach:

&lt;%@ Language="C#" %&gt;&lt;%string subDomain = Request.Url.Host.Split('.')[0].ToUpper();if (subDomain == "CLIENTXXX") Response.Write("Hello CLIENTXXX, your secret number is 33");else if (subDomain == "CLIENTYYY") Response.Write("Hello CLIENTYYY, your secret number is 44");else Response.Write(subDomain+" doesn't exist");%&gt;

Making a class-based structure would probably have more overhead than the dict-based structure, since in python classes actually use dicts when they are implemented.

C and C++ compilers will generate a warning when you compare signed and unsigned types; in your example code, you couldn't make your loop variable unsigned and have the compiler generate code without warnings (assuming said warnings were turned on).

Naturally, you're compiling with warnings turned all the way up, right?

And, have you considered compiling with "treat warnings as errors" to take it that one step further?

The downside with using signed numbers is that there's a temptation to overload them so that, for example, the values 0-&gt;n are the menu selection, and -1 means nothing's selected - rather than creating a class that has two variables, one to indicate if something is selected and another to store what that selection is.  Before you know it, you're testing for negative one all over the place and the compiler is complaining about how you're wanting to compare the menu selection against the number of menu selections you have - but that's dangerous because they're different types.  So don't do that.

The way we do this is to have a 'catch all' for our domain name registered in DNS so that anything.ourdomain.com will point to our server.

With Apache you can set up a similar catch-all for your vhosts.  The ServerName must be a single static name but the ServerAlias directive can contain a pattern.

Servername www.ourdomain.com
ServerAlias *.ourdomain.com


Now all of the domains will trigger the vhost for our project.  The final part is to decode the domain name actually used so that you can work out the username in your code, something like (PHP):

list( $username ) = explode( ".", $_SERVER[ "HTTP_HOST" ] );


or a RewriteRule as already suggested that silently maps user.ourdomain.com/foo/bar to www.ourdomain.com/foo/bar?user=user or whatever you prefer.


When I needed to write an application with a client-server model where the clients could leave and enter whenever they want, (I assume that's also the case for your application as you use mobile devices) I made sure that the clients send an online message to the server, indicating they were connected and ready to do whatever they needed doing.

at that time the server could send messages back to the client trough the same open connection.

Also, but I don't know if that is applicable for you, I had some sort of heartbeat the clients sent to the server, letting it know it was still online. That way the server knows when a client was forcibly disconnected from the network and it could mark that client back as offline.

Why wouldn't you just use an array with a size of 2?  A Queue is supposed to be able to dynamically grow and shrink.

Or create a wrapper class around an instance of Queue&lt;T&gt; instance and each time one enqueues a &lt;T&gt; object, check the size of the queue.  If larger than 2, dequeue the first item.


You should create your own class, a ringbuffer would probably fit your needs.

The data structures in .NET that allows you to specify capacity, except for array, uses this to build the internal data structure used to hold the internal data.

For instance, for a list, capacity is used to size an internal array. When you start adding elements to the list, it'll start filling this array from index 0 and up, and when it reaches your capacity, it increases the capacity to a new higher capacity, and continues filling it up.

I've knocked up a basic version of what I'm looking for, it's not perfect but it'll do the job until something better comes along.

public class LimitedQueue&lt;T&gt; : Queue&lt;T&gt;{    private int limit = -1;    public int Limit    {        get { return limit; }        set { limit = value; }    }    public LimitedQueue(int limit)        : base(limit)    {        this.Limit = limit;    }    public new void Enqueue(T item)    {        if (this.Count &gt;= this.Limit)        {            this.Dequeue();        }        base.Enqueue(item);    }}

You might be able to get some implementation ideas from the Ruby/ProgressBar library, which generates text progress bars. I stumbled across it a couple of months back but haven't made any use of it.

In my opinion it is more a question of personal preference.  nAnt is a great framework and MSBuild is almost as capable.  With the ability to easily develop custom tasks (in both frameworks) you can accomplish almost anything that you need to do.

I cannot answer the "still supported" portion of your questions, but I would say if you are already comfortable with nAnt then it's probably viable.  If you (or someone in your group) is familiar with MSBuild then that is a fine way to go as well.

Honestly it depends on what fits in to your environment better.  If you are using a lot of Non-Microsoft tools, nunit, ccnet, ncover.  You will probably find better support with nant.  Alternatively if you are using MSTest, TFSBuild, you will probably find MSBuild a better environment.  I would learn both and use which every fits more smoothly with your environment.

If you've already got a bunch of custom tasks you use with nAnt, stick with it - you don't gain much with MSBuild.  That said, there doesn't seem to be anything that nAnt can do that MSBuild can't at its core.  Both can call external tools, both can run .Net-based custom tasks, and both have a bunch of community tasks out there.

We're using MSBuild here for the same reason you are - it's the default build system for VS now, and we didn't have any nAnt-specific stuff to worry about.

The MSBuildCommunityTasks are a good third-party task base to start with, and covers most of the custom stuff I ever did in nAnt, including VSS and Subversion support.

On windows, curses works out of the box, ncurses doesn't, and for a progress bar curses should be sufficient. So, use curses instead of ncurses.

Also, both curses and ncurses are wafer-thin wrappers around the c library - that means you don't really need Ruby-specific tutorials.

However, on the site for the PickAxe you can download all the code examples for the book. The file "ex1423.rb" contains a curses demo which plays Pong - that should give you plenty of material to get you going.


Personally I think curses is overkill in this case.  While the curses lib is nice (and I frequently use it myself) it's a PITA to relearn every time I haven't needed it for 12 months which has to be the sign of a bad interface design.

If for some reason you can't get on with the progress bar lib Joey suggested roll your own and release it under a pretty free licence for instant kudos :)


Here is a solution that works in XP / Vista, but is definitely expandable to OSX, linux, I'd still be interested in another way.

public static function GetCurrentOSUser():String{
    // XP &amp; Vista only.
    var userDirectory:String = File.userDirectory.resolvePath("").nativePath;
    var startIndex:Number = userDirectory.lastIndexOf("\\") + 1
    var stopIndex:Number = userDirectory.length;
    var user = userDirectory.substring(startIndex, stopIndex);

    return user;
}



Not sure how credible this source is, but:


  The Windows Server 2008 Core edition can:
  
  
  Run the file server role.
  Run the Hyper-V virtualization server role.
  Run the Directory Services role.
  Run the DHCP server role.
  Run the IIS Web server role.
  Run the DNS server role.
  Run Active Directory Lightweight Directory Services.
  Run the print server role.
  
  
  The Windows Server 2008 Core edition cannot:
  
  
  Run a SQL Server.
  Run an Exchange Server.
  Run Internet Explorer.
  Run Windows Explorer.
  Host a remote desktop session.
  Run MMC snap-in consoles locally.
  



Also I would try:

File.userDirectory.name

But I don't have Air installed so I can't really test this...

There are some parallel extensions to .NET that are currently in testing and available at Microsoft's Parallel Computing Developer Center. They have a few interesting items that you would expect like Parallel foreach and a parallel version of LINQ called PLINQ. Some of the best information about the extensions is on Channel 9.

When you're using .Net 2.0 and Ajax - you should use:

ScriptManager.RegisterClientScriptBlock


It will work better in Ajax environments then the old Page.ClientScript version


You can set Apache to serve pages from anywhere with any restrictions but it's normally distributed in a more secure form.

Editing your apache files (http.conf is one of the more common names) will allow you to set any folder so it appears in your webroot.

EDIT:

alias myapp c:\myapp\

I've edited my answer to include the format for creating an alias in the http.conf file which is sort of like a shortcut in windows or a symlink under un*x where Apache 'pretends' a folder is in the webroot.  This is probably going to be more useful to you in the long term.

You can relocate it by editing the DocumentRoot setting in XAMPP\apache\conf\httpd.conf.

It should currently be:


  C:/xampp/htdocs


Change it to:


  C:/projects/transitCalculator/trunk


Try changing the argument names to "sender" and "args".  And, after you have it working, switch the call over to ScriptManager.RegisterClientScriptBlock, regardless of AJAX use.

Ok, per pix0r's, Sparks' and Dave's answers it looks like there are three ways to do this:



Virtual Hosts


Open C:\xampp\apache\conf\extra\httpd-vhosts.conf.
Un-comment line 19 (NameVirtualHost *:80).
Add your virtual host (~line 36):

&lt;VirtualHost *:80&gt;
    DocumentRoot C:\Projects\transitCalculator\trunk
    ServerName transitcalculator.localhost
    &lt;Directory C:\Projects\transitCalculator\trunk&gt;
        Order allow,deny
        Allow from all
    &lt;/Directory&gt;
&lt;/VirtualHost&gt;

Open your hosts file (C:\Windows\System32\drivers\etc\hosts).
Add

127.0.0.1 transitcalculator.localhost #transitCalculator


to the end of the file (before the Spybot - Search &amp; Destroy stuff if you have that installed).
Save (You might have to save it to the desktop, change the permissions on the old hosts file (right click > properties), and copy the new one into the directory over the old one (or rename the old one) if you are using Vista and have trouble).
Restart Apache.


Now you can access that directory by browsing to http://transitcalculator.localhost/.



Make an Alias


Starting ~line 200 of your http.conf file, copy everything between &lt;Directory "C:/xampp/htdocs"&gt; and &lt;/Directory&gt; (~line 232) and paste it immediately below with C:/xampp/htdocs replaced with your desired directory (in this case C:/Projects) to give your server the correct permissions for the new directory.
Find the &lt;IfModule alias_module&gt;&lt;/IfModule&gt; section (~line 300) and add

Alias /transitCalculator "C:/Projects/transitCalculator/trunk"


(or whatever is relevant to your desires) below the Alias comment block, inside the module tags.




Change your document root


Edit ~line 176 in C:\xampp\apache\conf\httpd.conf; change DocumentRoot "C:/xampp/htdocs" to #DocumentRoot "C:/Projects" (or whatever you want).
Edit ~line 203 to match your new location (in this case C:/Projects).




Notes: 


You have to use forward slashes "/" instead of back slashes "\".
Don't include the trailing "/" at the end.
restart your server.



I think if you are attempting to learn a new language do something fun in that language.

I learned python by writing lots of web spiders and little toys and for ruby I'd take exactly the same path.  Instead of finding a project that needs input do a couple of little personal projects to get a feel for the language.  You learn more by doing something then by reading lots of examples and other peoples code in those first few months.

A language like Ruby is structured in such a way you can do something productive straight out of the box without much support so jump right in and do something fun rather than think you have to do something for somebody else right at the beginning of your experimentation with a new language.

have fun with Ruby, it's on my short list of things to play with :)

I think we could also include non-.NET-specific approaches to parallel processing if those are among the best options to consider.

Or you could start throwing critical exceptions. Surely, an access violation exception will catch your users' attention.

@Larsenal

If you want to branch outside of .NET there has been a lot of discussion about Intel's Threading Building Blocks which is a parallel library for C++.

Outside the scope of your question so I debated not posting this but in Java there are actually 2 types of exceptions, checked and unchecked. The basic difference is that, much like in c[++], you dont have to catch an unchecked exception.

For a good reference try this 


This isn't much better:

public function createShipment($startZip, $endZip, $weight=null){    $weight = !$weight ? $this-&gt;getDefaultWeight() : $weight;}// or...public function createShipment($startZip, $endZip, $weight=null){    if ( !$weight )        $weight = $this-&gt;getDefaultWeight();}

The way I typically go about coding permission systems is having 6 tables.


Users - this is pretty straight forward it is your typical users table
Groups - this would be synonymous to your departments
Roles - this is a table with all permissions generally also including a human readable name and a description
Users_have_Groups - this is a many-to-many table of what groups a user belongs to
Users_have_Roles - another many-to-many table of what roles are assigned to an individual user
Groups_have_Roles - the final many-to-many table of what roles each group has


At the beginning of a users session you would run some logic that pulls out every role they have assigned, either directory or through a group. Then you code against those roles as your security permissions.

Like I said this is what I typically do but your millage may vary.

An approach I've used in various applications is to have a generic PermissionToken class which has a changeable Value property.  Then you query the requested application, it tells you which PermissionTokens are needed in order to use it.

For example, the Shipping application might tell you it needs:

new PermissionToken(){    Target = PermissionTokenTarget.Application,    Action = PermissionTokenAction.View,    Value = "ShippingApp"};

This can obviously be extended to Create, Edit, Delete etc and, because of the custom Value property, any application, module or widget can define its own required permissions.  YMMV, but this has always been an efficient method for me which I have found to scale well.


  Is there a way one can ensure that the
  exceptions thrown are always caught
  using try/catch by the calling
  function?


I find it rather funny, that the Java crowd - including myself - is trying to avoid checked Exceptions. They are trying to work their way around being forced to catch Exceptions by using RuntimeExceptions.

I agree with John Downey.

Personally, I sometimes use a flagged enumeration of permissions. This way you can use AND, OR, NOT and XOR bitwise operations on the enumeration's items.

"[Flags]public enum Permission{    VIEWUSERS = 1, // 2^0 // 0000 0001    EDITUSERS = 2, // 2^1 // 0000 0010    VIEWPRODUCTS = 4, // 2^2 // 0000 0100    EDITPRODUCTS = 8, // 2^3 // 0000 1000    VIEWCLIENTS = 16, // 2^4 // 0001 0000    EDITCLIENTS = 32, // 2^5 // 0010 0000    DELETECLIENTS = 64, // 2^6 // 0100 0000}"

Then, you can combine several permissions using the AND bitwise operator. 
For example, if a user can view &amp; edit users, the binary result of the operation is 0000 0011 which converted to decimal is 3. 
You can then store the permission of one user into a single column of your DataBase (in our case it would be 3).
Inside your application, you just need another bitwise operation (OR) to verify if a user has a particular permission or not. 

As far as I can tell Python, up through 2.5, only supports hexadecimal &amp; octal literals.  I did find some discussions about adding binary to future versions but nothing definite.

I am pretty sure this is one of the things due to change in Python 3.0 with perhaps bin() to go with hex() and oct().

EDIT:
lbrandy's answer is correct in all cases.


&gt;&gt;&gt; print int('01010101111',2)
687
&gt;&gt;&gt; print int('11111111',2)
255


Another way.


In addition to John Downey and jdecuyper's solutions, I've also added an "Explicit Deny" bit at the end/beginning of the bitfield, so that you can perform additive permissions by group, role membership, and then subtract permissions based upon explicit deny entries, much like NTFS works, permission-wise.

I have to agree with the OP 'wrong' dates really jar with my DD/MM/YYYY upbringing and I find ISO 8601 dates and times extremely easy to work with.  For once the standard got it right and engtech has the obvious answer that doesn't require localisation.

I was going to report the birthday input form on stack overflow as a bug because of how much of a sore thumb it is to the majority of the world.

From the Apple Developer Connection Safari Web Content Guide for iPhone page Specifying a Webpage Icon for Web Clip...


  The user can add a web application or
  webpage link to the Home screen. These
  links, represented by an icon, are
  called web clips. Follow these simple
  steps to specify an icon to represent
  your web application or webpage on
  iPhone.
  
  To specify an icon for the entire
  website (every page on the website),
  place an icon file in PNG format in
  the root document folder called
  apple-touch-icon.png.
  
  To specify an icon for a single
  webpage, or replace the website icon
  with a webpage-specific icon, add a
  link element to the webpage as in:
  
  &lt;link rel="apple-touch-icon" href="/custom_icon.png"/&gt;
  
  In the above example, replace
  custom_icon.png with your icon
  filename.
  
  See "Create an Icon for Your Web
  Application or Webpage" in iPhone
  Human Interface Guidelines in iPhone
  Human Interface Guidelines for webpage
  icon metrics.
  
  Note: The web clip feature is
  available in iPhone 1.1.3 and later.


And for the sake of completeness, a link to Scott Hanselman's posting, which contains some additional tips as well:

Add Home Screen iPhone Icons and Adjust the ViewPort

You should be able to check out the current version of the code and then create a git repository around it. Updating that and committing it to your local git repository should be painless. As should cloning it.

The only catch is that you need to have them both ignore each other (I've done something similar with SVN) by messing with the appropriate ignore files. I'm presuming SourceSafe let's you ignore things. And you'll need to do certain operations twice (like telling both that you are deleting a file).

I've made a suggestion that Stack Overflow implement an apple-touch-icon:

Add an apple-touch-icon for Safari on iPhone

A short form is convenient and helps avoid spelling mistakes. Localize as applicable, but be sure to display the expected format (do not leave the user blind). Provide a date-picker control as an optional aide to filling in the field.

As an extra, on-the-fly parsing and display of the date in long form might help too.

Well... KernelTrap has something on this. Looks like you can use vss2svn to pipe the Source Safe repo into a Subversion repository, then use the very nice git-svn to pull into a local git repo.
I would assume the commits back to VSS would not be a smooth, automatic process using this method.

While I would still like an answer to why my JS wasn't being recognized, the solution I found in the meantime (and should have done in the first place) is to use an Asp:CompareValidator instead of an Asp:CustomValidator.

While you cannot prevent usage of those inherited members to my knowledge, you should be able to hide them from IntelliSense using the EditorBrowsableAttribute:

Using System.ComponentModel;

[EditorBrowsable(EditorBrowsableState.Never)]
private string MyHiddenString = "Muahahahahahahahaha";


Edit: Just saw this in the documentation comments, which makes it kinda useless for this purpose:


  There is a prominent note that states that this attribute "does not suppress members from a class in the same assembly". That is true but not complete. Actually, the attribute does not suppress members from a class in the same solution.



I think you're best least hackish way is to consider composition as opposed to inheritance.

Or, you could create an interface that has the members you want, have your derived class implement that interface, and program against the interface.

One potential thing you can do is contain the object rather than extend from the other class.  This will give you the most flexibility in terms of exposing what you want to expose, but if you absolutely need the object to be of that type it is not the ideal solution (however you could expose the object from a getter).

Thus:

public class MyClass : BaseClass{    // Your stuff here}

Becomes:

public class MyClass{    private BaseClass baseClass;    public void ExposeThisMethod()    {        baseClass.ExposeThisMethod();    }}

Or:

public class MyClass{    private BaseClass baseClass;    public BaseClass BaseClass    {        get        {            return baseClass;        }    }}

Some time ago I wrote a quick and dirty exe that would update the version #'s in an assemblyinfo.{cs/vb} - I also have used rxfind.exe (a simple and powerful regex-based search replace tool) to do the update from a command line as part of the build process.  A couple of other helpfule hints:


separate the assemblyinfo into product parts (company name, version, etc.) and assembly specific parts (assembly name etc.).  See here
Also - i use subversion, so I found it helpful to set the build number to subversion revision number thereby making it really easy to always get back to the codebase that generated the assembly (e.g. 1.4.100.1502 was built from revision 1502).


Just wondering, what do you feel your method leaves to be desired?  You could replace the anonymous delegate with a.. named? delegate, something like

    public delegate void IoOperation(params string[] parameters);    public void FileDeleteOperation(params string[] fileName)    {        File.Delete(fileName[0]);    }    public void FileCopyOperation(params string[] fileNames)    {        File.Copy(fileNames[0], fileNames[1]);    }    public void RetryFileIO(IoOperation operation, params string[] parameters)    {        RetryTimer fileIORetryTimer = new RetryTimer(TimeSpan.FromHours(10));        bool success = false;        while (!success)        {            try            {                operation(parameters);                success = true;            }            catch (IOException e)            {                if (fileIORetryTimer.HasExceededRetryTimeout)                {                    throw;                }                fileIORetryTimer.SleepUntilNextRetry();            }        }    }    public void Foo()    {        this.RetryFileIO(FileDeleteOperation, "L:\file.to.delete" );        this.RetryFileIO(FileCopyOperation, "L:\file.to.copy.source", "L:\file.to.copy.destination" );    }

Override them like Michael Suggests above and to prevent folks from using the overridden (sp?) methods, mark them as obsolete:

[Obsolete("These are not supported in this class.", true)]public override  void dontcallmeanymore(){}

If the second parm is set to true, a compiler error will be generated if anyone tries to  call that method and the string in the first parm is the message.  If parm2 is false only a compiler warning will be generated.

I assume you don't mean cygwin, right?

How about powershell, then?


Are you asking about Linux shell as in an environment to work in? For that CygWin I think has been around the longest and is pretty robust: http://www.cygwin.com/

A while ago I found a windows port of all the popular linux commands I use (ls, grep, diff) and I simply unzip those to a file, add it to my PATH environment and then can run from there: http://unxutils.sourceforge.net/

Or are you talking about executing shell commands from within your code? If you're in the .net sphere, there is the Process.Start() method that will give you a lot of options.

Hope this helps!

If you're referring to simply accessing your IIS server from a remote location, remote desktop generally solves that problem.  Assuming your server has a static IP address or a host name you can access from the internet, remote desktop is a simple and relatively secure solution.

Is there a problem with this answer?  Now I have negative reputation...

The best way I can think of would be to use Cygwin over an OpenSSH connection.
Here's a document that explains how to do just that: 

http://www.ucl.ac.uk/cert/openssh_rdp_vnc.pdf

It is possible to skip the step of creating the empty database. You can create the new database as part of the restore process.

This is actually the easiest and best way I know of to clone a database. You can eliminate errors by scripting the backup and restore process rather than running it through the SQL Server Management Studio

There are two other options you could explore:


Detach the database, copy the .mdf file and re-attach.
Use SQL Server Integration Services (SSIS) to copy all the objects over


I suggest sticking with backup and restore and automating if necessary.

MySQLdb is what I have used before.

If you host is using Python version 2.5 or higher, support for sqlite3 databases is built in (sqlite allows you to have a relational database that is simply a file in your filesystem).  But buyer beware, sqlite is not suited for production, so it may depend what you are trying to do with it.

Another option may be to call your host and complain, or change hosts.  Honestly these days, any self respecting web host that supports python and mysql ought to have MySQLdb pre installed.

I usually do a project for the GUI a project for the business logic a project for data access and a project for unit tests.

But sometimes it is prudent to have separation based upon services (if you are using a service oriented architecture) Such as Authentication, Sales, etc. 

I guess the rule of thumb that I work off of is that if you can see it as a component that has a clear separation of concerns then a different project could be prudent. But I would think that folders versus projects could just be a preference or philosophy.

I personally feel that if reusable code is split into projects it is simpler to use other places than if it is just in folders.

Here's a dynamic sql script I've used in the past.  It can be further modified but it will give you the basics.  I prefer scripting it to avoid the mistakes you can make using the Management Studio:

Declare @OldDB varchar(100)Declare @NewDB varchar(100)Declare @vchBackupPath varchar(255)Declare @query varchar(8000)/*Test code to implement Select @OldDB = 'Pubs'Select @NewDB = 'Pubs2'Select @vchBackupPath = '\\dbserver\C$\Program Files\Microsoft SQL Server\MSSQL.1\MSSQL\Backup\pubs.bak'*/SET NOCOUNT ON;Select @query = 'Create Database ' + @NewDBexec(@query)Select @query = 'Declare @vBAKPath varchar(256)declare @oldMDFName varchar(100)declare @oldLDFName varchar(100)declare @newMDFPath varchar(100)declare @newLDFPath varchar(100)declare @restQuery varchar(800)select @vBAKPath = ''' + @vchBackupPath + '''select @oldLDFName = name from ' + @OldDB +'.dbo.sysfiles where filename like ''%.ldf%''select @oldMDFName = name from  ' + @OldDB +'.dbo.sysfiles where filename like ''%.mdf%''select @newMDFPath = physical_name from ' + @NewDB +'.sys.database_files where type_desc = ''ROWS''select @newLDFPath = physical_name from ' + @NewDB +'.sys.database_files where type_desc = ''LOG''select @restQuery = ''RESTORE DATABASE ' + @NewDB + ' FROM DISK = N'' + '''''''' + @vBAKpath + '''''''' + '' WITH MOVE N'' + '''''''' + @oldMDFName + '''''''' +  '' TO N'' + '''''''' + @newMDFPath + '''''''' +  '', MOVE N'' + '''''''' + @oldLDFName + '''''''' +  '' TO N'' + '''''''' + @newLDFPath + '''''''' +  '', NOUNLOAD, REPLACE, STATS = 10''exec(@restQuery)--print @restQuery'exec(@query)

denny wrote:


  I personally feel that if reusable code is split into projects it is simpler to use other places than if it is just in folders.


I really agree with this - if you can reuse it, it should be in a separate project.  With that said, it's also very difficult to reuse effectively :)

Here at SO, we've tried to be very simple with three projects:


MVC Web project (which does a nice job of separating your layers into folders by default)
Database project for source control of our DB
Unit tests against MVC models/controllers


I can't speak for everyone, but I'm happy with how simple we've kept it - really speeds the builds along!

By default, always just create new folder within the same project


You will get single assembly (without additional ILMerge gymnastic)
Easier to obfuscate (because you will have less public types and methods, ideally none at all)


Separating your source code into multiple projects makes only sense if you...


Have some portions of the source code that are part of the project but not deployable by default or at all (unit tests, extra plugins etc.)
More developers involved and you want to treat their work as consumable black box. (not very recommended)
If you can clearly separate your project into isolated layers/modules and you want to make sure that they can't cross-consume internal members. (also not recommended because you will need to decide which aspect is the most important)


If you think that some portions of your source code could be reusable, still don't create it as a new project. Just wait until you will really want to reuse it in another solution and isolate it out of original project as needed. Programming is not a lego, reusing is usually very difficult and often won't happen as planned.

If you are quite happy with MSBuild, then I would stick with MSBuild.  This may be one of those cases where the tool you learn first is the one you will prefer.  I started with NAnt and can't quite get used to MSBuild.  I'm sure they will both be around for quite some time.

There are some fundamental differences between the two, probably best highlighted by this conversation between some NAnt fans and a Microsoftie.

Interestingly, Jeremy Miller asked the exact opposite question on his blog last year.  

I'm not necessarily looking for the fastest but a nice balance, some of the server that this code is being developed for are fairly slow, the script that hashes and stores the password is taking 5-6 seconds to run, and I've narrowed it down to the hashing (if I comment the hashing out it runs, in 1-2 seconds).

It doesn't have to be the MOST secure, I'm not codding for a bank (right now) but I certainly WILL NOT store the passwords as plain-text.

In my experience its best to keep an the info in the profile to a bare minimum, only put the essentials in there that are directly needed for authentication. Other information such as addresses should be saved in your own database by your own application logic, this approach is more extensible and maintainable.

I think that depends on how many fields you need. To my knowledge, Profiles are essentially a long string that gets split at the given field sizes, which means that they do not scale very well if you have many fields and users.

On the other hand, they are built in, so it's an easy and standardized way, which means there is not a big learning curve and you can use it in future apps as well without needing to tweak it to a new table structure.

Rolling your own thing allows you to put it in a properly normalized database, which drastically improves performance, but you have to write pretty much all the profile managing code yourself.

Edit: Also, Profiles are not cached, so every access to a profile goes to the database first (it's then cached for that request, but the next request will get it from the database again)

If you're thinking about writing your own thing, maybe a custom Profile Provider gives you the best of both worlds - seamless integration, yet the custom stuff you want to do.

The active projects on Rubyforge are a great place to start.  What would be a good starter project is to pick one that is pretty popular but not a lot of developers.

If you are interested in Ruby on Rails, I'm working on Redmine right now.  It's been one of the most active projects and only has 5 developers.  Open Source Rails also has a good collection of projects.

I've found doing a Refactotum a great way to get started on a project.  Use the fact that you are new to your advantage, most people who have been on a project forget about simple things like gem dependencies and documentation


This website might help you out a bit more.  Also this one.

I'm working from a fairly rusty memory of a statistics course, but here goes nothing:

When you're doing analysis of variance (ANOVA), you actually calculate the F statistic as the ratio from the mean-square variances "between the groups" and the mean-square variances "within the groups".  The second link above seems pretty good for this calculation.

This makes the F statistic measure exactly how powerful your model is, because the "between the groups" variance is explanatory power, and "within the groups" variance is random error.  High F implies a highly significant model.

As in many statistical operations, you back-determine Sig. using the F statistic.  Here's where your Wikipedia information comes in slightly handy.  What you want to do is - using the degrees of freedom given to you by SPSS - find the proper P value at which an F table will give you the F statistic you calculated.  The P value where this happens [F(table) = F(calculated)] is the significance.

Conceptually, a lower significance value shows a very strong ability to reject the null hypothesis (which for these purposes means to determine your model has explanatory power).

Sorry to any math folks if any of this is wrong.  I'll be checking back to make edits!!!

Good luck to you.  Stats is fun, just maybe not this part.  =)

You need to put the match in parentheses, like this:

preg_replace("/([A-Z])/", "&lt;span class=\"initial\"&gt;$1&lt;/span&gt;", $str)


The value of 'false' is true. You need to remove the quotes:

$canMerge = ($condition1 &amp;&amp; $condition2) ? true : false;

This is broken because 'false' as a string will evaluate to true as a boolean.

However, this is an unneeded ternary expression, because the resulting values are simple true and false.  This would be equivalent:

$canMerge = ($condition1 &amp;&amp; $condition2);

The exact definitions of "vsize," "rss," "rprvt," "rshrd," and other obscure-looking abbreviations vary from OS to OS.  The manual pages for the "top" and "ps" commands will have some sort of description, but all such descriptions are simplified greatly (or are based on long-extinct kernel implementations).  "Process size" as a concept is fiendishly difficult to pin down in the general case.  Answers in specific instances depend heavily on the actual memory management implementation in the OS, and are rarely as satisfying as the tidy "process size" concept that exists in the minds of most users (and most developers).

For example, none of those numbers (nor, likely, any combination of them) can be used to tell you exactly how many such processes can run at once in a given amount of free memory.  But really, your best bet is to come at it from that end: why do you want this number, and what will you use it for?  Given that information, I think you'll get more useful answers.

The Publish to Provider functionality has worked great for me.  See Scott Gu's Blog Entry.

If you need something really robust look  at redgate software's tools here...if you are doing much SQL at all, these are worth the $$.

If you are looking for user facing interaction, stick with xml. It has more support, understanding, and general acceptance currently. If it's internal, I would say that protocol buffers are a great idea.
Maybe in a few years as more tools come out to support protocol buffers, then start looking towards that for a public facing api. Until then... JSON?

Backup and Restore is the most straight-forward way I know.  You have to be careful between servers as security credentials don't come with the restored database.

Sounds like you might like Git. There's a Google Talk explaining all about it.

From the preg_replace documentation on php.net:


  replacement  may contain references of
  the form \n or (since PHP 4.0.4) $n,
  with the latter form being the
  preferred one. Every such reference
  will be replaced by the text captured
  by the n'th parenthesized pattern.


See Flubba's example.

Its probably not exactly what your looking for, but you may be able to implement OS level clustering.

On Linux:


set raw mode
select and read the keystroke
restore normal settings



import sys
import select
import termios
import tty

def getkey():
    old_settings = termios.tcgetattr(sys.stdin)
    tty.setraw(sys.stdin.fileno())
    select.select([sys.stdin], [], [], 0)
    answer = sys.stdin.read(1)
    termios.tcsetattr(sys.stdin, termios.TCSADRAIN, old_settings)
    return answer

print """Menu
1) Say Foo
2) Say Bar"""

answer=getkey()

if "1" in answer: print "foo"
elif "2" in answer: print "bar"




for C# 3.5

without notifyicon therefore you will need to terminate this application in task manager manually

using System;using System.Drawing;using System.Windows.Forms;static class Program{    static void Main()    {        Timer timer = new Timer();        // timer.Interval = 4 minutes        timer.Interval = (int)(TimeSpan.TicksPerMinute * 4 / TimeSpan.TicksPerMillisecond);        timer.Tick += (sender, args) =&gt; { Cursor.Position = new Point(Cursor.Position.X + 1, Cursor.Position.Y + 1); };        timer.Start();        Application.Run();    }}

Something like this should work (though, you will want to change the interval).

public Form1()
{
    InitializeComponent();
    Timer Every4Minutes = new Timer();
    Every4Minutes.Interval = 10;
    Every4Minutes.Tick += new EventHandler(MoveNow);
    Every4Minutes.Start();
}

void MoveNow(object sender, EventArgs e)
{
    Cursor.Position = new Point(Cursor.Position.X - 1, Cursor.Position.Y - 1);
}



On Windows:

import msvcrt
answer=msvcrt.getch()



This is an active area of research. There are literally oodles of academic papers on the subject. It's going to be difficult to give you assistance especially w/o more deatails. Are you looking for specific types of text? Fonts? English-only? Are you familiar with the academic literature? 

"Text detection" is a standard problem in any OCR (optical character recognition) system and consequently there are lots of bits of code on the interwebs that deal with it. 

I could start listing piles of links from google but I suggest you just do a search for "text detection" and start reading :). There is ample example code available as well.

Wow, that took forever. Ok, here's what I've ended up with 

#!C:\python25\python.exeimport msvcrtprint """Menu1) Say Foo 2) Say Bar"""while 1:    char = msvcrt.getch()    if char == chr(27): #escape        break    if char == "1":        print "foo"        break    if char == "2":        print "Bar"        break

It fails hard using IDLE, the python...thing...that comes with python. But once I tried it in DOS (er, CMD.exe), as a real program, then it ran fine.

No one try it in IDLE, unless you have Task Manager handy.

I've already forgotten how I lived with menus that arn't super-instant responsive.


  Without the extra quotes around the input string parameter, the Javascript function thinks I'm passing in an integer.


Can you do some rudimentary string function to force JavaScript into changing it into a string? Like 

value = value + ""

Try putting the extra text inside the server-side script block and concatenating.

onclick='&lt;%# "ToggleDisplay(""" &amp;  DataBinder.Eval(Container.DataItem, "JobCode") &amp; """);" %&gt;'

Edit: I'm pretty sure you could just use double quotes outside the script block as well.

The  reason msvcrt fails in IDLE is because IDLE is not accessing the library that runs msvcrt. Whereas when you run the program natively in cmd.exe it works nicely. For the same reason that your program blows up on Mac and Linux terminals.

But I guess if you're going to be using this specifically for windows, more power to ya.

I had recently similar problem and the only way to solve it was to use plain old HTML codes for single (&amp;#39;) and double quotes (&amp;#34;).  

Source code was total mess of course but it worked.

Try

&lt;a id="aShowHide" onclick='ToggleDisplay(&amp;#34;&lt;%# DataBinder.Eval(Container.DataItem, "JobCode") %&gt;&amp;#34;);'&gt;Show/Hide&lt;/a&gt;


or

&lt;a id="aShowHide" onclick='ToggleDisplay(&amp;#39;&lt;%# DataBinder.Eval(Container.DataItem, "JobCode") %&gt;&amp;#39;);'&gt;Show/Hide&lt;/a&gt;



Chris' probably has the best pure answer to the question:

However, I'm curious about the root of the question.  If the user should always wrap the call in a try/catch block, should the user-called function really be throwing exceptions in the first place?

This is a difficult question to answer without more context regarding the code-base in question.  Shooting from the hip, I think the best answer here is to wrap the function up such that the recommended (if not only, depending on the overall exception style of the code) public interface does the try/catch for the user.  If you're just trying to ensure that there are no unhandled exceptions in your code, unit tests and code review are probably the best solution.


the problem is that the sphere can be distorted a number of ways, and having all those points known on the equator, lets say, wont help you map points further away.

You need better 'close' points, then you can assume these three points are on a plane with the fourth and do the interpolation --knowing that the distance of longitudes is a function, not a constant.

Ummm.  Maybe I am missing something about the question here, but if you have long/lat info, you also have the direction of north?

It seems you need to map geodesic coordinates to a projected coordinates system. For example osgb to wgs84.

The maths involved is non-trivial, but the code comes out a only a few lines. If I had more time I'd post more but I need a shower so I will be boring and link to the wikipedia entry which is pretty good.

Note: Post shower edited.

I'm not clear on whether or not you're wanting to add the asynchronous bits to the server in C# or the client in C++.

If you're talking about doing this in C++, desktop Windows platforms can do socket I/O asynchronously through the API's that use overlapped I/O.  For sockets, WSASend, WSARecv both allow async I/O (read the documentation on their LPOVERLAPPED parameters, which you can populate with events that get set when the I/O completes).

I don't know if Windows Mobile platforms support these functions, so you might have to do some additional digging. 

The solution is to use the TempData property to store the desired Request components.

For instance:

public ActionResult Send()
{
    TempData["form"] = Request.Form;
    return this.RedirectToAction(a =&gt; a.Form());
}


Then in your "Form" action you can go:

public ActionResult Form()
{
    /* Declare viewData etc. */

    if (TempData["form"] != null)
    {
        /* Cast TempData["form"] to 
        System.Collections.Specialized.NameValueCollection 
        and use it */
    }

    return View("Form", viewData);
}



Are there any more specific details on the kind of distortion?  If, for example, your latitudes and longitudes are "distorted" onto your 2D map using a Mercator projection, the conversion math is readily available.

If the map is distorted truly arbitrarily, there are lots of things you could try, but the simplest would probably be to compute a weighted average from your existing point mappings.  Your weights could be the squared inverse of the x/y distance from your new point to each of your existing points.

Some pseudocode:

estimate-latitude-longitude (x, y)

    numerator-latitude := 0
    numerator-longitude := 0
    denominator := 0

    for each point,
        deltaX := x - point.x
        deltaY := y - point.y
        distSq := deltaX * deltaX + deltaY * deltaY
        weight := 1 / distSq

        numerator-latitude += weight * point.latitude
        numerator-longitude += weight * point.longitude
        denominator += weight

    return (numerator-latitude / denominator, numerator-longitude / denominator)


This code will give a relatively simple approximation.  If you can be more precise about the way the projection distorts the geographical coordinates, you can probably do much better.

Here is some sample code. I think this is what you are looking for. The following displays exactly the same in Firefox 3 (mac) and IE7.



#absdiv {
  position: absolute; 
  left: 100px; 
  top: 100px; 
  width: 80%; 
  height: 60%; 
  background: #999;
}

#pctchild {
  width: 60%; 
  height: 40%; 
  background: #CCC;
}

#reldiv {
  position: relative;
  left: 20px;
  top: 20px;
  height: 25px;
  width: 40%;
  background: red;
}
&lt;div id="absdiv"&gt;
    &lt;div id="reldiv"&gt;&lt;/div&gt;
    &lt;div id="pctchild"&gt;&lt;/div&gt;
&lt;/div&gt;





recognizing text inside an image is indeed a hot topic for researchers in that field, but only begun to grow out of control when captcha's became the "norm" in terms of defense against spam bots. Why use captcha's as protection? well because it is/was very hard to locate (and read) text inside an image!

The reason why I mention captcha's is because the most advancement* is made within that tiny area, and I think that your solution could be best found there.
especially because captcha's are indeed about locating text (or something that resembles text) inside a cluttered image and afterwards trying to read the letters correctly.

so if you can find yourself a good open source captcha breaking tool you probably have all you need to continue your quest... 
You could probably even throw away the most dificult code that handles the character recognition itself, because those OCR's are used to read distorted text, something you don't have to do.

*: advancement in terms of visible, usable, and practical information for a "non-researcher"

I can't think of any cross platform way of what you want (or any guaranteed way full stop) but as you are using GetTickCount perhaps you aren't interested in cross platform :)

I'd use interprocess communications and set the intensive processes nice levels to get what you require but I'm not sure that's appropriate for your situation.

EDIT:
I agree with Bernard which is why I think a process rather than a thread might be more appropriate but it just might not suit your purposes.

If the data modification is not too time consuming (meaning, if the main purpose of the background thread is not the actual data modification), try moving the section that modifies the data to a delegate and Invoke'ing that delegate.

If the actual heavy work is on the data, you'll probably have to create a deep copy of this data to pass to the background thread, which will send the processed data back to the UI thread via Invoke again.

You should be able to do something like:

if (control.InvokeRequired){    control.Invoke(delegateWithMyCode);}else{    delegateWithMyCode();}

InvokeRequired is a property on Controls to see if you are on the correct thread, then Invoke will invoke the delegate on the correct thread.

UPDATE: Actually, at my last job we did something like this:

private void SomeEventHandler(Object someParam){    if (this.InvokeRequired)    {        this.Invoke(new SomeEventHandlerDelegate(SomeEventHandler), someParam);    }    // Regular handling code}

which removes the need for the else block and kind of tightens up the code.

Would use double.TryParse, it has performance benefits.


As I don't have a test case to go from I can't guarantee this solution, but it seems to me that a scenario similar to the one used to update progress bars in different threads (use a delegate) would be suitable here.

public delegate void DataBindDelegate();public DataBindDelegate BindData = new DataBindDelegate(DoDataBind);public void DoDataBind(){    DataBind();}

If the data binding needs to be done by a particular thread, then let that thread do the work!

I'd personally use int.tryparse, then double.tryparse.  Performance on those methods is quite fast.  They both return a Boolean.  If both fail then you have a string, per how you defined your data. 

On linux, you can change the scheduling priority of a thread with nice().

I would say, don't worry so much about such micro performance.  It is much better to just get something to work, and then make it as clear and concise and easy to read as possible.  The worst thing you can do is sacrifice readability for an insignificant amount of performance.

In the end, the best way to deal with performance issues is to save them for when you have data that indicates there is an actual performance problem... otherwise you will spend a lot of time micro-optimizing and actually cause higher maintenance costs for later on.

If you find this parsing situation is really the bottleneck in your application, THEN is the time to try and figure out what the fastest way to solve the problem is.  I think Jeff (and many others) have blogged about this sort of thing a lot.

Re: 2004

No, you will only move the code that changes the data into the delegate function (because the data change is what triggers the control update). Other than that, you should not have to write anything extra.


If the thread call is "illegal" (i.e. the DataBind call affects controls that were not created in the thread it is being called from) then you need to create a delegate so that even if the decision / preparation for the DataBind is not done in the control-creating thread, any resultant modification of them (i.e. DataBind()) will be.

You would call my code from the worker thread like so:

this.BindData.Invoke();

This would then cause the original thread to do the binding, which (presuming it is the thread that created the controls) should work.

This is more or less the perfect use case for SVK.  SVK is a command line front end for subversion that works with an entire local copy of the repository.  So your commits, updates, etc. work on the local repository and you can then sync with a master.  I would generally recommend SVK over plain subversion anyway as it makes a lot of things nicer.  No .svn folders, better branching and merging, better conflict resolution.


This looks like an excellent opportunity to have a look at Aspect Oriented Programming. Here is a good article on AOP in .NET. The general idea is that you'd extract the cross-functional concern (i.e. Retry for x hours) into a separate class and then you'd annotate any methods that need to modify their behaviour in that way. Here's how it might look (with a nice extension method on Int32)[RetryFor( 10.Hours() )]
public void DeleteArchive()
{
  //.. code to just delete the archive
}


In my experience with LINQ to SQL and LINQ to Entities a DataContext is synonymous to a connection to the database. So if you were to use multiple data stores you would need to use multiple DataContexts. My gut reaction is you wouldn't notice to much of a slow down with a DataContext that encompasses a large number of tables. If you did however you could always split the database logically at points where you can isolate tables that don't have any relationship to other sets of tables and create multiple contexts.

You can accomplish this with Section Handlers. There is a basic overview of how to write one at http://www.codeproject.com/KB/aspnet/ConfigSections.aspx however it refers to app.config which would be pretty much the same as writing one for use in web.config. This will allow you to essentially have your own XML tree in the config file and do some more advanced configuration.

Assuming you have the correct assemblies and a C# compiler you in theory can use whatever you want to edit the code and then just run the compiler by hand or using a build script. That being said it is a real pain doing .NET development without Visual Studio/SharpEdit/Monodevelop in my opinion.

I haven't done SWING development since my early CS classes but if it wasn't built in you could just inherit javax.swing.AbstractButton and create your own. Should be pretty simple to wire something together with their existing framework.

You could always try the Synth look &amp; feel. You provide an xml file that acts as a sort of stylesheet, along with any images you want to use. The code might look like this:

try {
    SynthLookAndFeel synth = new SynthLookAndFeel();
    Class aClass = MainFrame.class;
    InputStream stream = aClass.getResourceAsStream("\\default.xml");

    if (stream == null) {
        System.err.println("Missing configuration file");
        System.exit(-1);                
    }

    synth.load(stream, aClass);

    UIManager.setLookAndFeel(synth);
} catch (ParseException pe) {
    System.err.println("Bad configuration file");
    pe.printStackTrace();
    System.exit(-2);
} catch (UnsupportedLookAndFeelException ulfe) {
    System.err.println("Old JRE in use. Get a new one");
    System.exit(-3);
}


From there, go on and add your JButton like you normally would. The only change is that you use the setName(string) method to identify what the button should map to in the xml file.

The xml file might look like this:

&lt;synth&gt;
    &lt;style id="button"&gt;
        &lt;font name="DIALOG" size="12" style="BOLD"/&gt;
        &lt;state value="MOUSE_OVER"&gt;
            &lt;imagePainter method="buttonBackground" path="dirt.png" sourceInsets="2 2 2 2"/&gt;
            &lt;insets top="2" botton="2" right="2" left="2"/&gt;
        &lt;/state&gt;
        &lt;state value="ENABLED"&gt;
            &lt;imagePainter method="buttonBackground" path="dirt.png" sourceInsets="2 2 2 2"/&gt;
            &lt;insets top="2" botton="2" right="2" left="2"/&gt;
        &lt;/state&gt;
    &lt;/style&gt;
    &lt;bind style="button" type="name" key="dirt"/&gt;
&lt;/synth&gt;


The bind element there specifies what to map to (in this example, it will apply that styling to any buttons whose name property has been set to "dirt").

And a couple of useful links:

http://javadesktop.org/articles/synth/

http://docs.oracle.com/javase/tutorial/uiswing/lookandfeel/synth.html


This will allow you to pass a weight of 0 and still work properly.  Notice the === operator, this checks to see if weight matches "null" in both value and type (as opposed to ==, which is just value, so 0 == null == false).

PHP:

public function createShipment($startZip, $endZip, $weight=null){    if ($weight === null)        $weight = $this-&gt;getDefaultWeight();}

Thxm, Mc! http://asm.objectweb.org/ is another one. Excellent documentation on byte code instrumentation, but nothing "directly" aimed at writing a coverage tool - just some hints or ideas.

Quick'n Dirty:

First create your ConfigurationSection and ConfigurationElement classes:

public class MyStuffSection : ConfigurationSection
{
    ConfigurationProperty _MyStuffElement;

    public MyStuffSection()
    {
        _MyStuffElement = new ConfigurationProperty("MyStuff", typeof(MyStuffElement), null);

        this.Properties.Add(_MyStuffElement);
    }

    public MyStuffElement MyStuff
    {
        get
        {
            return this[_MyStuffElement] as MyStuffElement;
        }
    }
}

public class MyStuffElement : ConfigurationElement
{
    ConfigurationProperty _SomeStuff;

    public MyStuffElement()
    {
        _SomeStuff = new ConfigurationProperty("SomeStuff", typeof(string), "&lt;UNDEFINED&gt;");

        this.Properties.Add(_SomeStuff);
    }

    public string SomeStuff
    {
        get
        {
            return (String)this[_SomeStuff];
        }
    }
}


Then let the framework know how to handle your configuration classes in web.config:

&lt;configuration&gt;
  &lt;configSections&gt;
    &lt;section name="MyStuffSection" type="MyWeb.Configuration.MyStuffSection" /&gt;
  &lt;/configSections&gt;
  ...


And actually add your own section below:

  &lt;MyStuffSection&gt;
    &lt;MyStuff SomeStuff="Hey There!" /&gt;
  &lt;/MyStuffSection&gt;


Then you can use it in your code thus:

MyWeb.Configuration.MyStuffSection configSection = ConfigurationManager.GetSection("MyStuffSection") as MyWeb.Configuration.MyStuffSection;

if (configSection != null &amp;&amp; configSection.MyStuff != null)
{
    Response.Write(configSection.MyStuff.SomeStuff);
}


Yes, this is possible. One of the main pros for using Swing is the ease with which the abstract controls can be created and manipulates.

Here is a quick and dirty way to extend the existing JButton class to draw a circle to the right of the text.

package test;

import java.awt.Color;
import java.awt.Container;
import java.awt.Dimension;
import java.awt.FlowLayout;
import java.awt.Graphics;

import javax.swing.JButton;
import javax.swing.JFrame;

public class MyButton extends JButton {

    private static final long serialVersionUID = 1L;

    private Color circleColor = Color.BLACK;

    public MyButton(String label) {
        super(label);
    }

    @Override
    protected void paintComponent(Graphics g) {
        super.paintComponent(g);

        Dimension originalSize = super.getPreferredSize();
        int gap = (int) (originalSize.height * 0.2);
        int x = originalSize.width + gap;
        int y = gap;
        int diameter = originalSize.height - (gap * 2);

        g.setColor(circleColor);
        g.fillOval(x, y, diameter, diameter);
    }

    @Override
    public Dimension getPreferredSize() {
        Dimension size = super.getPreferredSize();
        size.width += size.height;
        return size;
    }

    /*Test the button*/
    public static void main(String[] args) {
        MyButton button = new MyButton("Hello, World!");

        JFrame frame = new JFrame();
        frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
        frame.setSize(400, 400);

        Container contentPane = frame.getContentPane();
        contentPane.setLayout(new FlowLayout());
        contentPane.add(button);

        frame.setVisible(true);
    }

}


Note that by overriding paintComponent that the contents of the button can be changed, but that the border is painted by the paintBorder method. The getPreferredSize method also needs to be managed in order to dynamically support changes to the content. Care needs to be taken when measuring font metrics and image dimensions.

For creating a control that you can rely on, the above code is not the correct approach. Dimensions and colours are dynamic in Swing and are dependent on the look and feel being used. Even the default Metal look has changed across JRE versions. It would be better to implement AbstractButton and conform to the guidelines set out by the Swing API. A good starting point is to look at the javax.swing.LookAndFeel and javax.swing.UIManager classes.

http://docs.oracle.com/javase/8/docs/api/javax/swing/LookAndFeel.html

http://docs.oracle.com/javase/8/docs/api/javax/swing/UIManager.html

Understanding the anatomy of LookAndFeel is useful for writing controls:
Creating a Custom Look and Feel


When I was first learning Java we had to make Yahtzee and I thought it would be cool to create custom Swing components and containers instead of just drawing everything on one JPanel.  The benefit of extending Swing components, of course, is to have the ability to add support for keyboard shortcuts and other accessibility features that you can't do just by having a paint() method print a pretty picture.  It may not be done the best way however, but it may be a good starting point for you.

Edit 8/6 - If it wasn't apparent from the images, each Die is a button you can click. This will move it to the DiceContainer below.  Looking at the source code you can see that each Die button is drawn dynamically, based on its value.





Here are the basic steps:


Create a class that extends JComponent
Call parent constructor super() in your constructors
Make sure you class implements MouseListener
Put this in the constructor:

enableInputMethods(true);   
addMouseListener(this);

Override these methods:

public Dimension getPreferredSize()  
public Dimension getMinimumSize()  
public Dimension getMaximumSize()

Override this method:

public void paintComponent(Graphics g)



The amount of space you have to work with when drawing your button is defined by getPreferredSize(), assuming getMinimumSize() and getMaximumSize() return the same value.  I haven't experimented too much with this but, depending on the layout you use for your GUI your button could look completely different.

And finally, the source code.  In case I missed anything.  


in 1.x there used to be things DataTables couldn't do which DataSets could (don't remember exactly what). All that was changed in 2.x.  My guess is that's why a lot of examples still use DataSets.  DataTables should be quicker as they are more lightweight. If you're only pulling a single resultset, its your best choice between the two.

It really depends on the sort of data you're bringing back.  Since a DataSet is (in effect) just a collection of DataTable objects, you can return multiple distinct sets of data into a single, and therefore more manageable, object.  

Performance-wise, you're more likely to get inefficiency from unoptimized queries than from the "wrong" choice of .NET construct.  At least, that's been my experience.


One feature of the DataSet is that if you can call multiple select statements in your stored procedures, the DataSet will have one DataTable for each.

Try the Managed Fusion Url Rewriter and Reverse Proxy:

http://urlrewriter.codeplex.com

The rule for rewriting this would be:

# clean up old rules and forward to new URL
RewriteRule ^/?user=(.*)  /users/$1 [NC,R=301]

# rewrite the rule internally
RewriteRule ^/users/(.*)  /?user=$1 [NC,L]



Scott Guthrie covers how to do this natively in .Net pretty extensively here.

I've used the httpmodule approach and it works well.  It's basically what ManagedFusion is doing for you.

if its .net on both sides:
think you should use binary serialization and send the byte[] result.
trusting your struct to be fully blittable can be trouble.
you will pay in some overhead (both cpu and network) but will be safe.

Whatever your [things] are need to be written outside of VBScript.

In VB6, you can write a Custom Collection class, then you'll need to compile to an ActiveX DLL and register it on your webserver to access it.

Make sure that the file has svn:keywords "Rev Id" and then put $Rev$ somewhere in there.

See this question and the answers to it.

If you need to populate each member variable by hand you can generalize it a bit as far as the primitives are concerned by using FormatterServices to retrieve in order the list of variable types associated with an object. I've had to do this in a project where I had a lot of different message types coming off the stream and I definitely didn't want to write the serializer/deserializer for each message. 

Here's the code I used to generalize the deserialization from a byte[].

public virtual bool SetMessageBytes(byte[] message)    {        MemberInfo[] members = FormatterServices.GetSerializableMembers(this.GetType());        object[] values = FormatterServices.GetObjectData(this, members);        int j = 0;        for (int i = 0; i &lt; members.Length; i++)        {            string[] var = members[i].ToString().Split(new char[] { ' ' });            switch (var[0])            {                case "UInt32":                    values[i] = (UInt32)((message[j] &lt;&lt; 24) + (message[j + 1] &lt;&lt; 16) + (message[j + 2] &lt;&lt; 8) + message[j + 3]);                    j += 4;                    break;                case "UInt16":                    values[i] = (UInt16)((message[j] &lt;&lt; 8) + message[j + 1]);                    j += 2;                    break;                case "Byte":                    values[i] = (byte)message[j++];                    break;                case "UInt32[]":                    if (values[i] != null)                    {                        int len = ((UInt32[])values[i]).Length;                        byte[] b = new byte[len * 4];                        Array.Copy(message, j, b, 0, len * 4);                        Array.Copy(Utilities.ByteArrayToUInt32Array(b), (UInt32[])values[i], len);                        j += len * 4;                    }                    break;                case "Byte[]":                    if (values[i] != null)                    {                        int len = ((byte[])values[i]).Length;                        Array.Copy(message, j, (byte[])(values[i]), 0, len);                        j += len;                    }                    break;                default:                    throw new Exception("ByteExtractable::SetMessageBytes Unsupported Type: " + var[1] + " is of type " +  var[0]);            }        }        FormatterServices.PopulateObjectMembers(this, members, values);        return true;    }

There is nothing VS specific with the MVC framework - it is just a bunch of DLLs that you can use. The wizards in VS just build you a quick-start framework.
ASP.NET MVC is "bin-deployable" - there is nothing too clever to set up on the server either - just point the wildcard ISAPI filter to ASP.NET

The search seems to be an "OR" instead of an "AND" :-( If you do a search for ASP.NET SVN on the search bar, you will not find it on Page 6 or so.

it definitely should get some refining.

Go to System Preferences &gt; Keyboard and Mouse, then choose Keyboard Shortcuts. At the bottom, ensure Full Keyboard Access is set to "All controls". It's a long time since I turned it on but I think that's all you need to do

Apple Menu &gt; System Preferences &gt; Keyboard &amp; Mouse &gt; Keyboard Shortcuts:

Change the radio button at the bottom from "Text boxes and lists only" to "All controls."

Edit: Dammit. We're a fast group around here aren't we? :-)

This is an alternative way (DataReader is faster than this one):

string s = "";SqlConnection conn = new SqlConnection("Server=192.168.1.1;Database=master;Connect Timeout=30;User ID=foobar;Password=raboof;");SqlDataAdapter da = new SqlDataAdapter("SELECT TOP 5 name, dbid FROM sysdatabases", conn);DataTable dt = new DataTable();da.Fill(dt);for (int i = 0; i &lt; dt.Rows.Count; i++){    s += dt.Rows[i]["name"].ToString() + " -- " + dt.Rows[i]["dbid"].ToString() + "\n";}MessageBox.Show(s);

It's in the System Preferences - this blog post shows where the setting is.

Probably it comes from VB6. Because with Option Base statement in VB6, you can alter the lower bound of arrays like this:

Option Base 1

Also in VB6, you can alter the lower bound of a specific array like this:

Dim myArray(4 To 42) As String

Check out this blog post: Tweaking the ICallbackEventHandler and Viewstate. The author seems to be addressing the very situation that you are experiencing: 


  So when using ICallbackEventHandler you have two obstacles to overcome to have updated state management for callbacks. First is the problem of the read-only viewstate. The other is actually registering the changes the user has made to the page before triggering the callback.


See the blog post for his suggestions on how to solve this. Also check out this forum post which discusses the same problem as well.

I have found the solution else where:

SELECT SUBSTRING(master.dbo.fn_varbintohexstr(HashBytes('MD5', 'HelloWorld')), 3, 32) 

Why not use For Each?  That way you don't need to care what the LBound and UBound are.

Dim x, y, z
x = Array(1, 2, 3)

For Each y In x
    z = DoSomethingWith(y)
Next



You should make your choice of server platform based on the environment as a whole, and that includes the admin/management interfaces supplied.

I'm afraid that if you don't like the way Windows implements management of IIS, then that's too bad.  Having said that, a bit of delving around in the WMI interfaces will generally yield a solution that you should find usable.  I used to do quite a bit of WMI scripting (mostly via PowerShell) in order to have a reliable environment rebuild capability.

I actually found both of those links you provided, but as noted they are simply describing the problem, not solving it. The author of the blog post suggests a workaround by using a different ViewState provider, but unfortunately that isn't a possibility in this case...I really need to leave the particulars of the ViewState alone and just hook on to what is being done out-of-the-box.

Try this:

using (FileStream stream = new FileStream(fileName, FileMode.Open)){    BinaryFormatter formatter = new BinaryFormatter();    StructType aStruct = (StructType)formatter.Deserialize(filestream);}

A VirtualHost would also work for this and may work better for you as you can host several projects without the need for subdirectories.  Here's how you do it:

httpd.conf (or extra\httpd-vhosts.conf relative to httpd.conf. Trailing slashes "\" might cause it not to work):

NameVirtualHost *:80# ...&lt;VirtualHost *:80&gt;      DocumentRoot C:\projects\transitCalculator\trunk\    ServerName transitcalculator.localhost    &lt;Directory C:\projects\transitCalculator\trunk\&gt;          Order allow,deny          Allow from all      &lt;/Directory&gt;&lt;/VirtualHost&gt; 

HOSTS file (c:\windows\system32\drivers\etc\hosts usually):

# localhost entries127.0.0.1 localhost transitcalculator.localhost

Now restart XAMPP and you should be able to access http://transitcalculator.localhost/ and it will map straight to that directory.

This can be helpful if you're trying to replicate a production environment where you're developing a site that will sit on the root of a domain name.  You can, for example, point to files with absolute paths that will carry over to the server:

&lt;img src="/images/logo.png" alt="My Logo" /&gt;

whereas in an environment using aliases or subdirectories, you'd need keep track of exactly where the "images" directory was relative to the current file.

I don't see any problem with your code.

just out of my head, what if you try to do it manually? does it work?

BinaryReader reader = new BinaryReader(stream);StructType o = new StructType();o.FileDate = Encoding.ASCII.GetString(reader.ReadBytes(8));o.FileTime = Encoding.ASCII.GetString(reader.ReadBytes(8));.........

also try

StructType o = new StructType();byte[] buffer = new byte[Marshal.SizeOf(typeof(StructType))];GCHandle handle = GCHandle.Alloc(buffer, GCHandleType.Pinned);Marshal.StructureToPtr(o, handle.AddrOfPinnedObject(), false);handle.Free();

then use buffer[] in your BinaryReader instead of reading data from FileStream to see whether you still get AccessViolation exception.


  I had no luck using the
  BinaryFormatter, I guess I have to
  have a complete struct that matches
  the content of the file exactly.


That makes sense, BinaryFormatter has its own data format, completely incompatible with yours.

I suppose you could compare the ad prints with the page views on your website (which you can get from your analytics software).

Since programs like AdBlock actually never request the advert, you would have to look the server logs to see if the same user accessed a webpage but didn't access an advert. This is assuming the advert is on the same server.

If your adverts are on a separate server, then I would suggest it's impossible to do so.

The best way to stop users from blocking adverts, is to have inline text adverts which are generated by the server and dished up inside your html.

Add the user id to the request for the ad:

&lt;img src="./ads/viagra.jpg?{user.id}"/&gt;

that way you can check what ads are seen by which users.

You need to think about the different ways that ads are blocked. The first thing to look at is whether they are running noscript, so you could add a script that would check for that. 

The next thing is to see if they are blocking flash, a small movie should do that.

If you look at the adblock site, there is some indication of how it does blocking:
How does element hiding work?

If you look further down that page, you will see that conventional chrome probing will not work, so you need to try and parse the altered DOM.


Most people use .NET serialization (there is faster binary and slower XML formatter, they both depend on reflection and are version tolerant to certain degree)

However, if you want the fastest (unsafe) way - why not:

Writing:

YourStruct o = new YourStruct();
byte[] buffer = new byte[Marshal.SizeOf(typeof(YourStruct))];
GCHandle handle = GCHandle.Alloc(buffer, GCHandleType.Pinned);
Marshal.StructureToPtr(o, handle.AddrOfPinnedObject(), false);
handle.Free();


Reading:

handle = GCHandle.Alloc(buffer, GCHandleType.Pinned);
o = (YourStruct)Marshal.PtrToStructure(handle.AddrOfPinnedObject(), typeof(YourStruct));
handle.Free();



Alright. From a theoretical point of view, given that the distortion is "arbitrary", and any solution requires you to model this arbitrary distortion, you obviously can't get an "answer". However, any solution is going to involve imposing (usually implicitly) some model of the distortion that may or may not reflect the reality of the situation.

Since you seem to be most interested in models that presume some sort of local continuity of the distortion mapping, the most obvious choice is the one you've already tried: linear interpolaton between the nearest points. Going beyond that is going to require more sophisticated mathematical and numerical analysis knowledge.

You are incorrect, however, in presuming you cannot expand this to more points. You can by using a least-squared error approach. Find the linear answer that minimizes the error of the other points. This is probably the most straight-forward extension. In other words, take the 5 nearest points and try to come up with a linear approximation that minimizes the error of those points. And use that. I would try this next.

If that doesn't work, then the assumption of linearity over the area of N points is broken. At that point you'll need to upgrade to either a quadratic or cubic model. The math is going to get hectic at that point.

The two self-balancing BSTs I'm most familiar with are red-black and AVL, so I can't say for certain if any other solutions are better, but as I recall, red-black has faster insertion and slower retrieval compared to AVL. 

So if insertion is a higher priority than retrieval, red-black may be a better solution.


Red-black is better than AVL for insertion-heavy applications. If you foresee relatively uniform look-up, then Red-black is the way to go. If you foresee a relatively unbalanced look-up where more recently viewed elements are more likely to be viewed again, you want to use splay trees.


You can add a mouse click event to the TreeView, then select the correct node using GetNodeAt given the mouse coordinates provided by the MouseEventArgs.

void treeView1MouseUp(object sender, MouseEventArgs e)
{
    if(e.Button == MouseButtons.Right)
    {
        // Select the clicked node
        treeView1.SelectedNode = treeView1.GetNodeAt(e.X, e.Y);

        if(treeView1.SelectedNode != null)
        {
            myContextMenuStrip.Show(treeView1, e.Location);
        }
    }
}



As always, Google is your friend:
http://nixbit.com/cat/programming/libraries/c-generic-library/
specifically:
http://nixbit.com/cat/programming/libraries/generic-data-structures-library/

Flash is certainly the most ubiquitous and portable solution.  98% of browsers have Flash installed.  Other alternatives are Quicktime, Windows Media Player, or even Silverlight (Microsoft's Flash competitor, which can be used to embed several video formats).

I would recommend using Flash (and it's FLV video file format) for embedding your video unless you have very specific requirements as far as video quality or DRM.

Flash is usually the product of choice: Everyone has it, and using the JW FLV Player makes it relatively easy on your side.

As for other Video Formats, there are WMV and QuickTime, but the players are rather "heavy", not everyone might have them and they feel so 1990ish...

Real Player... Don't let me even start ranting about that pile of ...

The only other alternative of Flash that I would personally consider is Silverlight, which allows streaming WMV Videos. I found the production of WMV much better and easier than FLV because all Windows FLV Encoders I tried are not really good and stable, whereas pretty much every tool can natively output WMV. The problem with Silverlight is that no one has that Browser Plugin (yet?). There is also a player from JW.

I have worked for a company that developed a system for distributing media content to dedicated "players". It was web based and used ASP.NET technology and have tried almost every possible media format you can think of and your choice really comes down to asking yourself:

does it needs to play directly out of the box, or can I make sure that the components required to play the videos can be installed beforehand?

If your answer is that it needs to play out of the box then really your only option is flash (I know that it is not installed by default, but most will already have it installed)

If it is not a big issue that extra components are needed then you can go with formats that are supported by windows media player

The reason why windows media player falls into the second option is because for some browsers and some formats extra components must be installed.

We had the luxury that the "players" were provided by us, so we could go for the second option, however even we tried to convert as much as possible back to flash because it handles way better than windows media player

I'm no expert on Agile development, but I would imagine that integrating some basic automated pen-test software into your build cycle would be a good start.  I have seen several software packages out there that will do basic testing and are well suited for automation.

Not an answer, but a warning: my company bought the 2007 Infragistics ASP.NET controls just for the Grid, and we regret that choice. 

The quality of API is horrible (in our opinion at least), making it very hard to program against the grid (for example, inconsistent naming conventions, but this is just an inconvenience, we have complaints about the object model as well).

So I can't say that I know of a better option, I just know I will give a try to something else before paying for Infragistics products again (and the email support we got was horrible as well).

I'm not a security expert, but I think the most important fact you should be aware of, before testing security, is what you are trying to protect. Only if you know what you are trying to protect, you can do a proper analysis of your security measures and only then you can start testing those implemented measures.

Very abstract, I know. However, I think it should be the first step of every security audit.

The Glib library used on the Gnome project may also be some use. Moreover it is pretty well tested.

IBM developer works has a good tutorial on its use: Manage C data using the GLib collections


SQLite databases exist independently, so there's not way to do this from the database level.

You will have to write your own code to do this.

I was wrestling with this problem several years ago (2004 I think).  We ran into the problem that Firefox doesn't allow scripts to read the clipboard by default (but you can grant access to the clipboard).

There's other ways of reading the clipboard data as well...Flash, for instance, can read the clipboard.  There's a good article on ajaxian to explain how do to this behind the scenes.

In the end, we couldn't find a web-based Grid that fit the bill, so we had to create our own in a mixture of Actionscript and Javascript.

I'm currently using dhtmlxGrid and we have the Excel copy/paste functionality working.  dhtmlXGrid is the most full featured javascript grid package that I've found.

On their website, dhtmlXGrid claims to support Clipboard functionality in the Professional version.  (However, I noticed the Sample on their site isn't working on my Firefox.  EDIT: It's probably the permissions issue that Nathan mentioned.)

In any case, we had to do some extra work to get the exact Excel copy and paste functionality we wanted.  We essentially had to override some of their functionality to get the desired behavior.  Their support was pretty good in helping us come up with a solution.

So to answer your question, you should be able to get them to support copy and paste if you purchase the Professional version.  I'm just warning you that it may take some additional work to fine tune that behavior.  

Overall, I'm happy with dhtmlXGrid.  We use a lot of their features.  Their support is pretty good.  They usually take one day to respond since they are in Europe (I think).  And Javascript is by its very nature open source so I can always dive in when I need to.  

The way I have done this is to create a command script file and pass this on the command line via the  /b command to psftp.exe.  I have also tried this in Perl and have yet to find a neater way of doing it.

There is an issue with this method, in that you already have to have accepted the RSA finger-print.  If not, then the script will either wait for user input to accept it or will skip over it if you are running in full batch mode, with a failure.  Also, if the server changes so that it's RSA finger-print changes (e.g. a cluster) then you need to re-accept the finger-print again.

Not an ideal method, but the only one I know.

I shall be watching this question incase anyone knows another way.

Using attributes, child config sections and constraints

There is also the possibility to use attributes which automatically takes care of the plumbing, as well as providing the ability to easily add constraints.

I here present an example from code I use myself in one of my sites. With a constraint I dictate the maximum amount of disk space any one user is allowed to use.

MailCenterConfiguration.cs:

namespace Ani {

    public sealed class MailCenterConfiguration : ConfigurationSection
    {
        [ConfigurationProperty("userDiskSpace", IsRequired = true)]
        [IntegerValidator(MinValue = 0, MaxValue = 1000000)]
        public int UserDiskSpace
        {
            get { return (int)base["userDiskSpace"]; }
            set { base["userDiskSpace"] = value; }
        }
    }
}


This is set up in web.config like so

&lt;configSections&gt;
    &lt;!-- Mailcenter configuration file --&gt;
    &lt;section name="mailCenter" type="Ani.MailCenterConfiguration" requirePermission="false"/&gt;
&lt;/configSections&gt;
...
&lt;mailCenter userDiskSpace="25000"&gt;
    &lt;mail
     host="my.hostname.com"
     port="366" /&gt;
&lt;/mailCenter&gt;


Child elements

The child xml element mail is created created in the same .cs file as the one above. Here I've added constraints on the port. If the port is assigned a value not in this range the runtime will complain when the config is loaded.

MailCenterConfiguration.cs:

public sealed class MailCenterConfiguration : ConfigurationSection
{
    [ConfigurationProperty("mail", IsRequired=true)]
    public MailElement Mail
    {
        get { return (MailElement)base["mail"]; }
        set { base["mail"] = value; }
    }

    public class MailElement : ConfigurationElement
    {
        [ConfigurationProperty("host", IsRequired = true)]
        public string Host
        {
            get { return (string)base["host"]; }
            set { base["host"] = value; }
        }

        [ConfigurationProperty("port", IsRequired = true)]
        [IntegerValidator(MinValue = 0, MaxValue = 65535)]
        public int Port
        {
            get { return (int)base["port"]; }
            set { base["port"] = value; }
        }


Use

To then use it practically in code, all you have to do is instantiate the MailCenterConfigurationObject, this will automatically read the relevant sections from web.config.

MailCenterConfiguration.cs

private static MailCenterConfiguration instance = null;
public static MailCenterConfiguration Instance
{
    get
    {
        if (instance == null)
        {
            instance = (MailCenterConfiguration)WebConfigurationManager.GetSection("mailCenter");
        }

        return instance;
    }
}


AnotherFile.cs

public void SendMail()
{
    MailCenterConfiguration conf = MailCenterConfiguration.Instance;
    SmtpClient smtpClient = new SmtpClient(conf.Mail.Host, conf.Mail.Port);
}


Check for validity

I previously mentioned that the runtime will complain when the configuration is loaded and some data does not comply to the rules you have set up (e.g. in MailCenterConfiguration.cs). I tend to want to know these things as soon as possible when my site fires up. One way to solve this is load the configuration in _Global.asax.cx.Application_Start_ , if the configuration is invalid you will be notified of this with the means of an exception. Your site won't start and instead you will be presented detailed exception information in the Yellow screen of death.

Global.asax.cs

protected void Application_ Start(object sender, EventArgs e)
{
    MailCenterConfiguration.Instance;
}



I'd been wrangling over the same question whilst retro fitting LINQ to SQL over a legacy DB. Our database is a bit of a whopper (150 tables) and after some thought and experimentation I elected to use multiple DataContexts. Whether this is considered an anti-pattern remains to be seen, but for now it makes life manageable.


Ive only built 2 applications that used the profile provider. Since then I have stayed away from using it. For both of the apps I used it to store information about the user such as their company name, address and phone number. 

This worked fine until our client wanted to be able to find a user by one of these fields.
Searching involved looping through every users profile and comparing the information to the search criteria. As the user base grew the search time became unacceptable to our client. The only solution was to create a table to store the users information. Search speed was increased immensely.

I would recommend storing this type of information in its own table.

Unfortunately the only API that isn't deprecated is located in the ApplicationServices framework, which doesn't have a bridge support file, and thus isn't available in the bridge. If you're wanting to use ctypes, you can use ATSFontGetFileReference after looking up the ATSFontRef.

Cocoa doesn't have any native support, at least as of 10.5, for getting the location of a font.

As I understand it, random access is in constant time for both Python's dicts and lists, the difference is that you can only do random access of integer indexes with lists.  I'm assuming that you need to lookup a node by its label, so you want a dict of dicts.

However, on the performance front, loading it into memory may not be a problem, but if you use too much you'll end up swapping to disk, which will kill the performance of even Python's highly efficient dicts.  Try to keep memory usage down as much as possible.  Also, RAM is amazingly cheap right now; if you do this kind of thing a lot, there's no reason not to have at least 4GB.

If you'd like advice on keeping memory usage down, give some more information about the kind of information you're tracking for each node.

I used to do that with FTP on windows (create a file of commands and shell out FTP.exe)

Xetius I tried to upvote yours and accept your answer too and I received an warning that I needed 25 rep to do that.  Sorry. 

I had no luck using the BinaryFormatter, I guess I have to have a complete struct that matches the content of the file exactly. I realised that in the end I wasn't interested in very much of the file content anyway so I went with the solution of reading part of stream into a bytebuffer and then converting it using

Encoding.ASCII.GetString()


for strings and

BitConverter.ToInt32()


for the integers.

I will need to be able to parse more of the file later on but for this version I got away with just a couple of lines of code.


Look around on github for some open source projects.  Some of the more popular projects are:


Rails
Merb
Rubinius


Look on the Popular Forked list and you'll probably see something that interests you.


AdBlock forum says this is used to detect AdBlock. After some tweaking you could use this to gather some statistics.

&lt;script language="JavaScript" type="text/JavaScript"&gt;setTimeout('detect_abp()', 10000);var isFF = (navigator.userAgent.indexOf("Firefox") &gt; -1) ? true : false;var hasABP = false;function detect_abp(){   if(isFF)   {         if(Components.interfaces.nsIAdblockPlus != undefined)        {           hasABP = true;        }        else        {          var AbpImage = document.createElement("IMG");          AbpImage.id = 'abp_detector';          AbpImage.src = '/textlink-ads.jpg';            AbpImage.style.width = '0px';          AbpImage.style.height = '0px';          AbpImage.style.top = '-1000px';          AbpImage.style.left = '-1000px';          document.body.appendChild(AbpImage);          hasABP = (document.getElementById('abp_detector').style.display == 'none');             var e = document.getElementsByTagName("iframe");            for (var i = 0; i &lt; e.length; i++)            {               if(e[i].clientHeight == 0)                {                    hasABP = true;                }            }              if(hasABP == true)              {               history.go(1);                location = "http://www.tweaktown.com/supportus.html";              window.location(location);                    }        }      }    }    &lt;/script&gt; 

Honestly the ASP.NET Membership / Roles features would work perfectly for the scenario you described. Writing your own tables / procs / classes is a great exercise and you can get very nice control over minute details, but after doing this myself I've concluded it's better to just use the built in .NET stuff. A lot of existing code is designed to work around it which is nice at well. Writing from scratch took me about 2 weeks and it was no where near as robust as .NETs. You have to code so much crap (password recovery, auto lockout, encryption, roles, a permission interface, tons of procs, etc) and the time could be better spent elsewhere.

Sorry if I didn't answer your question, I'm like the guy who says to learn c# when someone asks a vb question.

Try the following

var names = (from dr in dataTable.Rows
             select (string)dr["Name"]).Distinct().OrderBy(name =&gt; name);


this should work for what you need.


Keep in mind that TempData stores the form collection in session. If you don't like that behavior, you can implement the new ITempDataProvider interface and use some other mechanism for storing temp data. I wouldn't do that unless you know for a fact (via measurement and profiling) that the use of Session state is hurting you.

A few months back I wrote a blog post about  Fluent Interfaces and LINQ which used an Extension Method on IQueryable&lt;T&gt; and another class to provide the following natural way of paginating a LINQ collection.

var query = from i in ideas
            select i;
var pagedCollection = query.InPagesOf(10);
var pageOfIdeas = pagedCollection.Page(2);


You can get the code from the MSDN Code Gallery Page: Pipelines, Filters, Fluent API and LINQ to SQL.


You could also use a more OO approach:


Create a base class that does the error handling and calls an abstract method to perform the concrete work. (Template Method pattern)
Create concrete classes for each operation.


This has the advantage of naming each type of operation you perform and gives you a Command pattern - operations have been represented as objects.

If you're already shuffling the ViewState around anyway, you might as well use an UpdatePanel.  Its partial postbacks will update the page's ViewState automatically.

I disagree with John's answer. The DataContext (or Linq to Entities ObjectContext) is more of a "unit of work" than a connection. It manages change tracking, etc. See this blog post for a description:

Lifetime of a LINQ to SQL DataContext

The four main points of this blog post are that DataContext:


Is ideally suited
for a "unit of work" approach 
Is also designed for
"stateless" server operation
Is not designed for
    Long-lived usage

    Should be used very carefully after
    any SumbitChanges() operation.


Considering that, I don't think using more than one DataContext would do any harm- in fact, creating different DataContexts for different types of work would help make your LinqToSql impelmentation more usuable and organized. The only downside is you wouldn't be able to use sqlmetal to auto-generate your dmbl. 


I found a fairly elegant solution with Telerik's RadAjaxManager. It works quite nicely, essentially you register each control which might invoke a postback, and then register each control which should be re-drawn after that postback is performed asynchronously. The RadAjaxManager will update the DOM after the async postback and rewrite the ViewState and all affected controls. After taking a peek in Reflector, it looks a little kludgy under the hood, but it suits my purposes.

I don't understand why you would use a custom control for that, when the builtin ASP.NET AJAX UpdatePanel does the same thing.

It just adds more complexity, gives you less support, and makes it more difficult for others to work on your app.

Create a class object and return a list(T) of the query.


If you want the context menu to be dependent on the selected item you're best move I think is to use Jonesinator's code to select the clicked item. Your context menu content can then be dependent on the selected item.

Selecting the item first as opposed to just using it for the context menu gives a few advantages. The first is that the user has a visual indication as to which he clicked and thus which item the menu is associated with. The second is that this way it's a hell of a lot easier to keep compatible with other methods of invoking the context menu (e.g. keyboard shortcuts).

In WPF and Silverlight the binding infrastructure takes care of the switching to the UI thread.

Protocol buffers are intended to optimize communications between machines. They are really not intended for human interaction. Also, the format is binary, so it could not replace XML in that use case. 

I would also recommend JSON as being the most compact text-based format.

The open source tool Schema Spy works well. Sample output is available here. If your working with oracle databases, Oracle has a free SQL Developer Tool that generates schema documentation very similar to the output of schema spy.

Is your scripting language bytecode generating? Does it generate debug metadata? If so, bytecode instrumentation is probably the way to go. In fact existing tools like will probably work; perhaps with minimal modification (the typical problem is the tools are written to work with Java and assume com.foo.Bar.class corresponds to com/foo/Bar.java. Unwinding that assumption can be tedious.) EMMA is a ClassLoader that does byte-code re-writing for code-coverage collection in Java. The coding style is a little funky, but I recommend reading the source for some ideas.

If your scripting language is interpreted then you will need something higher-level (source level) that hooks into the interpreter.


You'll get different results for the different methods depending on whether you compile with optimisations on. You basically have a few options:

object o;

//checking with is
o is int

//check type
o.GetType() != typeof( int )

//cast and catch exception
try{ int j = (int) o; } 
catch {}

//use the tryparse
int.TryParse( Convert.ToString( o ), out j )


You can easily set up a console app that tries each of these 10,000 times and returns durations for each (test when o is an int and when it's something else).

The try-catch method is the quickest if the object does hold an int, and by far the slowest if it doesn't (even slower than GetType).  int.TryParse is pretty quick if you have a string, but if you have an unknown object it's slower.

Interestingly, with .Net 3.5 and optimisations turned on the o is int check takes the same time as try-catch when o actually is an int. o is int is only slightly slower if o actually is something else.

Annoyingly FxCop will throw up warnings if you do something like:

if( o is int )
    int j = (int) o;


But I think that's a bug in FxCop - it doesn't know int is a value type and recommends you to use o as int instead.

If your input is always a string int.TryParse is best, otherwise the is operator is quickest.

As you have a string I'd look at whether you need to know that it's an int, rather than a double.  If int.TryParse passes then so will double.TryParse so you could half the number of checks - return either double or string and floor the doubles when you expect an int.


Check out asio. It is a cross compatable c++ library for asyncronous IO. I am not sure if this would be useful for the server ( I have never tried to link a standard c++ DLL to a c# project) but for the client it would be useful.

We use it with our application, and it solved most of our IO concurrency problems.


Remote shell doesn't solve the productivity issue. (It merely makes things possible.)

From what I've heard, everything that the future Microsoft GUI:s do will be possible to do with powershell since the GUI:s use the same API:s as those that are available from powershell.

Personally, I love cygwin but cygwin can not help you manage Microsoft applications.

You might be surprised, however, how powerfull the Windows Scripting Host is when coupled with Window Management Instrumentation. I think IIS is fully manageable with WMI or some COM objects that can be easilly used from a JScript WSH script.


Something like this?

dim cars(2),x
cars(0)="Volvo"
cars(1)="Saab"
cars(2)="BMW"

For Each x in cars
  response.write(x &amp; "&lt;br /&gt;")
Next


See www.w3schools.com.

If you want to associate keys and values use a dictionary object instead:

Dim objDictionary
Set objDictionary = CreateObject("Scripting.Dictionary")
objDictionary.Add "Name", "Scott"
objDictionary.Add "Age", "20"
if objDictionary.Exists("Name") then
    ' Do something
else
    ' Do something else 
end if



I have used an httpmodule for url rewriting from www.urlrewriting.net with great success (albeit I believe a much earlier, simpler version)

If you have very few actual rewriting rules then url mappings built in to .NET 2.0 are probably an easier option, there are a few write ups of these on the web, the 4guysfromrolla one seems fairly exhaustive but as you can see they don't support regular expression mappings are are as such rendered fairly useless in a dynamic environment (assuming "smith" in your example is not a special case then these would be of no use)


I would also take a look at Passenger. It's a lot easier to get going than the traditional solution of Apache/nginx + Mongrel.


in our continuous integration setup we use SVNRevisionLabeller and pass the variables from this to MSBuild to use when creating the compiled website dll. It's then available to .NET using GetCurrentAssembly() in the final build.


In our rails app I have a secret (unpulished url, restricted to a certain class of authenticated user) action which literally does this

render :text =&gt; `svn info #{RAILS_ROOT}`


(this is the equivalent of Process.Start( "svn info..." ) if you're only familiar with .net)

If I'm wondering if the guy who manages the servers has updated the site recently, I can hit this URL, and have a look


You could try setting up your own python installation using Virtual Python.  Check out how to setup Django using it here.  That was written a long time ago, but it shows how I got MySQLdb setup without having root access or anything like it.  Once you've got the basics going, you can install any python library you want.


The dpUint testing framework has a test runner built with AIR which can be integrated with a build script.

There is also my FlexUnit automation kit which does more or less the same for FlexUnit. It has an Ant macro that makes it possible to run the tests as a part of an Ant script, for example:

&lt;target name="run-tests" depends="compile-tests"&gt;
  &lt;flexunit swf="${build.home}/tests.swf" failonerror="true"/&gt;
&lt;/target&gt;



About how to develop Flex applications the right way, I wouldn't look too much at the Cairngorm framework. It does claim to show "best practice" and so on, but I would say that the opposite is true. It's based around the use of global variables, and other things you should try to avoid. I've outlined some of the problems on my blog.

I would suggest that you look at the Mate framework instead, which has good documentation and good examples to get you going. It uses Flex to its full potential, doesn't rely on global variables as Cairngorm and PureMVC, and it makes it possible to write much more decoupled code.


It's possible to open multiple databases at once in Sqlite, but it's doubtful if can be done when working from Flex/AIR. In the command line client you run ATTACH DATABASE path/to/other.db AS otherDb and then you can refer to tables in that database as otherDb.tableName just as in MySQL or SQL Server.


  Tables in an attached database can be referred to using the syntax database-name.table-name. 
  
  ATTACH DATABASE documentation at sqlite.org



I've always used For Each...


You can also get the source from a Open Source code coverage tool and learn from it.


$rev and others like it are revisions for the individual files, so they won't change unless the file changes. The number on the webpage is (most likely, I'm assuming here) the svn revision number for the whole project. That is different than the file revisions, which others have been pointing to.

In this case I assume that CCNET is pulling the revision number of the project and rewriting a part of the webpage with that number. Any CI solution should be able to do this, set this up myself with CCNET and Teamcity (although not webpages, but automatic versioning of deployment/assembly versions).

In order for you to do this, use a CI solution that supports it, or use your build process (MSbuild/Nant) to store that version and write it to the files before "deploying" it.


Sql Server 2005 gives you the ability to specify a covering index.  This is an index that includes data from other columns at the leaf level, so you don't have to go back to the table to get columns that aren't included in the index keys.

create nonclustered index myidx on mytable (mycol1 asc, mycol2 asc) include (my_col3);

This is invaluable for a query that has mycol3 in the select list, and mycol1 and my_col2 in the where clause.


I just looked at the AIR SQL API, and there's an attach method on SQLConnection it looks exactly what you need.

I haven't tested this, but according to the documentation it should work:

var connection : SQLConnection = new SQLConnection();

connection.open(firstDbFile);
connection.attach(secondDbFile, "otherDb");

var statement : SQLStatement = new SQLStatement();

statement.connection = connection;
statement.text = "INSERT INTO main.myTable SELECT * FROM otherDb.myTable";
statement.execute();


There may be errors in that code snipplet, I haven't worked much with the AIR SQL API lately. Notice that the tables of the database opened with open are available using main.tableName, any attached database can be given any name at all (otherDb in the example above).


Watch out for the Gnu Scientific Library.  It's licensed under the GPL rather than LGPL.

As other folks mentioned, the Boost random classes are a good start.  Their implementation conforms to the PRNG code slated for TR1:

http://www.boost.org/doc/libs/1_35_0/libs/random/index.html
http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2003/n1452.html

If you have a recent version of the G++ compiler, you may find the TR1 libraries already included


On my project we're using Maven to build both our Flex RIA and the Java-based back end.  In order to build and test the Flex app we use the flex-mojos maven plugins.  They do a great job for us and I would highly recommend using Maven over Ant.

That being said, if you're already using Ant it can be a little tricky to transition over to Maven.  So if you're in that position I would recommend using the flexunit tasks available here: Ant Task

Both of these libraries do basically the same thing, they launch a generated flexunit test runner mxml application in a window and open a socket connection back to the build process using a JUnit test runner.  Amazingly enough it works pretty well.  The only problem is that you can't run it headless so if you want to run the build from a CI server you have to make sure that process has the ability to launch new windows otherwise it won't work.


We follow the practice of using a vendor directory which contains all vendor specific headers and binaries.  The goal is that anybody should be able to build the product just by checking it out and running some top level build script.  


I've previously used a component from here: www.weonlydo.com.  I didn't find it the easiest piece of kit to develop against but it got the job done in a hurry.


This is something that annoys me about MSSQL (rant on my blog). I wish MSSQL supported upsert. 

@Dillie-O's code is a good way in older SQL versions (+1 vote), but it still is basically two IO operations (the exists and then the update or insert)

There's a slightly better way on this post, basically:

--try an update
update tablename 
set field1 = 'new value',
    field2 = 'different value',
    ...
where idfield = 7

--insert if failed
if @@rowcount = 0 and @@error = 0
    insert into tablename 
           ( idfield, field1, field2, ... )
    values ( 7, 'value one', 'another value', ... )


This reduces it to one IO operations if it's an update, or two if an insert. 

MS Sql2008 introduces merge from the SQL:2003 standard:

merge tablename as target
using (values ('new value', 'different value'))
    as source (field1, field2)
    on target.idfield = 7
when matched then
    update
    set field1 = source.field1,
        field2 = source.field2,
        ...
when not matched then
    insert ( idfield, field1, field2, ... )
    values ( 7,  source.field1, source.field2, ... )


Now it's really just one IO operation, but awful code :-(


Instead of joining an open source project, find an itch you want to scratch.

I find my first year with a language is almost always throw away code (or at least, it should be).

Find a problem you (personally) want to solve. Use ruby to do it. You'll learn a lot.


As mentioned in the question, IEnumerable has a CopyToDataTable method:

IEnumerable&lt;DataRow&gt; query =
    from order in orders.AsEnumerable()
    where order.Field&lt;DateTime&gt;("OrderDate") &gt; new DateTime(2001, 8, 1)
    select order;

// Create a table from the query.
DataTable boundTable = query.CopyToDataTable&lt;DataRow&gt;();


Why won't that work for you?


Make a set of Data Transfer Objects, a couple of mappers, and return that via the .asmx.
You should never expose the database objects directly, as a change in the procedure schema will propagate to the web service consumer without you noticing it.


For reference&mdash;future Python possibilities:
Starting with Python 2.6 you can express binary literals using the prefix 0b or 0B:

&gt;&gt;&gt; 0b101111
47


You can also use the new bin function to get the binary representation of a number:

&gt;&gt;&gt; bin(173)
'0b10101101'


Development version of the documentation: What's New in Python 2.6


I think you can do this with the Row Test Attribute (available in MbUnit and later versions of NUnit) where you could specify several sets to populate one unit test.


We do this with xUnit.net for our automated builds. We use CruiseControl.net (and are trying out TeamCity). The MSBuild task that we run for continuous integration automatically changes the build number for us, so the resulting build ZIP file contains a properly versioned set of DLLs and EXEs.

Our MSBuild file contains a UsingTask reference for a DLL which does regular expression replacements: (you're welcome to use this DLL, as it's covered by the MS-PL license as well)


  &lt;UsingTask
     AssemblyFile="3rdParty\CodePlex.MSBuildTasks.dll"
     TaskName="CodePlex.MSBuildTasks.RegexReplace"/&gt;


Next, we extract the build number, which is provided automatically by the CI system. You could also get your source control provider to provide the source revision number if you want, but we found the build # in the CI system was more useful, because not only can see the integration results by the CI build number, that also provides a link back to the changeset(s) which were included in the build.


 &lt;!-- Cascading attempts to find a build number -->

 &lt;PropertyGroup Condition="'$(BuildNumber)' == ''">
   &lt;BuildNumber>$(BUILD_NUMBER)&lt;/BuildNumber>
 &lt;/PropertyGroup>
 &lt;PropertyGroup Condition="'$(BuildNumber)' == ''">
   &lt;BuildNumber>$(ccnetlabel)&lt;/BuildNumber>
 &lt;/PropertyGroup>
 &lt;PropertyGroup Condition="'$(BuildNumber)' == ''">
   &lt;BuildNumber>0&lt;/BuildNumber>
 &lt;/PropertyGroup>


(We try BUILD_NUMBER, which is from TeamCity, then ccnetlabel, which is from CC.net, and if neither is present, we default to 0, so that we can test the automated build script manually.)

Next, we have a task which sets the build number into a GlobalAssemblyInfo.cs file that we link into all of our projects:


 &lt;Target Name="SetVersionNumber">
   &lt;RegexReplace
       Pattern='AssemblyVersion\("(\d+\.\d+\.\d+)\.\d+"\)'
       Replacement='AssemblyVersion("$1.$(BuildNumber)")'
       Files='GlobalAssemblyInfo.cs'/>
   &lt;Exec Command="attrib -r xunit.installer\App.manifest"/>
 &lt;/Target>


This find the AssemblyVersion attribute, and replaces the a.b.c.d version number with a.b.c.BuildNumber. We will usually leave the source checked into the tree with the first three parts of the builder number fixed, and the fourth at zero (f.e., today it's 1.0.2.0).

In your build process, make sure the SetVersionNumber task precedes your build task. At the end, we use our Zip task to zip up the build results so that we have a history of the binaries for every automated build.


Using asynchronous communication is totally possible in single thread! 

There is a common design pattern in network software development called the reactor pattern (look at this book). Some well known network library provides an implementation of this pattern (look at ACE).

Briefly, the reactor is an object, you register all your sockets inside, and you wait for something. If something happened (new data arrived, connection close...) the reactor will notify you. And of course, you can use only one socket to send and received data asynchronously.



  Separating your source code into
  multiple projects makes only sense if
  you... 
  ... More developers involved
  and you want to treat their work as
  consumable black box. (not very
  recommended)  ...


Why isn't this recommended?  I've found it a very useful way to manage an application with several devs working on different portions.  Makes checkins much easier, mainly by virtually eliminating merges.  Very rarely will two devs have to work on the same project at the same time.


Separating features into projects is often a YAGNI architecture optimization. How often have you reused those separate projects, really? If it's not a frequent occurrence, you're complicating your development, build, deployment, and maintenance for theoretical reuse.

I much prefer separating into folders (using appropriate namespaces) and refactoring to separate projects when you've got a real-life reuse use case.


The "correct" way to do this is to respond to the WM_SYSCOMMAND message. In C# this looks something like this:

protected override void WndProc(ref Message m)
{
    // Abort screensaver and monitor power-down
    const int WM_SYSCOMMAND = 0x0112;
    const int SC_MONITOR_POWER = 0xF170;
    const int SC_SCREENSAVE = 0xF140;
    int WParam = (m.WParam.ToInt32() &amp; 0xFFF0);

    if (m.Msg == WM_SYSCOMMAND &amp;&amp;
        (WParam == SC_MONITOR_POWER || WParam == SC_SCREENSAVE)) return;

    base.WndProc(ref m);
}


According to MSDN, if the screensaver password is enabled by policy on Vista or above, this won't work. Presumably programmatically moving the mouse is also ignored, though I have not tested this.


Unit testing, Defense Programming and lots of logs

Unit testing

Make sure you unit test as early as possible (e.g. the password should be encrypted before sending, the SSL tunnel is working, etc). This would prevent your programmers from accidentally making the program insecure.

Defense Programming

I personally call this the Paranoid Programming but Wikipedia is never wrong (sarcasm). Basically, you add tests to your functions that checks all the inputs:


is the user's cookies valid?
is he still currently logged in?
are the function's parameters protected against SQL injection? (even though you know that the input are generated by your own functions, you will test anyway)


Logging

Log everything like crazy. Its easier to remove logs then to add them. A user have logged in? Log it. A user found a 404? Log it. The admin edited/deleted a post? Log it. Someone was able to access a restricted page? Log it.

Don't be surprised if your log file reaches 15+ Mb during your development phase. During beta, you can decide which logs to remove. If you want, you can add a flag to decide when a certain event is logged.


Statistics is hard :-). After a year of reading and re-reading books and papers and can only say with confidence that I understand the very basics of it.

You might wish to investigate ready-made libraries for whichever programming language you are using, because they are many gotcha's in math in general and statistics in particular (rounding errors being an obvious example).

As an example you could take a look at the R project, which is both an interactive environment and a library which you can use from your C++ code, distributed under the GPL (ie if you are using it only internally and publishing only the results, you don't need to open your code).


some day i work in a company that use VSS (and in other companies that use other less unknow SCM) but i prefer use SVN (someday i'll try GIT) for active development, for me and my group.

First of all, this situation it's only good idea, if commit to VSS are few over month, because working with other SCM (than VSS) give you more flexiblity, but commint to VSS from SVN is expensive in time.

My solution was:

VSS -> SVN: I have linux script (or ant script, or XXX script) that copy from currrent update directory work of VSS to current SVN, then refresh SVN client and update/merge/commit to SVN. With this, you are update from changes of the rest of company that use VSS.

SVN -> VSS: In this way, you need a checkout of all your modify files to VSS, then you can simply use the reverse script to copy from current update SVN directory (ignore .svn directories) and copy to current update VSS directory, update and commit.

But remember, in a few case does worth your time to do this.


No. For some things you will need the .net Framework (like reporting services), and you can't install it (in a supported way) in a server core.


I'm probably going a million miles in the wrong direct (but i'm only young :P ). but couldn't you add the graphic to a panel and then a mouselistener to the graphic object so that when the user on the graphic your action is preformed.


If you are querying a SQL Server database (Version 7 and up) you should replace the OleDb classes with corresponding classes in the System.Data.SqlClient namespace (SqlConnection, SqlCommand and SqlDataReader) as those classes have been optimized to work with SQL Server.

Another thing to note is that you should 'never' select all as this might lead to unexpected results later on if you add or remove columns to this table.


The problem is the strings in your struct. I found that marshaling types like byte/short/int is not a problem; but when you need to marshal into a complex type such as a string, you need your struct to explicitly mimic an unmanaged type. You can do this with the MarshalAs attrib.

For your example, the following should work:

[StructLayout(LayoutKind.Explicit)]
struct StructType
{
    [FieldOffset(0)]
    [MarshalAs(UnmanagedType.ByValTStr, SizeConst = 8)]
    public string FileDate;

    [FieldOffset(8)]
    [MarshalAs(UnmanagedType.ByValTStr, SizeConst = 8)]
    public string FileTime;

    [FieldOffset(16)]
    public int Id1;

    [FieldOffset(20)]
    [MarshalAs(UnmanagedType.ByValTStr, SizeConst = 66)] //Or however long Id2 is.
    public string Id2;
}



I have found that I also need to set accessibility.tabfocus to 7 in Firefox's about:config.


To get rid of the _AFXDLL error, have you tried changing to the settings to use MFC as a static lib instead of a DLL?  This is similar to what you're already doing in changing the runtime libs to static instead of DLL.


One consideration would be whether video playback is via progressive download or streaming. If it's progressive download, then I would say use Flash because you get a wider audience reach.

For streaming wmv, it is out of the box functionality provided by Windows Media Services

For streaming flash, you will have to install a streaming server on your Windows box. Some options are:


Adobe Flash Media Server (Commercial)
Wowza Media Server (Free/Commercial) 
Red5 Flash Server (Open Source)



There is another way which avoids tempdata. The pattern I like involves creating 1 action for both the original render and re-render of the invalid form. It goes something like this:

var form = new FooForm();

if (request.UrlReferrer == request.Url)
{
     // Fill form with previous request's data
}

if (Request.IsPost())
{
     if (!form.IsValid)
     {
         ViewData["ValidationErrors"] = ...
     } else {
         // update model
         model.something = foo.something;
         // handoff to post update action
         return RedirectToAction("ModelUpdated", ... etc);
     }
}

// By default render 1 view until form is a valid post
ViewData["Form"] = form;
return View();


That's the pattern more or less. A little pseudoy. With this you can create 1 view to handle rendering the form, re-displaying the values (since the form will be filled with previous values), and showing error messages.

When the posting to this action, if its valid it transfers control over to another action.

I'm trying to make this pattern easy in the .net validation framework as we build out support for MVC.


There's a couple of small cleanups you can make...

package
{
    import flash.filesystem.File;

    public class UserUtil
    {
        public static function get currentOSUser():String
        {
            var userDir:String = File.userDirectory.nativePath;
            var userName:String = userDir.substr(userDir.lastIndexOf(File.separator) + 1);
            return userName;
        }
    }
}


As Kevin suggested, use File.separator to make the directory splitting cross-platform (just tested on Windows and Mac OS X).

You don't need to use resolvePath("") unless you're looking for a child.

Also, making the function a proper getter allows binding without any further work.

In the above example I put it into a UserUtil class, now I can bind to UserUtil.currentOSUser, e.g:

&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;mx:WindowedApplication xmlns:mx="http://www.adobe.com/2006/mxml" layout="absolute"&gt;
    &lt;mx:Label text="{UserUtil.currentOSUser}"/&gt; 
&lt;/mx:WindowedApplication&gt;



I used the update url and I installed the JavaHL adapter, the Subclipse project itself and the SVNKit adapter BETA.

After this it worked fine for me, this is for linux platform hope it works for you.


I would strongly advocate you look at NetworkX. It's a battle-tested war horse and the first tool most 'research' types reach for when they need to do analysis of network based data. I have manipulated graphs with 100s of thousands of edges without problem on a notebook. Its feature rich and very easy to use. You will find yourself focusing more on the problem at hand rather than the details in the underlying implementation.

Example of Erdős-Rényi random graph generation and analysis


"""
Create an G{n,m} random graph with n nodes and m edges
and report some properties.

This graph is sometimes called the Erd##[m~Qs-Rényi graph
but is different from G{n,p} or binomial_graph which is also
sometimes called the Erd##[m~Qs-Rényi graph.
"""
__author__ = """Aric Hagberg (hagberg@lanl.gov)"""
__credits__ = """"""
#    Copyright (C) 2004-2006 by 
#    Aric Hagberg 
#    Dan Schult 
#    Pieter Swart 
#    Distributed under the terms of the GNU Lesser General Public License
#    http://www.gnu.org/copyleft/lesser.html

from networkx import *
import sys

n=10 # 10 nodes
m=20 # 20 edges

G=gnm_random_graph(n,m)

# some properties
print "node degree clustering"
for v in nodes(G):
    print v,degree(G,v),clustering(G,v)

# print the adjacency list to terminal 
write_adjlist(G,sys.stdout)


Visualizations are also straightforward:



More visualization: http://jonschull.blogspot.com/2008/08/graph-visualization.html


I think John is correct. 

"My main concern is that instantiating and disposing one huge DataContext class all the time for individual operations that relate to specific areas of the Database would be impose an unnecessary imposition on application resources"

How do you support that statement? What is your experiment that shows that a large DataContext is a performance bottleneck? Having multiple datacontexts is a lot like having multiple databases and makes sense in similar scenarios, that is, hardly ever. If you are working with multiple datacontexts you need to keep track of which objects belong to which datacontext and you can't relate objects that are not in the same data context. That is a costly design smell for no real benefit. 

@Evan "The DataContext (or Linq to Entities ObjectContext) is more of a "unit of work" than a connection"
That is precisely why you should not have more than one datacontext. Why would you want more that one "unit of work" at a time?


As already mentioned, NetworkX is very good, with another option being igraph. Both modules will have most (if not all) the analysis tools you're likely to need, and both libraries are routinely used with large networks.


Server Core won't be very useful (to me at least, and I think many others as well) until they get a version of .Net framework on it.  Maybe a specialized subset like they have in the Compact Framework on smart phones.


You can use a static class member to hold the default:

class Shipment
{
    public static $DefaultWeight = '0';
    public function createShipment($startZip,$endZip,$weight=Shipment::DefaultWeight) {
        // your function
    }
}



Neat trick with boolean OR operator:

public function createShipment($startZip, $endZip, $weight = 0){
    $weight or $weight = $this-&gt;getDefaultWeight();
    ...
}



\0 will also match the entire matched expression without doing an explicit capture using parenthesis.

preg_replace("/[A-Z]/", "&lt;span class=\"initial\"&gt;\\0&lt;/span&gt;", $str)


As always, you can go to php.net/preg_replace or php.net/&lt;whatever search term&gt; to search the documentation quickly. Quoth the documentation:


  \0 or $0 refers to the text matched by the whole pattern. 



Even if you didn't want to actually edit in VS, you could create the project there and edit the files in another editor.


This isn't a full answer for you, but on the left join piece you can use the DefaultIfEmpty operator like so:

var collection = 
from u in db.Universe
join history in db.History on u.id = history.id into temp
from h in temp.DefaultIfEmpty()
where h.dateCol &lt; DateTime.Now.Date.AddDays(-1)
select u.id, u.name, h.dateCol ?? '1900-01-01'


I haven't had the need to do any groupby commands yet, so I left that out as to not send you down the wrong path.  Two other quick things to note.  I have been unable to actually join on two parameters although as above there are ways to get around it.  Also, the ?? operator works really well in place of the isnull in SQL.  


if you're not going to be using mylyn just uncheck that dependency. I'm not really familiar with Aptana, but in eclipse you can expand whats being installed and uncheck anything you don't need.


Why use a BST at all? From your description a dictionary will work just as well, if not better. 

The only reason for using a BST would be if you wanted to list out the contents of the container in key order. It certainly doesn't sound like you want to do that, in which case go for the hash table. O(1) insertion and search, no worries about deletion, what could be better?


CC.NET is simply the build server technology, not the build script technology. We use CC.NET at work to very successfully call MSBuild build scripts with no problems.

NAnt is an older and more mature build scripting language, but they are both similar in how they work. There are very few things I could do in NAnt that I can't also do in MSBuild, so it really comes down to which one you are more comfortable with. As far as how active NAnt is, don't go by when the last release was...instead go by when the last nightly build was. NAnt tends to go a long time between releases, but the nightly builds are usually pretty stable.


I used to get these all the time on Apache1/fastcgi. I think it's caused by fastcgi hanging up before Ruby is done. 

Switching to mongrel is a good first step, but there's more to do. It's a bad idea to cull from web services on live pages, particularly from Rails. Rails is not thread-safe. The number of concurrent connections you can support equals the number of mongrels (or Passenger processes) in your cluster. 

If you have one mongrel and someone accesses a page that calls a web service that takes 10 seconds to time out, every request to your website will timeout during that time. Most of the load balancers just cycle through your mongrels blindly, so if you have two mongrels, every other request will timeout.

Anything that can be unpredictably slow needs to happen in a job queue. The first hit to /slow/action adds the job to the queue, and /slow/action keeps on refreshing via page refreshes or queries via ajax until the job is finished, and then you get your results from the job queue.  There are a few job queues for Rails nowadays, but the oldest and probably most widely used one is BackgroundRB.

Another alternative, depending on the nature of your app, is to cull the service every N minutes via cron, cache the data locally, and have your live page read from the cache. 


Your first step is to find and understand the parallelism in your problem. It is really easy to write multi-threaded code that performs no better than the single-threaded code it replaces. "Patterns for Parallel Programming" (Amazon) is a great introduction to the key concepts.

Once you have a workable design, start reading the articles in the "Concurrency" topic in the MSDN Magazine archives (link), particularly anything written by Jeff Richter. Those will give you the nuts and bolts stuff on the threading constructs specific to Windows and .NET. (The multi-threading section in Richter's "CLR via C# (Amazon)is short, but very insightful - highly recommended.)


There are many options and the best solution will depend on the nature of the problem you are trying to solve.  If you are trying to solve an embarassingly parallel problem then dividing and parallelising the tasks will be trivial.  In that case the challenge will come in distributing and managing the data used.  

Some suggestions would be:


ICE Grid which has bindings for .Net and other common languages
Velocity which is Microsoft's version of Oracle (Tangersol) Coherence
The forthcoming HPC offering from Microsoft Compute Cluster Server
Data Synapse Grid Server



There's some stuff in the Apache Portable Runtime (APR) that I'd expect to be very solid.


Check out Chapter 6 of Programming Collective Intelligence


From the Schema Spy documentation it supports multiple databases (see below). It also supports data types, constraints and maps relationships up to two degrees of separation.

The Samples are well worth looking at.

Schema Spy Supported Databases

IBM DB2 with the 'App' Driver,IBM DB2 with the 'Net' Driver, Firebird, HSQLDB Server,   Microsoft SQL Server, MySQL, Oracle with OCI8 Driver, Oracle with Thin Driver, PostgreSQL, Sybase Server with JDBC3 Driver, Sybase Server with JDBC2 Driver, DB2 UDB Type 4 Driver


I definitely recommend Weka which is an Open Source Data Mining Software written in Java:


  Weka is a collection of machine learning algorithms for data mining tasks. The algorithms can either be applied directly to a dataset or called from your own Java code. Weka contains tools for data pre-processing, classification, regression, clustering, association rules, and visualization. It is also well-suited for developing new machine learning schemes.


As mentioned above, it ships with a bunch of different classifiers like SVM, Winnow, C4.5, Naive Bayes (of course) and many more (see the API doc).
Note that a lot of classifiers are known to have much better perfomance than Naive Bayes in the field of spam detection or text classification.

Furthermore Weka brings you a very powerful GUI…


To make it more readable and maintainable, you can also split it up into multiple LINQ statements.


First, select your data into a new list, let's call it x1, do a projection if desired
Next, create a distinct list, from x1 into x2, using whatever distinction you require
Finally, create an ordered list, from x2 into x3, sorting by whatever you desire 



IE prior to 8 has a temporal aspect to its box model that most notably creates a problem with percentage based widths. In your case here an absolutely positioned div by default has no width. Its width will be worked out based on the pixel width of its content and will be calculated after the contents are rendered. So at the point IE encounters and renders your relatively positioned div its parent has a width of 0 hence why it itself collapses to 0.

If you would like a more in depth discussion of this along with lots of working examples, have a gander here.


Depending on what version of IIS you're considering, I would second lbrandy's recommendation to check out PowerShell. Microsoft is working on a PowerShell provider for IIS (specifically version 7). There is a decent post about this at http://blogs.iis.net/thomad/archive/2008/04/14/iis-7-0-powershell-provider-tech-preview-1.aspx. The upcoming version of PowerShell will also add remoting capabilities so that you can remotely manage machines. PowerShell is quite different from *NIX shells, though, so that is something to consider.

Hope this helps.


Like what so many people have already indicated, the answer here is "it depends". There are some things like repeating operations that are much simpler and cleaner in NAnt. See the MSDN forums for a discussion about this.


I am not aware of any API to do get the OS's scheduler to do what you want (even if your thread is idle-priority, if there are no higher-priority ready threads, yours will run).  However, I think you can improvise a fairly elegant throttling function based on what you are already doing. Essentially (I don't have a Windows dev machine handy):

Pick a default amount of time the thread will sleep each iteration.  Then, on each iteration (or on every nth iteration, such that the throttling function doesn't itself become a significant CPU load),


Compute the amount of CPU time your thread used since the last time your throttling function was called (I'll call this dCPU).  You can use the GetThreadTimes() API to get the amount of time your thread has been executing.
Compute the amount of real time elapsed since the last time your throttling function was called (I'll call this dClock).
dCPU / dClock is the percent CPU usage (of one CPU).  If it is higher than you want, increase your sleep time, if lower, decrease the sleep time.
Have your thread sleep for the computed time.


Depending on how your watchdog computes CPU usage, you might want to use GetProcessAffinityMask() to find out how many CPUs the system has.  dCPU / (dClock * CPUs) is the percentage of total CPU time available.

You will still have to pick some magic numbers for the initial sleep time and the increment/decrement amount, but I think this algorithm could be tuned to keep a thread running at fairly close to a determined percent of CPU.


The problem is it's not normal to want to leave the CPU idle while you have work to do. Normally you set a background task to IDLE priority, and let the OS handle scheduling it all the CPU time that isn't used by interactive tasks. 

It sound to me like the problem is the watchdog process. 

If your background task is CPU-bound then you want it to take all the unused CPU time for its task.

Maybe you should look at fixing the watchdog program?


What's your application domain? It depends. 

Since you used the word "Agile", I'm guessing it's a web app. I have a nice easy answer for you. 

Go buy a copy of Burp Suite (it's the #1 Google result for "burp" --- a sure endorsement!); it'll cost you 99EU, or ~$180USD, or $98 Obama Dollars if you wait until November. 

Burp works as a web proxy. You browse through your web app using Firefox or IE or whatever, and it collects all the hits you generate. These hits get fed to a feature called "Intruder", which is a web fuzzer. Intruder will figure out all the parameters you provide to each one of your query handlers. It will then try crazy values for each parameter, including SQL, filesystem, and HTML metacharacters. On a typical complex form post, this is going to generate about 1500 hits, which you'll look through to identify scary --- or, more importantly in an Agile context, new --- error responses.

Fuzzing every query handler in your web app at each release iteration is the #1 thing you can do to improve application security without instituting a formal "SDLC" and adding headcount. Beyond that, review your code for the major web app security hot spots:


Use only parameterized prepared SQL statements; don't ever simply concatenate strings and feed them to your database handle.
Filter all inputs to a white list of known good characters (alnum, basic punctuation), and, more importantly, output filter data from your query results to "neutralize" HTML metacharacters to HTML entities (quot, lt, gt, etc). 
Use long random hard-to-guess identifiers anywhere you're currently using simple integer row IDs in query parameters, and make sure user X can't see user Y's data just by guessing those identifiers.
Test every query handler in your application to ensure that they function only when a valid, logged-on session cookie is presented.
Turn on the XSRF protection in your web stack, which will generate hidden form token parameters on all your rendered forms, to prevent attackers from creating malicious links that will submit forms for unsuspecting users.
Use bcrypt --- and nothing else --- to store hashed passwords.



If you do go for creating several projects, make sure everyone who adds code to the solution is fully aware of the intention of them and do everything you can to get them to understand the dependencies between the projects. If you have ever tried to sort out the mess when someone has gone and added references that shouldn't have been there and got away with it for weeks you will understand this point


Tree Surgeon is a great tool which creates an empty .NET development tree.  It has been tweaked over years of use and implements lots of best practices.


There are some optimizations you can use when filling a DataTable, such as calling BeginLoadData(), inserting the data, then calling EndLoadData().  This turns off some internal behavior within the DataTable, such as index maintenance, etc.  See this article for further details.


On major difference is that DataSets can hold multiple tables and you can define relationships between those tables. 

If you are only retuning a single result set though I would think a DataTable would be more optimized. I would think there has to be some overhead (granted small) to offer the functionality a DataSet does and keep track of multiple DataTables. 


We use both Buildbot and Hudson for Jython development.  Both are useful, but have different strengths and weaknesses.

Buildbot's configuration is pure Python and quite simple once you get the hang of it (look at the epydoc-generated API docs for the most current info).  Buildbot makes it easier to define non-testing tasks and distribute the testers.  However, it really has no concept of individual tests, just textual, HTML, and summary output, so if you want to have multi-level browsable test output and so forth you'll have to build it yourself, or just use Hudson.

Hudson has terrific support for drilling down from overall results into test suites and individual tests; it also is great for comparing test output between builds, but the distributed (master/slave) stuff is comparatively more complicated because you need a Java environment on the slaves too; also, Hudson is less tolerant of flaky network links between the master and slaves.

So, to get the benefits of both tools, we run a single instance of Hudson, which catches the common test failures, then we do multi-platform regression with Buildbot.

Here are our instances:


Jython Hudson
Jython buildbot



Subclipse does not require Mylyn, but the update site includes a plugin that integrates Mylyn and Subclipse.  This is intended for people that use Mylyn.  In your case, you would want to just de-select Mylyn in the update dialog.

Subclipse also requires Subversion 1.5 and the corresponding version of the JavaHL native libraries.  I have written the start of an FAQ to help people understand JavaHL and how to get it.  See: http://desktop-eclipse.open.collab.net/wiki/JavaHL


SQL Server 2005 "Standard", "Developer" and "Enterprise" editions have SSIS, which replaced DTS from Sql server 2000. SSIS has a built in connection to it's own DB, and you can find a connection that someone else has written for MySQL. Here is one example. Once you have your connections, you should be able to create an SSIS package that moves data between the two. 

I have not had to move data from SQlServer to MySQL, but I imagine that once the MySQl connection is installed, it works the same as moving data between two SQLServer DBs, which is pretty straight forward.


Using MSSQL Management Studio i've transitioned tables with the MySQL OLE DB.  Right click on your database and go to "Tasks->Export Data" from there you can specify a MsSQL OLE DB source, the MySQL OLE DB source and create the column mappings between the two data sources.  

You'll most likely want to setup the database and tables in advance on the MySQL destination (the export will want to create the tables automatically, but this often results in failure). You can quickly create the tables in MySQL using the "Tasks->Generate Scripts" by right clicking on the database.  Once your creation scripts are generated you'll need to step through and search/replace for keywords and types that exist in MSSQL to MYSQL. 

Of course you could also backup the database like normal and find a utility which will restore the MSSQL backup on MYSQL. I'm not sure if one exists however.


You might also want to use something like BCEL to analyse which lines of source actually exist in the byte-code.  You don't want to report that things like blank lines and comments haven't been covered.


@Balloon
If you are using TortoiseSVN, you can use the packaged SubWCRev program. It queries a working copy and tells you just the highest revision number. Admittedly, this seems to be a client-side approach to a server-side problem, but since it's a nice command line program, you should be able to capture its output for use fairly easily.


If you are intending on reading a large number of columns or records it's also worth caching the ordinals and accessing the strongly-typed methods, e.g.

using (DbDataReader dr = cmd.ExecuteReader()) {
  if (dr.Read()) {
    int idxColumnName = dr.GetOrdinal("columnName");
    int idxSomethingElse = dr.GetOrdinal("somethingElse");

    do {
      Console.WriteLine(dr.GetString(idxColumnName));
      Console.WriteLine(dr.GetInt32(idxSomethingElse));
    } while (dr.Read());
  }
}



I really think it is better to split the project as well, but it all depends on the size of the project and the number of people working on it. 

For larger projects, I have a projects for  


data access (models)  
services  
front end  
tests  


I got the model from Rob Connery and his storefront application... seems to work really well. 

mvc-storefront


Really stupid question: Are you sure the string is being truncated, and not just broken at the linebreak you specify (and possibly not showing in your interface)? Ie, do you expect the field to show as


  This will be inserted \n This will not
  be


or 


  This will be inserted 
  
  This will not be


Also, what interface are you using? Is it possible that something along the way is eating your backslashes?


We are using Bitten wich is integrated with trac. And it's python based.


The way I understand things, Samba created tdb to allow "multiple concurrent writers" for any particular database file.  So if your workload has multiple writers your performance may be bad.  On the other hand, if your workload has lots of readers, then the question is how well your operating system handles multiple readers.


The closest you are going to get is using a Dictionary (as mentioned by Pacifika)

Dim objDictionary
Set objDictionary = CreateObject("Scripting.Dictionary")
objDictionary.CompareMode = vbTextCompare 'makes the keys case insensitive'
objDictionary.Add "Name", "Scott"
objDictionary.Add "Age", "20"


But I loop through my dictionaries like a collection

For Each Entry In objDictionary
  Response.write objDictionary(Entry) &amp; "&lt;br /&gt;"
Next


You can loop through the entire dictionary this way writing out the values which would look like this:

Scott
20


You can also do this

For Each Entry In objDictionary
  Response.write Entry &amp; ": " &amp; objDictionary(Entry) &amp; "&lt;br /&gt;"
Next


Which would produce

 Name: Scott
 Age: 20



In short: don't do this by hand, link/use existing software.  And sain_grocen's answer is incorrect.  :(

These are all tests for significance of parameter estimates that are typically used in Multivariate response Multiple Regressions.  These would not be simple things to do outside of a statistical programming environment.  I would suggest either getting the output from a pre-existing statistical program, or using one that you can link to and use that code.

I'm afraid that the first answer (sain_grocen's) will lead you down the wrong path.  His explanation is likely of a special case of what you are actually dealing with.  The anova explained in his links is for a single variate response, in a balanced design.  These aren't the F statistics you are seeing.  The names in your output (Pillai's Trace, Hotelling's Trace,...) are some of the available multivariate versions.  They have F distributions under certain assumptions.  I can't explain a text books worth of material here, I would advise you to start by looking at 
"Applied Multivariate Statistical Analysis" by Johnson and Wichern


While it's possible to create lots of test data and see what happens, it's more efficient to try to minimize the data being used.

From a typical QA perspective, you would want to identify different classifications of inputs. Produce a set of input values for each classification and determine the appropriate outputs. 

Here's a sample of classes of input values


valid triangles with small numbers such as (1 billion, 2, billion, 2 billion)
valid triangles with large numbers such as (0.000001, 0.00002, 0.00003)
valid obtuse triangles that are 'almost'flat such as (10, 10, 19.9999)
valid acute triangles that are 'almost' flat such as (10, 10, 0000001)
invalid triangles with at least one negative value
invalid triangles where the sum of two sides equals the third
invalid triangles where the sum of two sides is greater than the third
input values that are non-numeric


...

Once you are satisfied with the list of input classifications for this function, then you can create the actual test data.  Likely, it would be helpful to test all permutations of each item.  (e.g. (2,3,4), (2,4,3), (3,2,4), (3,4,2), (4,2,3), (4,3,2)) Typically, you'll find there are some classifications you missed (such as the concept of inf as an input parameter).

Random data for some period of time may be helpful as well, that can find strange bugs in the code, but is generally not productive.

More likely, this function is being used in some specific context where additional rules are applied.(e.g. only integer values or values must be in 0.01 increments, etc.) These add to the list of classifications of input parameters.


You're going to want to use the join into construct to create a group query.

TestContext db = new TestContext(CreateSparqlTripleStore());
var q = from a in db.Album
        join t in db.Track on a.Name equals t.AlbumName into tracks
        select new Album{Name = a.Name, Tracks = tracks};
foreach(var album in q){
    Console.WriteLine(album.Name);
    foreach (Track track in album.Tracks)
    {
        Console.WriteLine(track.Title);
    }
}



Can you explain more why SPSS itself isn't a fine solution to the problem?  Is it that it generates pivot tables as output that are hard to manipulate?  Is it the cost of the program?  

F-statistics can arise from any number of particular tests.  The F is just a distribution (loosely:  a description of the "frequencies" of groups of values), like a Normal (Gaussian), or Uniform.  In general they arise from ratios of variances.  Opinion: many statisticians (myself included), find F-based tests to be unstable (jargon:  non-robust).

The particular output statistics (Pillai's trace, etc.) suggest that the original analysis is a MANOVA example, which as other posters describe is a complicated, and hard to get right procedure.

I'm guess also that, based on the MANOVA, and the use of SPSS, this is a psychology or sociology project... if not please enlighten.  It might be that other, simpler models might actually be easier to understand and more repeatable.  Consult your local university statistical consulting group, if you have one.  

Good luck!  


It should be pointed out that you don't want to encrypt the password, you want to hash it.

Encrypted passwords can be decrypted, letting someone see the password. Hashing is a one-way operation so the user's original password is (cryptographically) gone.



As for which algorithm you should choose - use the currently accepted standard one:


SHA-256


And when you hash the user's password, be sure to also hash in some other junk with it. e.g.:


password: password1
salt: PasswordSaltDesignedForThisQuestion


Append the salt to the user's password: 

String s = HashStringSHA256("password1PasswordSaltDesignedForThisQuestion");



Whatever you do, don't write your own encryption algorithm.  Doing this will almost guarantee (unless you're a cryptographer) that there will be a flaw in the algorithm that will make it trivial to crack.


Want to slow down the hashing function?


$seed = sha1(uniqid(rand(), true));
$hash = sha1($seed . $password);
for($k=0; $k

I think there's enough hashing to slow down the algorithm a little, and making a very strong hash.


If you use Firebird or InterBase, there's a tool called IBDesc that does a great job.


Don't waste your time on SPAM filtering usages.
Spammers easily bypass Bayesian filtering by adding random text to their spam emails.

FIX:
OK,OK, Bayesian filtering can be useful when trained personally.
However, at a corporate level or above, its probably useless.


Although I'm not familiar with afxext.h, I am wondering what about it makes it incompatible with Windows NT4.... 

However, to answer the original question:
"My research to date indicates that it is impossible to build an application for execution on Windows NT 4.0 using Visual Studio (C++, in this case) 2005."

The answer should be yes especially if the application was originally written or running on NT4!  With the afxext.h thing aside, this should be an easy YES.

The other thing I am finding trouble with is the loose nature in which people are throwing out the NT term.  Granted most people think of 'NT' as Windows NT4 but it's still ambiguous because 'most people' is not equal to 'all people.'

In reality the term 'NT' is equal to the NT series. The NT series is NT3, NT4, NT5 (2000, XP, 2003) and NT6 (Vista).

Win32 is a subsystem which you target your C/C++ code too.  So I see no reason why one should not be able target this NT4 platform &amp; subsystem or, if this is a platform porting excercise, remove the MFC dependencies that VC is possibly imposing.

Adding the afxext.h to the mix, it sounds to me like a subsystem compatibility issue.  It's part of MFC from my Google research.  The afxext.h appears to be the MFC (Microsoft Foundation Class) extensions.

Can you remove your dependency on MFC?  What type of application is this? (CLR, service, GUI interface?)  Can you convert project to an unmanaged C++ project in VC 8.0?

Hopefully some of this will help you along.


You should also use the Sandcastle Help File Builder. It provides you with a ndoc like GUI for generating help files so you don't have to do anything from a command prompt.

Welcome to the Sandcastle Help File Builder Project


Best option: I would instead recommend to use a standard date picker.

Alternative: every time the content of the edit control changes, parse it and display (in a separate control?) the long format of the date (ie: input "03/04/09" display "Your input: March 4, 2009")


I find it highly unlikely for Postgres to truncate your data on input - it either rejects it or stores it as is.

milen@dev:~$ psql
Welcome to psql 8.2.7, the PostgreSQL interactive terminal.

Type:  \copyright for distribution terms
       \h for help with SQL commands
       \? for help with psql commands
       \g or terminate with semicolon to execute query
       \q to quit

milen=&gt; create table EscapeTest (text varchar(50));
CREATE TABLE
milen=&gt; insert into EscapeTest (text) values ('This will be inserted \n This will not be');
WARNING:  nonstandard use of escape in a string literal
LINE 1: insert into EscapeTest (text) values ('This will be inserted...
                                              ^
HINT:  Use the escape string syntax for escapes, e.g., E'\r\n'.
INSERT 0 1
milen=&gt; select * from EscapeTest;
          text
------------------------
 This will be inserted
  This will not be
(1 row)

milen=&gt;



An alternative to FlexUnit is the AsUnit testing tools. There are versions for actionscript 2 and 3. It also has good integration with Project Sprouts, which is a build tool for Flex and Flash similar to ant, however it uses ruby rake tasks and includes excellent dependency management along the lines of maven.

No IDE integration that I know of however.


On Solaris, you can get detailed information on a process's memory usage with the pmap command. In particular, pmap -x &lt;pid&gt; shows you how much of a process's memory is shared and how much is specifically used by that process. This is useful for working out the "marginal" memory usage of a process -- with this technique you can avoid double-counting shared libraries.


Unhandled exception behavior in a .NET 1.x WinForms app depends on:


The type of thread that threw the exception
Whether it occurred during window message processing
Whether a debugger was attached to the process
The DbgJitDebugLaunchSetting registry setting
The jitDebugging flag in App.Config
Whether you overrode the WinForms exception handler
Whether you handled the CLR’s exception event
The phase of the moon


The default behaviour of unhandled exceptions is:


If the exception occurs on the main thread when pumping window messages, it's intercepted by the Windows Forms exception handler.
If the exception occurs on the main thread when pumping window messages, it will terminate the app process unless it's intercepted by the Windows Forms exception handler.
If the exception occurs on a manual, threadpool, or finalizer thread, it's swallowed by the CLR.


The points of contact for an unhandled exception are: 


Windows Forms exception handler.
The JIT-debug registry switch DbgJitDebugLaunchSetting.
The CLR unhandled exception event.


The Windows Form built-in exception handling does the following by default:


Catches an unhandled exception when:

exception is on main thread and no debugger attached.
exception occurs during window message processing.
jitDebugging = false in App.Config.

Shows dialog to user and prevents app termination.


You can disable the latter behaviour by setting jitDebugging = true in App.Config. But remember that this may be your last chance to stop app termination. So the next step to catch an unhandled exception is registering for event Application.ThreadException, e.g. :

Application.ThreadException += new
Threading.ThreadExceptionHandler(CatchFormsExceptions);


Note the registry setting DbgJitDebugLaunchSetting under HKEY_LOCAL_MACHINE\Software.NetFramework. This has one of three values of which I'm aware:


0: shows user dialog asking "debug or terminate".
1: lets exception through for CLR to deal with.
2: launches debugger specified in DbgManagedDebugger registry key.


In Visual Studio, go to Tools>Options>Debugging>JIT to set this key to 0 or 2. But a value of 1 is usually best on an end-user's machine. Note that this registry key is acted on before the CLR unhandled exception event.

This last event is your last chance to log an unhandled exception. It's triggered before your Finally blocks have executed. You can intercept this event as follows:

AppDomain.CurrentDomain.UnhandledException += new
System.UnhandledExceptionEventHandler(CatchClrExceptions);



I don't know the specific tools, but there are some utilities that record / replay clicks. In other words, you could automate the "click" on the print dialog.  (I know this is a hack, but when all else fails...)


I wrote a blog post about this issue. 

The bottom line is that if you want cheap updates ... and you want to be safe for concurrent usage. try: 

update t
set hitCount = hitCount + 1
where pk = @id

if @@rowcount &lt; 1 
begin 
   begin tran
      update t with (serializable)
      set hitCount = hitCount + 1
      where pk = @id
      if @@rowcount = 0
      begin
         insert t (pk, hitCount)
         values (@id,1)
      end
   commit tran
end


This way you have 1 operation for updates and a max of 3 operations for inserts. so, if you are generally updating this is a safe cheap option. 

I would also be very careful not to use anything that is unsafe for concurrent usage. Its really easy to get primary key violations or duplicate rows in production. 


I assume from your question that your research colleagues want to automate the process by which certain statistical analyses are performed (i.e., they want to batch process data sets).  You have two options:

1) SPSS is now scriptable through python (as of version 15) - go to spss.com and search for python.  You can write python scripts to automate data analyses and extract key values from pivot tables, and then process the answers any way you like.  This has the virtue of allowing an exact comparison between the results from your python script and the hand-calculated efforts in SPSS of your collaborators.  Thus you won't have to really know any statistics to do this work (which is a key advantage)

2) You could do this in R, a free statistics environment, which could probably be scripted.  This has the disadvantage that you will have to learn statistics to ensure that you are doing it correctly.


TeamCity has some Python integration.

But TeamCity is:


not open-source
is not small, but rather feature rich
is free for small-mid teams.



As Brett said, its better to use a vb component to create collections. Dictionary objects are not very commonly used in ASP unless for specific need based applications. 


There is a good reason to NOT USE For i = LBound(arr) To UBound(arr)

dim arr(10) allocates eleven members of the array, 0 through 10 (assuming the VB6 default Option Base).

Many VB6 programmers assume the array is one-based, and never use the allocated arr(0). We can remove a potential source of bugs by using For i = 1 To UBound(arr) or For i = 0 To UBound(arr), because then it is clear whether arr(0) is being used.

For each makes a copy of each array element, rather than a pointer. 

This has two problems. 


When we try to assign a value to an array element, it doesn't reflect on original. This code assigns a value of 47 to the variable i, but does not affect the elements of arr.

for each i in arr
  i = 47
next i
We don't know the index of an array element in a for each, and we are not guaranteed the sequence of elements (although it seems to be in order.)



From your brief description, it sounds like protocol buffers is not the right fit.  The phrase "structured content created by hand in a text editor" pretty much screams for XML.

But if you want efficient, low latency communications with data structures that are not shared outside your organization, binary serialization such as protocol buffers can offer a huge win.


The trouble you have is that there could be situations where the answer could be all three types.

3 could be an int, a double or a string!

It depends upon what you are trying to do and how important it is that they are a particular type. It might be best just to leave them as they are as long as you can or, alternatively, some up with a method to mark each one (if you have control of the source of the original string).


Reading straight into structs is evil - many a C program has fallen over because of different byte orderings, different compiler implementations of fields, packing, word size.......

You are best of serialising and deserialising byte by byte. Use the build in stuff if you want or just get used to BinaryReader.


You shouldn't be using an exception here. This obviously isn't an exceptional case if you need to be expecting it everywhere you use this function!

A better solution would be to get the function to return an instance of something like this. In debug builds (assuming developers exercise code paths they've just written), they'll get an assert if they forget to check whether the operation succeded or not.

class SearchResult
{
  private:
    ResultType result_;
    bool succeeded_;
    bool succeessChecked_;

  public:
    SearchResult(Result&amp; result, bool succeeded)
      : result_(result)
      , succeeded_(succeeded)
      , successChecked_(false)
    {
    }

    ~SearchResult()
    {
      ASSERT(successChecked_);
    }

    ResultType&amp; Result() { return result_; }
    bool Succeeded() { successChecked_ = true; return succeeded_; }
}



LBound may not always be 0.  

Whilst it is not possible to create an array that has anything other than a 0 Lower bound in VBScript, it is still possible to retrieve an array of variants from a COM component which may have specified a different LBound.

That said I've never come across one that has done anything like that.


I find that you can also use a hybrid approach too, especially in larger projects.  A lot of our nant scripts are being converted to msbuild when new components are developed.  Both support the same major features and can call each other if you find a task that is natively supported in one but not the other.

For new .NET development starting with MSBuild can save you a lot of time since it can run the solution files directly.  Extending from the main compilation to perform other tasks (source control, deployment, etc) works quite well.


VS.NET defaults the Assembly version to 1.0.* and uses the following logic when auto-incrementing: it sets the build part to the number of days since January 1st, 2000, and sets the revision part to the number of seconds since midnight, local time, divided by two. See this MSDN article.

Assembly version is located in an assemblyinfo.vb or assemblyinfo.cs file. From the file: 

' Version information for an assembly consists of the following four values:
'
'      Major Version
'      Minor Version 
'      Build Number
'      Revision
'
' You can specify all the values or you can default the Build and Revision Numbers 
' by using the '*' as shown below:
' &lt;Assembly: AssemblyVersion("1.0.*")&gt; 

&lt;Assembly: AssemblyVersion("1.0.0.0")&gt; 
&lt;Assembly: AssemblyFileVersion("1.0.0.0")&gt; 



To add to @BradWilson's answer: "You could also get your source control provider to provide the source revision number if you want"

To connect Subversion and MSBuild:
MSBuild Community Tasks Project 


With the brand new Python 2.6, you have a standard solution with the itertools module that returns the Cartesian product of iterables :

import itertools

print list(itertools.product([1,2,3], [4,5,6]))
   [(1, 4), (1, 5), (1, 6),
   (2, 4), (2, 5), (2, 6),
   (3, 4), (3, 5), (3, 6)]


You can provide a "repeat" argument to perform the product with an iterable and itself:

print list(itertools.product([1,2], repeat=3))
[(1, 1, 1), (1, 1, 2), (1, 2, 1), (1, 2, 2),
(2, 1, 1), (2, 1, 2), (2, 2, 1), (2, 2, 2)]


You can also tweak something with combinations as well :

print list(itertools.combinations('123', 2))
[('1', '2'), ('1', '3'), ('2', '3')]


And if order matters, there are permutations :

print list(itertools.permutations([1,2,3,4], 2))
[(1, 2), (1, 3), (1, 4),
   (2, 1), (2, 3), (2, 4),
   (3, 1), (3, 2), (3, 4),
   (4, 1), (4, 2), (4, 3)]


Of course all that cool stuff don't exactly do the same thing, but you can use them in a way or another to solve you problem. 

Just remember that you can convert a tuple or a list to a set and vice versa using list(), tuple() and set().


Subversion 1.5 introduced write through proxy support for webdav servers over the existing SvnSync support that was added in 1.4. This allows you to have local mirrors for retrieving files and history, but commits are committed directly to the master repository. If setup correctly the local mirrors receive the changes immediately.

See the Svn Book for more details.


I'd hate to be Captain Obvious here...but what about a plain old .NET Gridview control?  You can copy Excel data into it and out of it...and you can run it on any system with the .NET platform installed.


There must be a method in Cocoa to get a list of fonts, then you would have to use the PyObjC bindings to call it..

Depending on what you need them for, you could probably just use something like the following..

import os
def get_font_list():
    fonts = []
    for font_path in ["/Library/Fonts", os.path.expanduser("~/Library/Fonts")]:
        if os.path.isdir(font_path):
            fonts.extend(
                [os.path.join(font_path, cur_font) 
                 for cur_font in os.listdir(font_path)
                ]
            )
    return fonts



I strongly agree with Daan's point: create a test program, and make sure the way in which it accesses data mimics as closely as possible the patterns you expect your application to have. This is extremely important with BDB because different access patterns yield very different throughput.

Other than that, these are general factors I found to be of major impact on throughput:


Access method (which in your case i guess is BTREE).
Level of persistency with which you configured DBD (for example, in my case the 'DB_TXN_WRITE_NOSYNC' environment flag improved write performance by an order of magnitude, but it compromises persistency) 
Does the working set fit in cache?
Number of Reads Vs. Writes.
How spread out your access is (remember that BTREE has a page level locking - so accessing different pages with different threads is a big advantage).
Access pattern - meanig how likely are threads to lock one another, or even deadlock, and what is your deadlock resolution policy (this one may be a killer).
Hardware (disk &amp; memory for cache).


This amounts to the following point:
Scaling a solution based on DBD so that it offers greater concurrency has two key ways of going about it; either minimize the number of locks in your design or add more hardware.


The workaround is to fix the multi-threaded DLL. Simple instructions

Once you've got those working, coming up with a more generic solution should be trivial.


I would recommend that you pull up the C5 Library.  Unlike SCG (System.Collections.Generic), C5 is programmed to interface and designed to be subclassed.  Most public methods are virtual and none of the classes are sealed.  This way, you won't have to use that icky "new" keyword which wouldn't trigger if your LimitedQueue&lt;T&gt; were cast to a SCG.Queue&lt;T&gt;.  With C5 and using close to the same code as you had before, you would derive from the CircularQueue&lt;T&gt;.  The CircularQueue&lt;T&gt; actually implements both a stack and a queue, so you can get both options with a limit nearly for free.  I've rewritten it below with some 3.5 constructs:

using C5;

public class LimitedQueue&lt;T&gt; : CircularQueue&lt;T&gt;
{
    public int Limit { get; set; }

    public LimitedQueue(int limit) : base(limit)
    {
        this.Limit = limit;
    }

    public override void Push(T item)
    {
        CheckLimit(false);
        base.Push(item);
    }

    public override void Enqueue(T item)
    {
        CheckLimit(true);
        base.Enqueue(item);
    }

    protected virtual void CheckLimit(bool enqueue)
    {
        while (this.Count &gt;= this.Limit)
        {
            if (enqueue)
            {
                this.Dequeue();
            }
            else
            {
                this.Pop();
            }
        }
    }
}


I think that this code should do exactly what you were looking for.


If you're talking about ColdFusion (which I assume you are from the tags) then I'm not sure this is doable but I may be very wrong here...

IIRC, When CF compiles it essentially compiles into a interpreted form of the CFML as a plain old java source file, this is then compiled into the class.  Therefore, any instrumentation that you may have will apply to the intermediary version rather than the CFML itself.

Saying that though, Adobe have got the CF debugger now which can step though code, so please prove me wrong - I'd love code coverage in CFML.


This episode of HanselMinutes covers exactly what I was hoping to hear. Apparently Git can be used locally then attached to external subversion/vss repositories as need. They talk about it 14 ~ 15 minutes in. 


