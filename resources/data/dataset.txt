I have an absolutely positioned div containing several children, one of which is a relatively positioned div. When I use a percentage-based width on the child div, it collapses to '0' width on Internet&nbsp;Explorer&nbsp;7, but not on Firefox or Safari.

If I use pixel width, it works. If the parent is relatively positioned, the percentage width on the child works.


Is there something I'm missing here?
Is there an easy fix for this besides the pixel-based width on the
child?
Is there an area of the CSS specification that covers this?



How do you expose a LINQ query as an ASMX web service? Usually, from the business tier, I can return a typed DataSet or DataTable which can be serialized for transport over ASMX.

How can I do the same for a LINQ query? Is there a way to populate a typed DataSet or DataTable via a LINQ query?: 

public static MyDataTable CallMySproc()    
{    
    string conn = ...;

    MyDatabaseDataContext db = new MyDatabaseDataContext(conn);    
    MyDataTable dt = new MyDataTable();

    // execute a sproc via LINQ
    var query = from dr in db.MySproc().AsEnumerable
    select dr;

    // copy LINQ query resultset into a DataTable -this does not work !    
    dt = query.CopyToDataTable();

    return dt;
}


How can I get the resultset of a LINQ query into a DataSet or DataTable? Alternatively, is the LINQ query serializeable so that I can expose it as an ASMX web service?


If I have a trigger before the update on a table, how can I throw an error that prevents the update on that table?


Let's say I have a DataTable with a Name column. I want to have a collection of the unique names ordered alphabetically. The following query ignores the order by clause.

var names =
    (from DataRow dr in dataTable.Rows
    orderby (string)dr["Name"]
    select (string)dr["Name"]).Distinct();


Why does the orderby not get enforced?


How do you page through a collection in LINQ given that you have a startIndex and a count?


The version of Subclipse (1.2.4) currently available through Aptana's automatic Plugins Manager does not work with the newest version of Subversion.

I see on the Subclipse website however that they have 1.4.2 out for Eclipse. So I added a new remote update site to my Update manager. When I tried to install it, it told me I needed Mylyn 3.0.0. So after much searching I found Mylyn 3.0.0 and added another new remote update site to my update manager. Then when I tried to install that, it told me I needed org.eclipse.ui 3.3.0 or equivalent.

Looking at the configuration details for Aptana, it looks like it is built against eclipse 3.2.2.

Does anyone know if there is a way to upgrade the version of Eclipse Aptana that is built against to 3.3.0? Or if there is some other way to get Subclipse to work with the very newest version of Subversion?

I know this isn't necessarily a "programming" question, but I hope it's ok since it's highly relevant to the programming experience.


I've been banging my head against SQL Server 2005 trying to get a lot of data out.  I've been given a database with nearly 300 tables in it and I need to turn this into a MySQL database.  My first call was to use bcp but unfortunately it doesn't produce valid CSV - strings aren't encapsulated, so you can't deal with any row that has a string with a comma in it (or whatever you use as a delimiter) and I would still have to hand write all of the create table statements, as obviously CSV doesn't tell you anything about the data types.

What would be better is if there was some tool that could connect to both SQL Server and MySQL, then do a copy. You lose views, stored procedures, trigger, etc, but it isn't hard to copy a table that only uses base types from one DB to another... is it?

Does anybody know of such a tool?  I don't mind how many assumptions it makes or what simplifications occur, as long as it supports integer, float, datetime and string. I have to do a lot of pruning, normalising, etc. anyway so I don't care about keeping keys, relationships or anything like that, but I need the initial set of data in fast!


Stack Overflow has a subversion version number at the bottom:


  svn revision: 679


I want to use such automatic versioning with my .NET Web Site/Application, Windows Forms, WPD projects/solutions.

How do I implement this?


I want to print HTML from a C# web service.  The Web Browser control is overkill, and does not function well in a service environment, nor does it function well on a system with very tight security constraints.  Is there any sort of free .NET library that will support the printing of a basic HTML page?  Here is the code I have so far, which does not run properly.

public void PrintThing(string document)
{
    if (Thread.CurrentThread.GetApartmentState() != ApartmentState.STA)
    {
        Thread thread =
            new Thread((ThreadStart) delegate { PrintDocument(document); });
        thread.SetApartmentState(ApartmentState.STA);
        thread.Start();
    }
    else
    {
        PrintDocument(document);
    }
}

protected void PrintDocument(string document)
{
    WebBrowser browser = new WebBrowser();
    browser.DocumentText = document;
    while (browser.ReadyState != WebBrowserReadyState.Complete)
    {
        Application.DoEvents();
    }
    browser.Print();
}


This works fine when called from UI-type threads, but nothing happens when called from a service-type thread.  Changing Print() to ShowPrintPreviewDialog() yields the following IE script error:


  Error: 'dialogArguments.___IE_PrintType' is null or not an object
  URL: res://ieframe.dll/preview.dlg


And a small empty print preview dialog appears.


One of the fun parts of multi-cultural programming is number formats.


Americans use 10,000.50
Germans use 10.000,50
French use 10 000,50


My first approach would be to take the string, parse it backwards, until I encounter a separator and use this as my decimal separator. There is an obvious flaw with that: 10.000 would be interpreted as 10.

Another approach: if the string contains 2 different non-numeric characters, use the last one as the decimal separator and discard the others. If I only have one, check if it occurs more than once and discard it if it does. If it only appears once, check if it has 3 digits after it. If yes, discard it, otherwise use it as decimal separator.

The obvious "best solution" would be to detect the User's culture or Browser, but that does not work if you have a Frenchman using an en-US Windows/Browser.

Does the .net Framework contain some mythical black magic floating point parser that is better than Double.(Try)Parse() in trying to auto-detect the number format?


Yes, I know.  The existence of a running copy of SQL Server 6.5 in 2008 is absurd.

That stipulated, what is the best way to migrate from 6.5 to 2005?  Is there any direct path?  Most of the documentation I've found deals with upgrading 6.5 to 7.

Should I forget about the native SQL Server upgrade utilities, script out all of the objects and data, and try to recreate from scratch?

I was going to attempt the upgrade this weekend, but server issues pushed it back till next.  So, any ideas would be welcomed during the course of the week.

Update.  This is how I ended up doing it:


Back up the database in question and Master on 6.5.
Execute SQL Server 2000's instcat.sql against 6.5's Master.  This allows SQL Server 2000's OLEDB provider to connect to 6.5.
Use SQL Server 2000's standalone "Import and Export Data" to create a DTS package, using OLEDB to connect to 6.5.  This successfully copied all 6.5's tables to a new 2005 database (also using OLEDB).
Use 6.5's Enterprise Manager to script out all of the database's indexes and triggers to a .sql file.
Execute that .sql file against the new copy of the database, in 2005's Management Studio.
Use 6.5's Enterprise Manager to script out all of the stored procedures.
Execute that .sql file against the 2005 database.  Several dozen sprocs had issues making them incompatible with 2005.  Mainly non-ANSI joins and quoted identifier issues.
Corrected all of those issues and re-executed the .sql file.
Recreated the 6.5's logins in 2005 and gave them appropriate permissions.


There was a bit of rinse/repeat when correcting the stored procedures (there were hundreds of them to correct), but the upgrade went great otherwise.

Being able to use Management Studio instead of Query Analyzer and Enterprise Manager 6.5 is such an amazing difference.  A few report queries that took 20-30 seconds on the 6.5 database are now running in 1-2 seconds, without any modification, new indexes, or anything.  I didn't expect that kind of immediate improvement.


MySQL has this incredibly useful yet properitary REPLACE INTO SQL Command. 

I wonder: Can this easily be emulated in SQL Server 2005?

Starting a new Transaction, doing a Select() and then either UPDATE or INSERT and COMMIT is always a little bit of a pain, especially when doing it in the application and therefore always keeping 2 versions of the statement.

I wonder if there is an easy and universal way to implement such a function into SQL Server 2005?



What's the optimal level of concurrency that the C++ implementation of BerkeleyDB can reasonably support?
How many threads can I have hammering away at the DB before throughput starts to suffer because of resource contention?


I've read the manual and know how to set the number of locks, lockers, database page size, etc., but I'd just like some advice from someone who has real-world experience with BDB concurrency.

My application is pretty simple, I'll be doing gets and puts of records that are about 1KB each. No cursors, no deleting.


What are the best practices for checking in BIN directories in a collaborative development environment using SVN?  Should project level references be excluded from checkin?  Is it easier to just add all bin directories?

I develop a lot of DotNetNuke sites and it seems that in a multi-developer environment, it's always a huge task to get the environment setup correctly.

The ultimate goal (of course) is to have a new developer checkout the trunk from SVN, restore the DNN database and have it all just 'work'...


When is it appropriate to use an unsigned variable over a signed one? What about in a for loop?

I hear a lot of opinions about this and I wanted to see if there was anything resembling a consensus. 

for (unsigned int i = 0; i &lt; someThing.length(); i++) {  
    SomeThing var = someThing.at(i);  
    // You get the idea.  
}


I know Java doesn't have unsigned values, and that must have been a concious decision on Sun Microsystems' part. 


I am using the Photoshop's javascript API to find the fonts in a given PSD.

Given a font name returned by the API, I want to find the actual physical font file that that font name corresponds to on the disc.

This is all happening in a python program running on OSX so I guess I'm looking for one of:


Some Photoshop javascript
A Python function
An OSX API that I can call from python



I have a Ruby on Rails Website that makes HTTP calls to an external Web Service.

About once a day I get a SystemExit (stacktrace below) error email where a call to the service has failed.  If I then try the exact same query on my site moments later it works fine.
It's been happening since the site went live and I've had no luck tracking down what causes it.

Ruby is version 1.8.6 and rails is version 1.2.6.

Anyone else have this problem?

This is the error and stacktrace.


A SystemExit occurred
/usr/local/lib/ruby/gems/1.8/gems/rails-1.2.6/lib/fcgi_handler.rb:116:in `exit'
/usr/local/lib/ruby/gems/1.8/gems/rails-1.2.6/lib/fcgi_handler.rb:116:in `exit_now_handler'
/usr/local/lib/ruby/gems/1.8/gems/activesupport-1.4.4/lib/active_support/inflector.rb:250:in `to_proc'
/usr/local/lib/ruby/1.8/net/protocol.rb:133:in `call'
/usr/local/lib/ruby/1.8/net/protocol.rb:133:in `sysread'
/usr/local/lib/ruby/1.8/net/protocol.rb:133:in `rbuf_fill'
/usr/local/lib/ruby/1.8/timeout.rb:56:in `timeout'
/usr/local/lib/ruby/1.8/timeout.rb:76:in `timeout'
/usr/local/lib/ruby/1.8/net/protocol.rb:132:in `rbuf_fill'
/usr/local/lib/ruby/1.8/net/protocol.rb:116:in `readuntil'
/usr/local/lib/ruby/1.8/net/protocol.rb:126:in `readline'
/usr/local/lib/ruby/1.8/net/http.rb:2017:in `read_status_line'
/usr/local/lib/ruby/1.8/net/http.rb:2006:in `read_new'
/usr/local/lib/ruby/1.8/net/http.rb:1047:in `request'
/usr/local/lib/ruby/1.8/net/http.rb:945:in `request_get'
/usr/local/lib/ruby/1.8/net/http.rb:380:in `get_response'
/usr/local/lib/ruby/1.8/net/http.rb:543:in `start'
/usr/local/lib/ruby/1.8/net/http.rb:379:in `get_response'



I'm starting work on a hobby project with a python codebase and would like to set up some form of continuous integration (i.e. running a battery of test-cases each time a check-in is made and sending nag e-mails to responsible persons when the tests fail) similar to CruiseControl or TeamCity.

I realize I could do this with hooks in most VCSes, but that requires that the tests run on the same machine as the version control server, which isn't as elegant as I would like. Does anyone have any suggestions for a small, user-friendly, open-source continuous integration system suitable for a Python codebase?


I would like to test a function with a tuple from a set of fringe cases and normal values. For example, while testing a function which returns true whenever given three lengths that form a valid triangle, I would have specific cases, negative / small / large numbers, values close-to being overflowed, etc.; what is more, main aim is to generate combinations of these values, with or without repetition, in order to get a set of test data.

(inf,0,-1), (5,10,1000), (10,5,5), (0,-1,5), (1000,inf,inf),
...


As a note: I actually know the answer to this, but it might be helpful for others, and a challenge for people here! --will post my answer later on.


I'm looking for a performant, reasonably robust RNG using no special hardware. It can use mathematical methods (Mersenne Twister, etc), it can "collect entropy" from the machine, whatever. On Linux/etc we have a drand48() which generates 48 random bits. I'd like a similar function/class for C++ or C# which can generate more than 32 bits of randomness and which low-order bits are equally as random as high-order bits.

It doesn't have to be cryptographically secure but it must not use or be based on the C-language rand() or .NET System.Random.

Any source code, links to source, etc. would be appreciated! Failing that, what TYPE of RNG should I be looking for?


An MFC application that I'm trying to migrate uses afxext.h, which causes _AFXDLL to get set, which causes this error if I set /MT:


  Please use the /MD switch for _AFXDLL builds


My research to date indicates that it is impossible to build an application for execution on Windows NT 4.0 using Visual Studio (C++, in this case) 2005.

Is this really true? Are there any workaround available?


I would like the version property of my application to be incremented for each build but I'm not sure on how to enable this functionality in Visual Studio (2005/2008). I have tried to specify the AssemblyVersion as 1.0.* but it doesn't get me exactly what I want. 

I'm also using a settings file and in earlier attempts when the assembly version changed my settings got reset to the default since the application looked for the settings file in another directory. 

I would like to be able to display a version number in the form of 1.1.38 so when a user finds a problem I can log the version they are using as well as tell them to upgrade if they have an old release.

A short explanation of how the versioning works would also be appreciated. When does the build and revision number get incremented?

What is the fastest, yet secure way to encrypt passwords in (PHP preferred), and for which ever method you choose is it portable?

In other words if I later migrate my website to a different server will my passwords continue to work?

The method I am using now as I was told is dependent on the exact versions of the libraries installed on the server.


What is the best way to localise a date format descriptor?

As anyone from a culture which does not use the mm/dd/yyyy format knows, it is annoying to have to enter dates in this format. The .NET framework provides some very good localisation support, so it's trivial to parse dates according to the users culture, but you often want to also display a helpful hint as to the format required (especially to distinguish between yy and yyyy which is interchangeable in most cultures).

What is the best way to do this in a way that make sense to most users (e.g. dd/M/yyy is confusing because of the change in case and the switching between one and two letters).


I can get Python to work with Postgresql but I cannot get it to work with MySQL. The main problem is that on the shared hosting account I have I do not have the ability to install things such as Django or PySQL, I generally fail when installing them on my computer so maybe it's good I can't install on the host.

I found bpgsql really good because it does not require an install, it's a single file that I can look at, read and then call the functions of. Does anybody know of something like this for MySQL?

This is ASP classic, not .Net.  We have to get a way to SFTP into a server to upload and download a couple of files, kicked off by a user.

What have other people used to do SFTP in ASP classic?  Not necessarily opposed to purchasing a control.


I'm trying to maintain a Setup Project in Visual Studio 2003 (yes, it's a legacy application). The problem we have at the moment is that we need to write registry entries to HKCU for every user on the computer. They need to be in the HKCU rather than HKLM because they are the default user settings, and they do change per user. My feeling is that


This isn't possible
This isn't something the installer should be doing, but something the application should be doing (after all what happens when a user profile is created after the install?).


With that in mind, I still want to change as little as possible in the application, so my question is, is it possible to add registry entries for every user in a Visual Studio 2003 setup project? 

And, at the moment the project lists five registry root keys (HKEY_CLASSES_ROOT, HKEY_CURRENT_USER, HKEY_LOCAL_MACHINE, HKEY_USERS, and User/Machine Hive). I don't really know anything about the Users root key, and haven't seen User/Machine Hive. Can anyone enlighten me on what they are? Perhaps they could solve my problem above.


SQL:

SELECT
   u.id,
   u.name,
   isnull(MAX(h.dateCol), '1900-01-01') dateColWithDefault
FROM universe u
LEFT JOIN history h 
   ON u.id=h.id 
   AND h.dateCol&lt;GETDATE()-1
GROUP BY u.Id, u.name



Is there an easy way to produce MSDN-style documentation from the Visual Studio XML output?
I'm not patient enough to set up a good xslt for it because I know I'm not the first person to cross this bridge.  

Also, I tried setting up sandcastle recently, but it really made my eyes cross.  Either I was missing something important in the process or it is just way too involved.

I know somebody out there has a really nice dead-simple solution.

I'm reiterating here because I think my formatting made that paragraph non-inviting to read:


  I gave sandcastle a try but had a really hard time getting it set up.
  What I really have in mind is something much simpler.


That is, unless I just don't understand the sandcastle process.  It seemed like an awful lot of extra baggage to me just to produce something nice for the testers to work with.


What's the simplest way to connect and query a database for a set of records in C#?


Attempting to insert an escape character into a table results in a warning. 

For example:

create table EscapeTest (text varchar(50));

insert into EscapeTest (text) values ('This is the first part \n And this is the second');


Produces the warning:

WARNING:  nonstandard use of escape in a string literal


(Using PSQL 8.2)

Anyone know how to get around this?


I'm maintaining a .NET 1.1 application, and one of the things I've been tasked with is making sure the user doesn't see any unfriendly error notifications.

I've added handlers to Application.ThreadException and AppDomain.CurrentDomain.UnhandledException, which do get called. My problem is that the standard CLR error dialog is still displayed (before the exception handler is called).

Jeff talks about this problem on his blog here and here. But there's no solution. So what is the standard way in .NET 1.1 to handle uncaught exceptions and display a friendly dialog box?

Edit: Jeff's response was marked as the correct answer, because the link he provided has the most complete information on how to do what's required.


I'm looking for a way to delete a file which is locked by another process using C#. I suspect the method must be able to find which process is locking the file (perhaps by tracking the handles, although I'm not sure how to do this in C#) then close that process before being able to complete the file delete using File.Delete().


What is the correct way to get the process size on Solaris, HP-UX and AIX? Should we use top or ps -o vsz or something else?


I was wondering if there is any good and clean oo implementation of bayesian filtering for spam and text classification? For learning purposes.

Exceptions in C++ don't need to be caught (no compile time errors) by the calling function. So it's up to developer's judgment whether to catch it using try/catch (unlike in Java). 

Is there a way one can ensure that the exceptions thrown are always caught using try/catch by the calling function?


Given that indexing is so important as your data set increases in size, can someone explain how does indexing works at a database agnostic level?

For information on queries to index a field, check out How do I index a database column


Hopefully, I can get answers for each database server.

For an outline of how indexing works check out: http://stackoverflow.com/questions/1108/how-does-database-indexing-work

I need to be able to manipulate a large (10^7 nodes) graph in python. The data corresponding to each node/edge is minimal, say, a small number of strings. What is the most efficient, in terms of memory and speed, way of doing this? 

A dict of dicts is more flexible and simpler to implement, but I intuitively expect a list of lists to be faster. The list option would also require that I keep the data separate from the structure, while dicts would allow for something of the sort:

graph[I][J]["Property"]="value"


What would you suggest?



Yes, I should have been a bit clearer on what I mean by efficiency. In this particular case I mean it in terms of random access retrieval.

Loading the data in to memory isn't a huge problem. That's done once and for all. The time consuming part is visiting the nodes so I can extract the information and measure the metrics I'm interested in.

I hadn't considered making each node a class (properties are the same for all nodes) but it seems like that would add an extra layer of overhead? I was hoping someone would have some direct experience with a similar case that they could share. After all, graphs are one of the most common abstractions in CS.


I have just recently started to study Ruby, and in lieu of Jeff's advice over the weekend...


Stop theorizing.
Write lots of software.
Learn from your mistakes. 


...I was interested in honing my skills while helping out the Open Source Community the process so I thought I'd ask if anyone have any suggestions for cool/interesting Open Source Projects written in Ruby that you know of or are involved in.

I am looking to allow users to control of subdomain of an app I am toying with, much like Basecamp where it is customusername.seework.com.

What is required on the DNS end to allow these to be created dynamically and be available instantly. 

And how do you recommend dealing with this in the logic of the site? Htaccess rule to  lookup the subdomain in the DB?


I have a client-server app where the client is on a Windows Mobile 6 device, written in C++ and the server is on full Windows and written in C#. 

Originally, I only needed it to send messages from the client to the server, with the server only ever sending back an acknowledgment that it received the message.  Now, I would like to update it so that the server can actually send a message to the client to request data.  As I currently have it set up so the client is only in receive mode after it sends data to the server, this doesn't allow for the server to send a request at any time.  I would have to wait for client data.  My first thought would be to create another thread on the client with a separate open socket, listening for server requests...just like the server already has in respect the the client.  Is there a way, within the same thread and using the same socket, to all the server to send requests at any time?

Can you use something to the affect of WaitForMultipleObjects() and pass it a receive buffer and an event that tells it there is data to be sent?


I have a Queue&lt;T&gt; object that I have initialised to a capacity of 2, but obviously that is just the capacity and it keeps expanding as I add items.  Is there already an object that automatically dequeues an item when the limit is reached, or is the best solution to create my own inherited class?

I am using MSBuild to build my stuff. I want to use CruiseControl.net as by Build Server.

Now, CCNET refers nAnt a lot, but it looks as if ccnet can do most of the stuff nant could do through the project configuration and msbuild. Also, nAnt seems a bit unsupported, with a Beta release that is almost a year old now.

In short: I am actually quite happy with MSBuild (especially since it's the "official" compiler front end) and a bit uncomfortable with nAnt, but I do not want to judge prematurely.

What would be reasons to use nAnt over MSBuild? Especially with ccnet, which seems to overlap a bit with nant in terms of features (and adding the automated build related stuff)

I'd like to create a progress bar to indicate the status of an a batch job in Ruby. 

I've read some tutorials / libraries on using (n)curses, none of which were particularly helpful in explaining how to create an "animated" progress bar in the terminal or using curses with Ruby. 

I'm already aware of using a separate thread to monitor the progress of a given job, I'm just not sure how to proceed with drawing a progress bar. 



Update  

ProgressBar class was incredibly straight-forward, perfectly solved my problem.


I need the name of the current logged in user in my Air/Flex application. The application will only be deployed on Windows machines. I think I could attain this by regexing the User directory, but am open to other ways.

This is an open-ended question.  What approaches should I consider?

I'm setting up a dedicated SQL Server 2005 box on Windows Server 2008 this week, and would like to pare it down to be as barebones as possible while still being fully functional.

To that end, the "Server Core" option sounds appealing, but I'm not clear about whether or not I can run SQL Server on that SKU.  Several services are addressed on the Microsoft website, but I don't see any indication about SQL Server.

Does anyone know definitively?


I have a custom validation function in JavaScript in a user control on a .Net 2.0 web site which checks to see that the fee paid is not in excess of the fee amount due. 

I've placed the validator code in the ascx file, and I have also tried using Page.ClientScript.RegisterClientScriptBlock() and in both cases the validation fires, but cannot find the JavaScript function.

The output in Firefox's error console is "feeAmountCheck is not defined". Here is the function (this was taken directly from firefox->view source)

&lt;script type="text/javascript"&gt;
    function feeAmountCheck(source, arguments)
    {
        var amountDue = document.getElementById('ctl00_footerContentHolder_Fees1_FeeDue');
        var amountPaid = document.getElementById('ctl00_footerContentHolder_Fees1_FeePaid');

        if (amountDue.value &gt; 0 &amp;&amp; amountDue &gt;= amountPaid)
        {
            arguments.IsValid = true;
        }
        else
        {
            arguments.IsValid = false;
        }

        return arguments;
    }
&lt;/script&gt;


Any ideas as to why the function isn't being found? How can I remedy this without having to add the function to my master page or consuming page?


Is it possible to configure xampp to serve up a file outside of the htdocs directory?

For instance, say I have a file located as follows:

C:\projects\transitCalculator\trunk\TransitCalculator.php

and my xampp files are normally served out from:

C:\xampp\htdocs\

(because that's the default configuration) Is there some way to make Apache recognize and serve up my TransitCalculator.php file without moving it under htdocs? Preferably I'd like Apache to serve up/have access to the entire contents of the projects directory, and I don't want to move the projects directory under htdocs.

edit: edited to add Apache to the question title to make Q/A more "searchable"


I often encounter the following scenario where I need to offer many different types of permissions. I primarily use ASP.NET / VB.NET with SQL Server 2000.

Scenario

I want to offer a dynamic permission system that can work on different parameters. Let's say that I want to give either a department or just a specific person access to an application. And pretend that we have a number of applications that keeps growing.

In the past, I have chosen one of the following two ways that I know to do this.

1) Use a single permission table with special columns that are used for determining a how to apply the parameters. The special columns in this example are TypeID and TypeAuxID. The SQL would look something like this.

SELECT COUNT(PermissionID)
FROM application_permissions
WHERE
(TypeID = 1 AND TypeAuxID = @UserID) OR
(TypeID = 2 AND TypeAuxID = @DepartmentID)
AND ApplicationID = 1


2) Use a mapping table for each type of permission, then joining them all together.

SELECT COUNT(perm.PermissionID)
FROM application_permissions perm
LEFT JOIN application_UserPermissions emp
ON perm.ApplicationID = emp.ApplicationID
LEFT JOIN application_DepartmentPermissions dept
ON perm.ApplicationID = dept.ApplicationID
WHERE q.SectionID=@SectionID
  AND (emp.UserID=@UserID OR dept.DeptID=@DeptID OR
 (emp.UserID IS NULL AND dept.DeptID IS NULL)) AND ApplicationID = 1
ORDER BY q.QID ASC


My Thoughts

I hope that the examples make sense. I cobbled them together.

The first example requires less work, but neither of them feel like the best answer. Is there a better way to handle this?


I'm trying to do this (which produces an unexpected T_VARIABLE error):

public function createShipment($startZip, $endZip, $weight = $this-&gt;getDefaultWeight()){}


I don't want to put a magic number in there for weight, since the object I am using has a "defaultWeight" parameter that all new shipments get if you don't specify a weight. I can't put the defaultWeight in the shipment itself, because it changes from shipment group to shipment group. Is there a better way to do it than the following?

public function createShipment($startZip, $endZip, weight = 0){
    if($weight &lt;= 0){
        $weight = $this-&gt;getDefaultWeight();
    }
}



How do you express an integer as a binary number with Python literals?

I was easily able to find the answer for hex:

    &gt;&gt;&gt; 0x12AF
    4783
    &gt;&gt;&gt; 0x100
    256


and octal:

    &gt;&gt;&gt; 01267
    695
    &gt;&gt;&gt; 0100
    64


How do you use literals to express binary in Python?



Summary of Answers


Python 2.5 and earlier: can express binary using int('01010101111',2) but not with a literal.
Python 2.5 and earlier: there is no way to express binary literals.
Python 2.6 beta: You can do like so: 0b1100111 or 0B1100111.
Python 2.6 beta: will also allow 0o27 or 0O27 (second character is the letter O) to represent an octal.
Python 3.0 beta: Same as 2.6, but will no longer allow the older 027 syntax for octals.



How do I set the icon that appears on the iPhone for the web sites I create?

My office has a central Source Safe 2005 install that we use for source control. I can't change what the office uses on the server. 

I develop on a laptop and would like to have a different local source control repository that can sync with the central server (when available) regardless of the what that central provider is. The reason for the request is so I can maintain a local stable branch/build for client presentations while continuing to develop without having to jump through flaming hoops. Also, as a consultant, my clients may request that I use their source control provider and flexibility here would make life easier.

Can any of the existing distributed source control clients handle that?

I'm looking for some way to effectively hide inherited members. I have a library of classes which inherit from common base classes.  Some of the more recent descendant classes inherit dependency properties which have become vestigial and can be a little confusing when using IntelliSense or using the classes in a visual designer.

These classes are all controls that are written to be compiled for either WPF or Silverlight 2.0.  I know about ICustomTypeDescriptor and ICustomPropertyProvider, but I'm pretty certain those can't be used in Silverlight.  

It's not as much a functional issue as a usability issue.  What should I do?

Update

Some of the properties that I would really like to hide come from ancestors that are not my own and because of a specific tool I'm designing for, I can't do member hiding with the new operator.  (I know, it's ridiculous)


I've never been completely happy with the way exception handling works, there's a lot exceptions and try/catch brings to the table (stack unwinding, etc.), but it seems to break a lot of the OO model in the process.

Anyway, here's the problem:

Let's say you have some class which wraps or includes networked file IO operations (e.g. reading and writing to some file at some particular UNC path somewhere). For various reasons you don't want those IO operations to fail, so if you detect that they fail you retry them and you keep retrying them until they succeed or you reach a timeout. I already have a convenient RetryTimer class which I can instantiate and use to sleep the current thread between retries and determine when the timeout period has elapsed, etc.

The problem is that you have a bunch of IO operations in several methods of this class, and you need to wrap each of them in try-catch / retry logic.

Here's an example code snippet:

RetryTimer fileIORetryTimer = new RetryTimer(TimeSpan.FromHours(10));bool success = false;while (!success){    try    {        // do some file IO which may succeed or fail        success = true;    }    catch (IOException e)    {        if (fileIORetryTimer.HasExceededRetryTimeout)        {            throw e;        }        fileIORetryTimer.SleepUntilNextRetry();    }}

So, how do you avoid duplicating most of this code for every file IO operation throughout the class? My solution was to use anonymous delegate blocks and a single method in the class which executed the delegate block passed to it. This allowed me to do things like this in other methods:

this.RetryFileIO( delegate()    {        // some code block    } );

I like this somewhat, but it leaves a lot to be desired. I'd like to hear how other people would solve this sort of problem.

As a LAMP developer considering moving to a .Net IIS platform, one of my concerns is the loss of productivity due to lack of shell... Has anyone else had this experience?  Is there possibly a Linux shell equivalent for Windows?


I always create a new empty database, after that backup and restore of the existing database into it, but is this really the best way? As it seems very error prone and over complicated for me.


When spliting a solution in to logical layers, when is it best to use a separate project over just grouping by a folder?

I'm writing an app to help facilitate some research, and part of this involves doing some statistical calculations. Right now, the researchers are using a program called SPSS. Part of the output that they care about looks like this:



They're really only concerned about the F and Sig. values. My problem is that I have no background in statistics, and I can't figure out what the tests are called, or how to calculate them.

I thought the F value might be the result of the F-test, but after following the steps given on Wikipedia, I got a result that was different from what SPSS gives.


I am looking for guidance regarding the best practice around the use of the Profile feature in ASP.NET.

How do you decide what should be kept in the built-in user Profile, or if you should create your own DB Table and add a column for the desired fields? For example, a user has a ZIP code, should I save the ZIP code in my own table, or should I add it to the web.config xml profile and access it via the user profile ASP.NET mechanize? 

The pros/cons I can think of are that since I don't know the profile very well (it is a bit of a Matrix right now), I probably can do whatever I want if I go the table route (e.g., SQL to get all the users in the same ZIP code as the current user); I don't know if I can do the same if I use the ASP.NET profile.


I was just looking through some information about Google's protocol buffers data interchange format.  Has anyone played around with the code or even created a project around it?

I'm currently using XML in a Python project for structured content created by hand in a text editor, and I was wondering what the general opinion was on Protocol Buffers as a user-facing input format.  The speed and brevity benefits definitely seem to be there, but there are so many factors when it comes to actually generating and processing the data.

I am trying to grab the capital letters of a couple of words and wrap them in span tags. I am using preg_replace for extract and wrapping purposes, but it's not outputting anything.

preg_replace("/[A-Z]/", "&lt;span class=\"initial\"&gt;$1&lt;/span&gt;", $str)



I am trying to set a flag to show or hide a page element, but it always displays even when the expression is false. 

$canMerge = ($condition1 &amp;&amp; $condition2) ? 'true' : 'false';...&lt;?php if ($canMerge) { ?&gt;Stuff&lt;?php } ?&gt;

What's up?

Is it possible to create "federated" Subversion servers?
As in one server at location A and another at location B that sync up their local versions of the repository automatically.  That way when someone at either location interacts with the repository they are accessing their respective local server and therefore has faster response times.

I've got a menu in Python. That part was easy. I'm using raw_input() to get the selection from the user. 

The problem is that raw_input (and input) require the user to press Enter after they make a selection. Is there any way to make the program act immediately upon a keystroke? Here's what I've got so far:

import sys
print """Menu
1) Say Foo
2) Say Bar"""
answer = raw_input("Make a selection&gt; ")

if "1" in answer: print "foo"
elif "2" in answer: print "bar"


It would be great to have something like

print menu
while lastKey = "":
    lastKey = check_for_recent_keystrokes()
if "1" in lastKey: #do stuff...



OK. This is a bit of a vanity app, but I had a situation today at work where I was in a training class and the machine was set to lock every 10 minutes.  Well, if the trainers got excited about talking - as opposed to changing slides - the machine would lock up.

I'd like to write a teeny app that has nothing but a taskbar icon that does nothing but move the mouse by 1 pixel every 4 minutes.  

I can do that in 3 ways with Delphi (my strong language) but I'm moving to C# for work and I'd like to know the path of least resistance there.

I am currently working on a project and my goal is to locate text in an image. OCR'ing the text is not my intention as of yet. I want to basically obtain the bounds of text within an image. I am using the AForge.Net imaging component for manipulation. Any assistance in some sense or another?

Update 2/5/09:
I've since went along another route in my project. However I did attempt to obtain text using MODI (Microsoft Office Document Imaging). It allows you to OCR an image and pull text from it with some ease.


On the project that I am working on I have a couple of databases. Each table and each column in the database has a description set (as an extended property in SQL 2005). As a part of the documentation going to the client we need to produce a data dictionary showing all of the tables and columns along with a collection of meta data (data-type, optionality, constraints).
Is anyone using a tool to automatically create this kind of document? If so, which tools do you use? I have used Data Dictionary Creator which is awesome but it doesn't seem to do data types or optionality (unless you want to add in custom fields and fill them in yourself).

How do I delimit a Javascript databound string parameter in an anchor OnClick event?


I have an anchor tag in an ASP.NET Repeater control.  
The OnClick event of the anchor contains a call to a Javascript function.  
The Javascript function takes a string for its input parameter.  
The string parameter is populated with a databound value from the Repeater.


I need the "double quotes" for the Container.DataItem.
I need the 'single quotes' for the OnClick.

And I still need one more delimiter (triple quotes?) for the input string parameter of the Javascript function call.

Since I can't use 'single quotes' again, how do I ensure the Javascript function knows the input parameter is a string and not an integer?

Without the extra quotes around the input string parameter, the Javascript function thinks I'm passing in an integer.

Cheers in advance for any knowledge you can drop.

The anchor:

&lt;a id="aShowHide" onclick='ToggleDisplay(&lt;%# DataBinder.Eval(Container.DataItem, "JobCode") %&gt;);' &gt;Show/Hide&lt;/a&gt;    


and here is the Javascript:

&lt;script language="JavaScript" type="text/javascript"&gt;
/* Shows/Hides the Jobs Div */
  function ToggleDisplay(jobCode)
  {
  /* Each div has it's ID set dynamically ('d' plus the JobCode) */
    var elem = document.getElementById('d' + jobCode);

    if (elem) 
    {
      if (elem.style.display != 'block') 
      {
        elem.style.display = 'block';
        elem.style.visibility = 'visible';
      } 
      else
      {
        elem.style.display = 'none';
        elem.style.visibility = 'hidden';
      }
    }
  }
&lt;/script&gt;



I have a bunch of latitude/longitude pairs that map to known x/y coordinates on a (geographically distorted) map.

Then I have one more latitude/longitude pair. I want to plot it on the map as best is possible. How do I go about doing this?

At first I decided to create a system of linear equations for the three nearest lat/long points and compute a transformation from these, but this doesn't work well at all. Since that's a linear system, I can't use more nearby points either.

You can't assume North is up: all you have is the existing lat/long->x/y mappings.

EDIT: it's not a Mercator projection, or anything like that. It's arbitrarily distorted for readability (think subway map). I want to use only the nearest 5 to 10 mappings so that distortion on other parts of the map doesn't affect the mapping I'm trying to compute.

Further, the entire map is in a very small geographical area so there's no need to worry about the globe--flat-earth assumptions are good enough.

Using ASP.NET MVC there are situations (such as form submission) that may require a RedirectToAction.  

One such situation is when you encounter validation errors after a form submission and need to redirect back to the form, but would like the URL to reflect the URL of the form, not the action page it submits to.

As I require the form to contain the originally POSTed data, for user convenience, as well as validation purposes, how can I pass the data through the RedirectToAction()?  If I use the viewData parameter, my POST parameters will be changed to GET parameters.


In order to fully use LinqToSql in an ASP.net 3.5 application, it is necessary to create DataContext classes (which is usually done using the designer in VS 2008). From the UI perspective, the DataContext is a design of the sections of your database that you would like to expose to through LinqToSql and is integral in setting up the ORM features of LinqToSql.

My question is: I am setting up a project that uses a large database where all tables are interconnected in some way through Foreign Keys. My first inclination is to make one huge DataContext class that models the entire database. That way I could in theory (though I don't know if this would be needed in practice) use the Foreign Key connections that are generated through LinqToSql to easily go between related objects in my code, insert related objects, etc.

However, after giving it some thought, I am now thinking that it may make more sense to create multiple DataContext classes, each one relating to a specific namespace or logical interrelated section within my database. My main concern is that instantiating and disposing one huge DataContext class all the time for individual operations that relate to specific areas of the Database would be impose an unnecessary imposition on application resources. Additionally, it is easier to create and manage smaller DataContext files than one big one. The thing that I would lose is that there would be some distant sections of the database that would not be navigable through LinqToSql (even though a chain of relationships connects them in the actual database). Additionally, there would be some table classes that would exist in more than one DataContext.

Any thoughts or experience on whether multiple DataContexts (corresponding to DB namespaces) are appropriate in place of (or in addition to) one very large DataContext class (corresponding to the whole DB)?

I was just wondering if there is an elegant way to set the maximum CPU load for a particular thread doing intensive calculations. 
Right now I have located the most time consuming loop in the thread (it does only compression) and use GetTickCount() and Sleep() with hardcoded values. It makes sure that the loop continues for a certain period of time and than sleeps for a certain minimal time. It more or less does the job i.e. guarantees that the thread will not use more than 50% of CPU. However behavior is dependent on the number of CPU cores (huge disadvantage) and simply ugly (smaller disadvantage :)). Any ideas?


When you data bind in C#, the thread that changes the data causes the control to change too. But if this thread is not the one on which the control was created, you'll get the above exception.

I surfed the net and found no good answer.

Anyone?


I have values stored as strings in a DataTable where each value could really represent an int, double, or string (they were all converted to strings during an import process from an external data source). I need to test and see what type each value really is.

What is more efficient for the application (or is there no practical difference)?


Try to convert to int (and then double). If conversion works, the return true. If an exception is thrown, return false.
Regular expressions designed to match the pattern of an int or double
Some other method?



Looking for books or other references that discuss actually "how" to write a code coverage tool in Java; some of the various techniques or tricks - source vs. byte code instrumentation. This is for a scripting language that generates java byte code under the hood.


I want to get the MD5 Hash of a string value in SQL Server 2005. I do this with the following command:

SELECT HashBytes('MD5', 'HelloWorld')


However, this returns a VarBinary instead of a VarChar value. If I attempt to convert 0x68E109F0F40CA72A15E05CC22786F8E6 into a VarChar I get há ðô§*à\Â'†øæ instead of 68E109F0F40CA72A15E05CC22786F8E6.

Is there any SQL-based solution?

Yes


Instead of writing my ASP.NET C# applications in Visual Studio, I used my favorite text editor UltraEdit32.

Is there anyway I can implement MVC without the use of VS?


The web applications I develop often require co-dependant configuration settings and there are also settings that have to change as we move between each of our environments.  

All our settings are currently simple key value pairs but it would be useful to create custom config sections so that it is obvious when two values need to change together or when the settings need to change for an environment.

What's the best way to create custom config sections and are there any special considerations to make when retrieving the values?

Is there a way to create a JButton with your own button graphic and not just with an image inside the button? 

If not, is there another way to create a custom button in java?


I'm currently working on an application with a frontend written in Adobe Flex 3. I'm aware of FlexUnit but what I'd really like is a unit test runner for Ant/NAnt and a runner that integrates with the Flex Builder IDE (AKA Eclipse). Does one exist? 

Also, are there any other resources on how to do Flex development "the right way" besides the Cairngorm microarchitecture example?


I currently use a DataTable to get results from a database which I can use in my code.

However, many example on the web show using a DataSet instead and accessing the table(s) through the collections method.

Is there any advantage, performance wise or otherwise, of using DataSets or DataTables as a storage method for SQL results?


Is there a way of mapping data collected on a stream or array to a data structure or vice-versa?
In C++ this would simply be a matter of casting a pointer to the stream as a data type I want to use (or vice-versa for the reverse)
eg: in C++

Mystruct * pMyStrct = (Mystruct*)&amp;SomeDataStream;
pMyStrct-&gt;Item1 = 25;

int iReadData = pMyStrct-&gt;Item2;


obviously the C++ way is pretty unsafe unless you are sure of the quality of the stream data when reading incoming data, but for outgoing data is super quick and easy.


How do I rewrite URL's in ASP.NET?

I would like users to be able to goto http://www.website.com/users/smith instead of http://www.website.com/?user=smith


I want to be able to do:For Each thing In things
End For

CLASSIC ASP - NOT .NET!

I see in the Stack Overflow footer that the SVN Revision number is displayed. Is this automated and if so, how does one implement it in ASP.NET?

(Solutions in other languages are acceptable)

I have a problem with maintaining state in an ASP.NET AJAX page. Short version: I need some way to update the page ViewState after an async callback has been made, to reflect any state changes the server made during the async call. 

This seems to be a common problem, but I will describe my scenario to help explain:

I have a grid-like control which has some JavaScript enhancements - namely, the ability to drag and drop columns and rows. When a column or row is dropped into a new position, an AJAX method is invoked to notify the control server-side and fire a corresponding server-side event ("OnColumnMoved" or "OnRowMoved").

ASP.NET AJAX calls, by default, send the entire page as the request. That way the page goes through a complete lifecycle, viewstate is persisted and the state of the control is restored before the RaiseCallbackEvent method is invoked.

However, since the AJAX call does not update the page, the ViewState reflects the original state of the control, even after the column or row has been moved. So the second time a client-side action occurs, the AJAX request goes to the server and the page &amp; control are built back up again to reflect the first state of the control, not the state after the first column or row was moved.

This problem extends to many implications. For example if we have a client-side/AJAX action to add a new item to the grid, and then a row is dragged, the grid is built server-side with one less item than on the client-side.

And finally &amp; most seriously for my specific example, the actual data source object we are acting upon is stored in the page ViewState. That was a design decision to allow keeping a stateful copy of the manipulated data which can either be committed to DB after many manipulations or discarded if the user backs out. That is very difficult to change.

So, again, I need a way for the page ViewState to be updated on callback after the AJAX method is fired.


In the code below

For i = LBound(arr) To UBound(arr)


What is the point in asking using LBound? Surely that is always 0.


In Windows, in any windows form or web browser, you can use the tab button to switch focus through all of the form fields. 

It will stop on textboxes, radiobuttons, checkboxes, dropdown menus, etc. 

However, in Mac OSX, tab skips dropdown menus. Is there anyway to change this behavior, or access the above items mentioned, without using a mouse?


I'm trying to read binary data using C#. I have all information about the layout of the data in the files I want to read. I'm able to read the data "chunk by chunk", i.e. getting the first 40 bytes of data converting it to a string, get the next 40 bytes, ...
Since there are at least three slighlty different version of the data, I would like to read the data directly into a struct. It just feels so much more right than by reading it "line by line".
I have tried the following approach but to no avail:StructType aStruct;
int count = Marshal.SizeOf(typeof(StructType));
byte[] readBuffer = new byte[count];
BinaryReader reader = new BinaryReader(stream);
readBuffer = reader.ReadBytes(count);
GCHandle handle = GCHandle.Alloc(readBuffer, GCHandleType.Pinned);
aStruct = (StructType) Marshal.PtrToStructure(handle.AddrOfPinnedObject(), typeof(StructType));
handle.Free();

The stream is an opened FileStream from which I have began to read from. I get an AccessViolationException when using Marshal.PtrToStructure.
The stream contains more information than I'm trying to read since I'm not interested in data at the end of the file.
The struct is defined like:[StructLayout(LayoutKind.Explicit)]
struct StructType
{
    [FieldOffset(0)]
    public string FileDate;
    [FieldOffset(8)]
    public string FileTime;
    [FieldOffset(16)]
    public int Id1;
    [FieldOffset(20)]
    public string Id2;
}

The examples code is changed from original to make this question shorter.
How would I read binary data from a file into a struct?

Regarding Agile development.

What are the best practices for testing security per release?  

If it is a monthly release, are there shops doing pen-tests every month?


What is the best way to record statistics on the number of visitors visiting my site that have set their browser to block ads?


I've been able to find details on several self-balancing BSTs through several sources, but I haven't found any good descriptions detailing which one is best to use in different situations (or if it really doesn't matter).  

I want a BST that is optimal for storing in excess of ten million nodes. The order of insertion of the nodes is basically random, and I will never need to delete nodes, so insertion time is the only thing that would need to be optimized.  

I intend to use it to store previously visited game states in a puzzle game, so that I can quickly check if a previous configuration has already been encountered.


I have 2 SQLite databases, one downloaded from a server (server.db), and one used as storage on the client (client.db). I need to perform various sync queries on the client database, using data from the server database.

For example, I want to delete all rows in the client.db tRole table, and repopulate with all rows in the server.db tRole table.

Another example, I want to delete all rows in the client.db tFile table where the fileID is not in the server.db tFile table.

In SQL Server you can just prefix the table with the name of the database. Is there anyway to do this in SQLite using Adobe Air?

How can I find out which node in a tree list the context menu has been activated? For instance right-clicking a node and selecting an option from the menu. 

I can't use the TreeViews' SelectedNode property because the node is only been right-clicked and not selected.


What are good libraries for C with datastructures like vectors, deques, stacks, hashmaps, treemaps, sets, etc.? Plain C, please, and platform-independent.

A quick glance at the present-day internet would seem to indicate that Adobe Flash is the obvious choice for embedding video in a web page.  Is this accurate, or are they other effective choices?  Does the choice of ASP.NET as a platform influence this decision?

Any good recommendations for a platform agnostic (i.e. Javascript) grid control/plugin that will accept pasted Excel data and can emit Excel-compliant clipboard data during a Copy?

I believe Excel data is formatted as CSV during "normal" clipboard operations.



dhtmlxGrid looks promising, but the online demo's don't actually copy contents to my clipboard!


I've used a WordPress blog and a Screwturn Wiki (at two separate jobs) to store private, company-specific KB info, but I'm looking for something that was created to be a knowledge base.  Specifically, I'd like to see:


Free/low cost
Simple method for users to subscribe to KB (or just sections) to get updates
Ability to do page versioning/audit changes
Limit access to certain pages for certain users
Very simple method of posting/editing articles
Very simple method of adding images to articles
Excellent (fast, accurate) searching abilities
Ability to rate and comment on articles


I liked using the Wordpress blog because it allowed me to use Live Writer to add/edit articles and images, but it didn't have page versioning (that I could see).

I like using Screwturn wiki because of it's ability to track article versions, and I like it's clean look, but some non-technical people balk at the input and editing.

I know I need to have (although I don't know why) an Order By clause on the end of a SQL query that uses any aggregate functions like count, sum, avg, etc:

select count(userID), userName from users group by userName

When else would GROUP BY be useful, and what are the performance ramifications?

Let's aggregate a list of free quality web site design templates. There are a million of these sites out there, but most are repetitive and boring. 

I'll start with freeCSStemplates.org

I also think other sites should follow some sort of standards, for example here are freeCSStemplates standards


Released for FREE under the Creative Commons Attribution 2.5 license 
Very lightweight in terms of images 
Tables-free (ie. they use no tables for layout purposes) 
W3C standards compliant and valid (XHTML Strict) 
Provided with public domain photos, generously provided by PDPhoto.org and Wikimedia Commons 



When working on ASP.NET 1.1 projects I always used the Global.asax to catch all errors. I'm looking for a similar way to catch all exceptions in a Windows Forms user control, which ends up being a hosted IE control. What is the proper way to go about doing something like this?


Here's what I use:

SELECT CAST(FLOOR(CAST(getdate() as FLOAT)) as DATETIME)


I'm thinking there may be a better and more elegant way.

Requirements:


It has to be as fast as possible (the less casting, the better).
The final result has to be a datetime type, not a string.



We have a simple utility class in-house for our database calls (a light wrapper around ADO.NET), but I am thinking of creating classes for each database/object.  Would it be smart thing to do so, or would it only benefit if we were using the full MVC framework for ASP.NET?

So we have this:

SQLWrapper.GetRecordset(connstr-alias, sql-statement, parameters);
SQLWrapper.GetDataset(connstr-alias, sql-statement, parameters);
SQLWrapper.Execute(connstr-alias, sql-statement, parameters);


Thinking of doing this:

Person p = Person.get(id);
p.fname = "jon";
p.lname = "smith";
p.Save();


or for a new record -

Person p = new Person();
p.fname = "Jon";
p.lname = "Smith";
p.Save();
p.Delete();


Would this be smart, or would it be overkill? I can see the benefit for reuse, changing database, and maintenance/readability.


The table doesn't have a last updated field and I need to know when existing data was updated. So adding a last updated field won't help (as far as I know).


I have a table with a structure like the following:

------------------------------
LocationID     | AccountNumber
------------------------------
long-guid-here | 12345
long-guid-here | 54321


To pass into another stored procedure, I need the XML to look like this:

&lt;root&gt; 
    &lt;clientID&gt;12345&lt;/clientID&gt;
    &lt;clientID&gt;54321&lt;/clientID&gt;
&lt;/root&gt;


The best I've been able to do so far was getting it like this:

&lt;root clientID="10705"/&gt;


I'm using this SQL statement:

SELECT
    1 as tag,
    null as parent,
    AccountNumber as 'root!1!clientID'
FROM
    Location.LocationMDAccount
WHERE
    locationid = 'long-guid-here'
FOR XML EXPLICIT


So far, I've looked at the documentation on the MSDN page, but I've not come out with the desired results.



@KG,

Yours gave me this output actually:

&lt;root&gt;
  &lt;Location.LocationMDAccount&gt;
    &lt;clientId&gt;10705&lt;/clientId&gt;
  &lt;/Location.LocationMDAccount&gt;
&lt;/root&gt;


I'm going to stick with the FOR XML EXPLICIT from Chris Leon for now.


I would like to make a nightly cron job that fetches my stackoverflow page and diffs it from the previous day's page, so I can see a change summary of my questions, answers, ranking, etc.

Unfortunately, I couldn't get the right set of cookies, etc, to make this work.  Any ideas?

Also, when the beta is finished, will my status page be accessible without logging in?


How do I page results in SQL Server 2005?

I tried it in SQL Server 2000, but there was no reliable way to do this. I'm now wondering if SQL Server 2005 has any built in method?

What I mean by paging is, for example, if I list users by their username, I want to be able to only return the first 10 records, then the next 10 records and so on.

Any help would be much appreciated.


What would be the best way to fill a C# struct from a byte[] array where the data was from a C/C++ struct?  The C struct would look something like this (my C is very rusty):

typedef OldStuff {
    CHAR Name[8];
    UInt32 User;
    CHAR Location[8];
    UInt32 TimeStamp;
    UInt32 Sequence;
    CHAR Tracking[16];
    CHAR Filler[12];
}


And would fill something like this:

[StructLayout(LayoutKind.Explicit, Size = 56, Pack = 1)]
public struct NewStuff
{
    [MarshalAs(UnmanagedType.ByValTStr, SizeConst = 8)]
    [FieldOffset(0)]
    public string Name;

    [MarshalAs(UnmanagedType.U4)]
    [FieldOffset(8)]
    public uint User;

    [MarshalAs(UnmanagedType.ByValTStr, SizeConst = 8)]
    [FieldOffset(12)]
    public string Location;

    [MarshalAs(UnmanagedType.U4)]
    [FieldOffset(20)]
    public uint TimeStamp;

    [MarshalAs(UnmanagedType.U4)]
    [FieldOffset(24)]
    public uint Sequence;

    [MarshalAs(UnmanagedType.ByValTStr, SizeConst = 16)]
    [FieldOffset(28)]
    public string Tracking;
}


What is best way to copy OldStuff to NewStuff, if OldStuff was passed as byte[] array?

I'm currently doing something like the following, but it feels kind of clunky.

GCHandle handle;
NewStuff MyStuff;

int BufferSize = Marshal.SizeOf(typeof(NewStuff));
byte[] buff = new byte[BufferSize];

Array.Copy(SomeByteArray, 0, buff, 0, BufferSize);

handle = GCHandle.Alloc(buff, GCHandleType.Pinned);

MyStuff = (NewStuff)Marshal.PtrToStructure(handle.AddrOfPinnedObject(), typeof(NewStuff));

handle.Free();


Is there better way to accomplish this?



Would using the BinaryReader class offer any performance gains over pinning the memory and using Marshal.PtrStructure?


Occasionally, I've come across a webpage that tries to pop open a new window (for user input, or something important), but the popup blocker prevents this from happening.

What methods can the calling window use to make sure the new window launched properly?

I am writing an application that needs to bring window of an external app to the foreground, and not necessarily steal focus (there is a setting the user can toggle to steal/not steal focus).
What is the best way to go about this using the win32 API? I have tried SetForeground() but it always steals focus and does not consistenly work.
What is the best way to go about this? Any thoughts?

What is the best unobtrusive CAPTCHA for web forms? One that does not involve a UI, rather a non-UI Turing test. I have seen a simple example of a non UI CAPTCHA like the Nobot  control from Microsoft. I am looking for a CAPTCHA that does not ask the user any question in any form. No riddles, no what's in this image.


In interpreted programming languages, such as PHP and JavaScript, what are the repercussions of going with an Object Oriented approach over a Procedural approach?

Specifically what I am looking for is a checklist of things to consider when creating a web application and choosing between Procedural and Object Oriented approaches, to optimize not only for speed, but maintainability as well. Cited research and test cases would be helpful as well if you know of any articles exploring this further.

Bottom line: how big (if any) is the performance hit really, when going with OO vs. Procedural in an interpreted language?

I have a simple page with my ScriptManager and my UpdatePanel, and my ContentTemplate has one ListBox and one Label.  I am just trying to catch its OnSelectionChanged so I can update the text in the Label.  No matter what settings I try to tweak, I always get a full-page postback.

Is this really not going to work, or am I just screwing this up?


I'm developing some cross platform software targeting Mono under Visual Studio and would like to be able to build the installers for Windows and Linux (Ubuntu specifically) with a single button click. I figure I could do it by calling cygwin from a post-build event, but I was hoping for at best a Visual Studio plugin or at worst a more Windows-native way of doing it. It seems like the package format is fairly simple and this must be a common need.

edit: Re-asked question under other account due to duplicate login issue.

I have a perl variable $results that gets returned from a service.  The value is supposed to be an array, and $results should be an array reference.  However, when the array has only one item in it, $results will be set to that value, and not a referenced array that contains that one item.

I want to do a foreach loop on the expected array.  Without checking ref($results) eq 'ARRAY', is there any way to have something equivalent to the following:

foreach my $result (@$results) {    # Process $result}

That particular code sample will work for the reference, but will complain for the simple scalar.

EDIT: I should clarify that there is no way for me to change what is returned from the service.  The problem is that the value will be a scalar when there is only one value and it will be an array reference when there is more than one value.

I'm compiling a NAnt project on linux with TeamCity Continuous Integration server. I have been able to generate a test report by running NAnt on mono thru a Command Line Runner but don't have the options of using the report like a NAnt Runner. I'm also using MBUnit for the testing framework.

How can I merge in the test report and display "Tests failed: 1 (1 new), passed: 3049" for the build?

Update: take a look at MBUnitTask its a NAnt task that uses sends messages that TeamCity expects from NUnit so it lets you use all of TeamCity's features for tests.

MBUnitTask

Update: Galio has better support so you just have to reference the Galio MBUnit 3.5 dlls instead of the MBUnit 3.5 dlls and switch to the galio runner to make it work.


I have seen these being used every which way, and have been accused of using them the wrong way (though in that case, I was using them that way to demonstrate a point).

So, what do you think are the best practices for employing Extension Methods?

Should development teams create a library of extension methods and deploy them across various projects?

Should there be a collection of common extension methods in the form of an open source project?

Update: have decided to create an organization wide extension methods library 


I have been trying to find a really fast way to parse yyyy-mm-dd [hh:mm:ss] into a Date object. Here are the 3 ways I have tried doing it and the times it takes each method to parse 50,000 date time strings.

Does anyone know any faster ways of doing this or tips to speed up the methods?

castMethod1 takes 3673 ms 
castMethod2 takes 3812 ms 
castMethod3 takes 3931 ms


Code:

private function castMethod1(dateString:String):Date {
    if ( dateString == null ) {
        return null;
    }

    var year:int = int(dateString.substr(0,4));
    var month:int = int(dateString.substr(5,2))-1;
    var day:int = int(dateString.substr(8,2));

    if ( year == 0 &amp;&amp; month == 0 &amp;&amp; day == 0 ) {
        return null;
    }

    if ( dateString.length == 10 ) {
        return new Date(year, month, day);
    }

    var hour:int = int(dateString.substr(11,2));
    var minute:int = int(dateString.substr(14,2));
    var second:int = int(dateString.substr(17,2));

    return new Date(year, month, day, hour, minute, second);
}


-

private function castMethod2(dateString:String):Date {
    if ( dateString == null ) {
        return null;
    }

    if ( dateString.indexOf("0000-00-00") != -1 ) {
        return null;
    }

    dateString = dateString.split("-").join("/");

    return new Date(Date.parse( dateString ));
}


-

private function castMethod3(dateString:String):Date {
    if ( dateString == null ) {
        return null;
    }

    var mainParts:Array = dateString.split(" ");
    var dateParts:Array = mainParts[0].split("-");

    if ( Number(dateParts[0])+Number(dateParts[1])+Number(dateParts[2]) == 0 ) {
        return null;
    }

    return new Date( Date.parse( dateParts.join("/")+(mainParts[1]?" "+mainParts[1]:" ") ) );
}




No, Date.parse will not handle dashes by default. And I need to return null for date time strings like "0000-00-00".


If I have managed to locate and verify the existence of a file using Server.MapPath and I now want to send the user directly to that file, what is the fastest way to convert that absolute path back into a relative web path?

Is there anyone working solo and using fogbugz out there? I'm interested in personal experience/overhead versus paper.

I am involved in several projects and get pretty hammered with lots of details to keep track of... Any experience welcome.

(Yes I know Mr. Joel is on the stackoverflow team... I still want good answers :)

CSS and Javascript files don't change very often, so I want them to be cached by the web browser. But I also want the web browser to see changes made to these files without requiring the user to clear their browser cache. Also want a solution that works well with a version control system such as Subversion.




  Some solutions I have seen involve adding a version number to the end of the file in the form of a query string.
  
  Could use the SVN revision number to automate this for you: ASP.NET Display SVN Revision Number


Can you specify how you include the Revision variable of another file? That is in the HTML file I can include the Revision number in the URL to the CSS or Javascript file.

In the Subversion book it says about Revision: "This keyword describes the last known revision in which this file changed in the repository".


  Firefox also allows pressing CTRL+R to reload everything on a particular page.


To clarify I am looking for solutions that don't require the user to do anything on their part.


I'm wondering how to make a release build that includes all necessary dll files into the .exe so the program can be run on a non-development machine without it having to install the microsoft redistributable on the target machine.

Without doing this you get the error message that the application configuration is not correct and to reinstall.  

I'm wondering if there's a way to do what I can do below with Python, in Ruby:

sum = reduce(lambda x, y: x + y, map(lambda x, y: x * y, weights, data))

I have two arrays of equal sizes with the weights and data but I can't seem to find a function similar to map in Ruby, reduce I have working.

I need to set the height of every textbox on my form, some of which are nested within other controls.  I thought I could do something like this:

private static IEnumerator&lt;TextBox&gt; FindTextBoxes(Control rootControl)
{
    foreach (Control control in rootControl.Controls)
    {
        if (control.Controls.Count &gt; 0)
        {
            // Recursively search for any TextBoxes within each child control
            foreach (TextBox textBox in FindTextBoxes(control))
            {
                yield return textBox;
            }
        }

        TextBox textBox2 = control as TextBox;
        if (textBox2 != null)
        {
            yield return textBox2;
        }
    }
}


Using it like this:

foreach(TextBox textBox in FindTextBoxes(this))
{
    textBox.Height = height;
}


But of course the compiler spits its dummy, because foreach expects an IEnumerable rather than an IEnumerator.

Is there a way to do this without having to create a separate class with a GetEnumerator() method?


I would like to be able to use the Tab key within the a text box to tab over four spaces. The way it is now, the Tab key jumps my cursor to the next input.

Is there some JavaScript that will capture the Tab key in the text box before it bubbles up to the UI?

I understand some browsers (i.e. FireFox) may not allow this. How about a custom key-combo like Shift+Tab, or Ctrl+Q?


I know that we can get the MAC address of a user via IE (ActiveX objects).

Is there a way to do the same thing in all browsers? (Especially since FF is gaining browser share everyday!)

I use Firebug and the Mozilla JS console heavily, but every now and then I run into an IE-only JavaScript bug, which are really hard to locate (ex: error on line 724, when the source html only has 200 lines).

I would love to have a lightweight JS tool (a la firebug) for Internet Explorer, something I can install in seconds on a client's PC if I run into an error and then uninstall. Some Microsoft tools take some serious download and configuration time.

Any ideas?


We recently discovered that the Google Maps API does not play nicely with SSL.  Fair enough, but what are some options for overcoming this that others have used effectively?


  Will the Maps API work over SSL (HTTPS)?
  
  At this time, the Maps API is not
  available over a secure (SSL)
  connection. If you are running the
  Maps API on a secure site, the browser
  may warn the user about non-secure
  objects on the screen.


We have considered the following options


Splitting the page so that credit card collection (the requirement for SSL) is not on the same page as the Google Map.
Switching to another map provider, such as Virtual Earth.  Rumor has it that they support SSL.
Playing tricks with IFRAMEs.  Sounds kludgy.
Proxying the calls to Google.  Sounds like a lot of overhead.


Are there other options, or does anyone have insight into the options that we have considered?

I have a very simple problem which requires a very quick and simple solution in SQL Server 2005.

I have a table with x Columns. I want to be able to select one row from the table and then transform the columns into rows.

TableA
Column1, Column2, Column3


SQL Statement to ruturn

ResultA
Value of Column1
Value of Column2
Value of Column3




@Kevin: I've had a google search on the topic but alot of the example where overly complex for my example, are you able to help further?

@Mario: The solution I am creating has 10 columns which stores the values 0 to 6 and I must work out how many columns have the value 3 or more. So I thought about creating a query to turn that into rows and then using the generated table in a subquery to say count the number of rows with Column >= 3


What is BODMAS and why is it useful in programming?

I'm using MSVE, and I have my own tiles I'm displaying in layers on top. Problem is, there's a ton of them, and they're on a network server. In certain directories, there are something on the order of 30,000+ files. Initially I called Directory.GetFiles, but once I started testing in a pseudo-real environment, it timed out.

What's the best way to programatically list, and iterate through, this many files?

Edit: My coworker suggested using the MS indexing service. Has anyone tried this approach, and (how) has it worked?

Generally when I use ClickOnce when I build a VB.NET program but it has a few downsides. I've never really used anything else, so I'm not sure
what my options are.

Downsides to ClickOnce:


Consists of multiple files - Seems easier to distribute one file than manageing a bunch of file and the downloader to download those files.
You have to build it again for CD installations (for when the end user dosn't have internet)
Program does not end up in Program Files - It ends up hidden away in some application catch folder, making it much harder to shortcut to.


Pros to ClickOnce:


It works. Magically. And it's built
into VisualStudio 2008 express.
Makes it easy to upgrade the
application.


Does Windows Installer do these things as well? I know it dosen't have any of the ClickOnce cons, but It would be nice to know if it also has the ClickOnce pros.

Update:
I ended up using Wix 2 (Wix 3 was available but at the time I did the project, no one had a competent tutorial). It was nice because it supported the three things I (eventually) needed. An optional start-up-with-windows shortcut, a start-up-when-the-installer-is-done option, and three paragraphs of text that my boss thinks will keep uses from clicking the wrong option.


I need to create a backup of a SQL Server 2005 Database that's only the structure...no records, just the schema. Is there any way to do this?
EDIT: I'm trying to create a backup file to use with old processes, so a script wouldn't work for my purposes, sorry

So far I just use the SAP Developers' network at sdn.sap.com but I would love something a bit more grassroots with bleeding-edge social features.

Blogs and forums and any opportunity to seek help and critique outside of my immediate team would be great...

I've got TotroiseSVN installed and have a majority of my repositories checking in and out from C:\subversion\ and a couple checking in and out from a network share (I forgot about this when I originally posted this question).

This means that I don't have a "subversion" server per-se.

How do I integrate TortoiseSVN and Fogbugz?

Edit: inserted italics

I'm trying to write some php to upload a file to a folder on my webserver. Here's what I have:

&lt;?php
    if ( !empty($_FILES['file']['tmp_name']) ) {
        move_uploaded_file($_FILES['file']['tmp_name'], './' . $_FILES['file']['name']);
        header('Location: http://www.mywebsite.com/dump/');
        exit;
    }
?&gt;

&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
    "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"&gt;
&lt;html&gt;
    &lt;head&gt;
        &lt;title&gt;Dump Upload&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;h1&gt;Upload a File&lt;/h1&gt;
        &lt;form action="upload.php" enctype="multipart/form-data" method="post"&gt;
            &lt;input type="hidden" name="MAX_FILE_SIZE" value="1000000000" /&gt;
            Select the File:&lt;br /&gt;&lt;input type="file" name="file" /&gt;&lt;br /&gt;
            &lt;input type="submit" value="Upload" /&gt;
        &lt;/form&gt;
    &lt;/body&gt;
&lt;/html&gt;


I'm getting these errors:


  Warning: move_uploaded_file(./test.txt) [function.move-uploaded-file]: failed to open stream: Permission denied in E:\inetpub\vhosts\mywebsite.com\httpdocs\dump\upload.php on line 3
  
  Warning: move_uploaded_file() [function.move-uploaded-file]: Unable to move 'C:\WINDOWS\Temp\phpA30E.tmp' to './test.txt' in E:\inetpub\vhosts\mywebsite.com\httpdocs\dump\upload.php on line 3
  
  Warning: Cannot modify header information - headers already sent by (output started at E:\inetpub\vhosts\mywebsite.com\httpdocs\dump\upload.php:3) in E:\inetpub\vhosts\mywebsite.com\httpdocs\dump\upload.php on line 4


PHP version 4.4.7
Running IIS on a Windows box. This particular file/folder has 777 permissions.

Any ideas?


Any one know decent way to reference a SQLite database using the above mentioned tools? I tried using ODBC (the SQLite driver) but while the connection is good, I get no data returned. Like I can't see any tables in Data Connection (VS 2008). Is there a better way?

Edit: corrected typos

I have a MySQL table with approximately 3000 rows per user. One of the columns is a datetime field, which is mutable, so the rows aren't in chronological order.

I'd like to visualize the time distribution in a chart, so I need a number of individual datapoints. 20 datapoints would be enough.

I could do this:

select timefield from entries where uid = ? order by timefield;


and look at every 150th row.

Or I could do 20 separate queries and use limit 1 and offset. 

But there must be a more efficient solution...


The discussion of Dual vs. Quadcore is as old as the Quadcores itself and the answer is usually "it depends on your scenario". So here the scenario is a Web Server (Windows 2003 (not sure if x32 or x64), 4 GB RAM, IIS, ASP.net 3.0).

My impression is that the CPU in a Webserver does not need to be THAT fast because requests are usually rather lightweight, so having more (slower) cores should be a better choice as we got many small requests.

But since I do not have much experience with IIS load balancing and since I don't want to spend a lot of money only to find out I've made the wrong choice, can someone who has a bit more experience comment on whether or not More Slower or Fewer Faster cores is better?


One simple method I've used in the past is basically just creating a second table whose structure mirrors the one I want to audit, and then create an update/delete trigger on the main table.  Before a record is updated/deleted, the current state is saved to the audit table via the trigger.

While effective, the data in the audit table is not the most useful or simple to report off of.  I'm wondering if anyone has a better method for auditing data changes?

There shouldn't be too many updates of these records, but it is highly sensitive information, so it is important to the customer that all changes are audited and easily reported on.


I'm trying out the following query:

SELECT A,B,C FROM table WHERE field LIKE 'query%'
UNION
SELECT A,B,C FROM table WHERE field LIKE '%query'
UNION
SELECT A,B,C FROM table WHERE field LIKE '%query%'
GROUP BY B ORDER BY B ASC LIMIT 5


That's three queries stuck together, kindasorta. However, the result set that comes back reflects results from query #3 BEFORE results from query #1 (undesired).

Is there any way to prioritize these so that results come as all for query #1, then all for query #2 then all for query #3? I don't want to do this in PHP just yet (not to mention having to control for results that showed up in the first query not to show in the second and so forth).

edit: many thanks to all! the solution turned out to be a mix of all answers. worked beautifully!.


I'm downloading a web page (tag soup HTML) with XMLHttpRequest and I want to take the output and turn it into a DOM object that I can then run XPATH queries on. How do I convert from a string into DOM object?

It appears that the general solution is to create a hidden iframe and throw the contents of the string into that. There has been talk of updating DOMParser to support text/html but as of Firefox 3.0.1 you still get an NS_ERROR_NOT_IMPLEMENTED if you try.

Is there any option besides using the hidden iframe trick? And if not, what is the best way to do the iframe trick so that your code works outside the context of any currently open tabs (so that closing tabs won't screw up code, etc)?

This is an example of why I'm looking for a solution other than the iframe hack, if I have to write all that code to have a robust solution, then I'd rather keep looking for something else.


Given the constraint of only using T-Sql in Sql Server 2005, is there a better way to remove the decimal point from a money datatype than a conversion to a varchar (here implicitly) and then a replace of the decimal point?

Here is what I have currently.

SELECT REPLACE(1.23, '.', ''), REPLACE(19.99, '.', '')

Which returns the desired 123 and 1999, but I was wondering if there was a better way.  Any thoughts?

Since the WMI class Win32_OperatingSystem only includes OSArchitecture in Windows Vista, I quickly wrote up a method using the registry to try and determine whether or not the current system is a 32 or 64bit system.

private Boolean is64BitOperatingSystem()
{
    RegistryKey localEnvironment = Registry.LocalMachine.OpenSubKey("SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Environment");
    String processorArchitecture = (String) localEnvironment.GetValue("PROCESSOR_ARCHITECTURE");

    if (processorArchitecture.Equals("x86")) {
        return false;
    }
    else {
        return true;
    }
}


It's worked out pretty well for us so far, but I'm not sure how much I like looking through the registry. Is this a pretty standard practice or is there a better method?

Edit: Wow, that code looks a lot prettier in the preview. I'll consider linking to a pastebin or something, next time.


I inherited a Windows Forms app written in VB.Net.  Certain parts of the app run dreadfully slow.  What's the easiest way to find which parts of the code are holding things up?  I'm looking for a way to quickly find the slowest subroutines and tackle them first in an attempt to speed up the app.

I know that there are several code profiler products available for purchase which will show how long each subroutine takes, but I was hoping to find a free solution.

Most recent edits in bold
I am using the .net HttpListener class, but I'm won't be running on IIS and am not using ASP.net.  This web site describes what code to actual use to implement SSL with asp.net and this site describes how to set up the certificates (although I'm not sure if it works only for IIS or not).  

The class documentation describes various types of authentication (basic, digest, Windows, etc.) --- none of them refer to SSL.  It does say that if HTTPS is used, you will need to set a server certificate.  Is this going to be a one line property setting and HttpListener figures out the rest?  

In short, I need to know how to set up the certificates and how to modify the code to implement SSL.

Although it doesn't occur when I'm trying to access HTTPS, I did notice an error in my System Event log - the source is "Schannel" and the content of the message is:


  A fatal error occurred when attempting
  to access the SSL server credential
  private key. The error code returned
  from the cryptographic module is
  0x80090016.


Edit:
Steps taken so far


Created a working HTTPListener in C# that works for HTTP connections (e.g. "http://localhost:8089/foldername/"
Created a certificate using makecert.exe
Added the certificate to be trusted using certmgr.exe
Used Httpcfg.exe to listen for SSL connections on a test port (e.g. 8090)
Added port 8080 to the HTTPListener via listener.Prefixes.Add(https://localhost:8090/foldername/");
tested an HTTP client connection, e.g. (http://localhost:8089/foldername/") in a browser and receive correct return
tested an HTTPS client connection, e.g. (http://localhost:8090/foldername/") in a browser and receive "Data Transfer Interrupted" (in Firefox)
debugging in visual studio shows that the listener callback that receives the requests never gets hit when the HTTPS connection starts - I don't see any place that I could set a breakpoint to catch anything else earlier.
netstat shows that listening ports are open for both HTTPS and HTTP.  the HTTPS port does go to TIME_WAIT after a connection is attempted.
Fiddler and HTTPAnalyzer don't catch any of the traffic, I guess it doesn't get far enough in the process to show up in those HTTP analysis tools


Questions


What could the problem be? 
Is there a piece of .Net code I am missing (meaning I have to do more in C# other than simply add a prefix to the listener that points to HTTPS, which is what i have done)
Have a missed a configuration step somewhere? 
What else might I do to analyze the problem?
Is the error message in the System Event log a sign of the problem?  If so how would it be fixed?



What is the best way to support multiple languages for the interface in an ASP.NET MVC application? I've seen people use resource files for other applications. Is this still the best way?


I'm trying to convince my providers to use ANT instead of Rational Application Development so anyone can recompile, recheck, redeploy the solution anyplace, anytime, anyhow. :P

I started a build.xml for a project that generates a JAR file but stopped there and I need real examples to compare notes. My good friends! I don't have anyone close to chat about this! 

This is my build.xml so far. 

(*) I edited my question based in the suggestion of to use pastebin.ca

In C can I pass a multidimensional array to a function as a single argument when I don't know what the dimensions of the array are going to be ?

In addition my multidimensional array may contain types other than strings.

Currently I know of only two ways to cache data (I use PHP but I assume that the same will apply to most languages).


Save the cache to a file
Save the cache to a large DB field


Are there any other (perhaps better) ways of caching or is it really just this simple?

I've been tasked with redesigning part of a ms-sql database structure which currently involves a lot of views, some of which contain joins to other views. 

Anyway, I wonder if anyone here could recommend a utility to automatically generate diagrams to help me visualise the whole structure.

What's the best program you've used for such problems?


I am in the process of figuring out a cache strategy for our current setup, currently have multiple web servers and wanted to know what is the best way to cache data in this environment. I have done research about MemCache and the native asp.net caching but wanted to get some feedback first. Should I go with a Linux box if I use MemCache or a win32 port of MemCache.


I'm looking for a simple algorithm to 'serialize' a directed graph. In particular I've got a set of files with interdependencies on their execution order, and I want to find the correct order at compile time. I know it must be a fairly common thing to do - compilers do it all the time - but my google-fu has been weak today. What's the 'go-to' algorithm for this?

Is there a Windows equivalent of the Unix command, nice?

I'm specifically looking for something I can use at the command line, and not the "Set Priority" menu from the task manager.

My attempts at finding this on Google have been thwarted by those who can't come up with better adjectives.


I'd like to use a LinqDataSource control on a page and limit the amount of records returned.  I know if I use code behind I could do something like this:

IEnumerable&lt;int&gt; values = Enumerable.Range(0, 10);
IEnumerable&lt;int&gt; take3 = values.Take(3);


Does anyone know if something like this is possible with a LinqDataSource control?

[Update]

I'm going to use the LinqDataSource with the ListView control, not a GridView or Repeater.  The LinqDataSource wizard does not provide the ability to limit the number of records return.  The Advanced options only allow you to enabled deletes, inserts, and updates.


Is there a trivial, or at least moderately straight-forward way to generate territory maps (e.g. Risk)?

I have looked in the past and the best I could find were vague references to Voronoi diagrams.  An example of a Voronoi diagram is this:

.

These hold promise, but I guess i haven't seen any straight-forward ways of rendering these, let alone holding them in some form of data structure to treat each territory as an object.

Another approach that holds promise is flood fill, but again I'm unsure on the best way to start with this approach.

Any advice would be much appreciated.


I've been doing ASP.NET development for a little while now, and I've used both the GridView and the DataGrid controls before for various things, but I never could find a really good reason to use one or the other. I'd like to know:

What is the difference between these 2 ASP.NET controls? What are the advantages or disadvantages of both? Is one any faster? Newer? Easier to maintain?

The intellisense summary for the controls doesn't seem to describe any difference between the two. They both can view, edit, and sort data and automatically generate columns at runtime.

Edit: Visual Studio 2008 no longer lists DataGrid as an available control in the toolbox. It is still available (for legacy support I assume) if you type it in by hand though.

So my company stores alot of data in a foxpro database and trying to get around the performance hit of touching it directly I was thinking of messaging anything that can be done asynchronously for a snappier user experience.  I started looking at ActiveMQ but don't know how well C# will hook with it.  Wanting to hear what all of you guys think.

edit : It is going to be a web application.  Anything touching this foxpro is kinda slow (probably because the person who set it up 10 years ago messed it all to hell, some of the table files are incredibly large).  We replicate the foxpro to sql nightly and most of our data reads are ok being a day old so we are focusing on the writes. plus the write affects a critical part of the user experience (purchasing), we store it in sql and then just message to have it put into foxpro when it can.  I wish we could just get rid of the foxpro, unfortunately the company doesn't want to get rid of a very old piece of software they bought that depends on it.

What is the best method for executing FTP commands from a SQL Server stored procedure? we currently use something like this:

EXEC master..xp_cmdshell 'ftp -n -s:d:\ftp\ftpscript.xmt 172.1.1.1'

The problem is that the command seems to succeed even if the FTP ended in error. Also, the use of xp_cmdshell requires special permissions and may leave room for security issues.

I have a process in erlang that is supposed to do something immediately after spawn, then send the result back to the parent when it is finished.  How do I figure out the PID of the process that spawned it?

Lots of people talk about writing tests for their code before they start writing their code. This practice is generally known as Test Driven Development or TDD for short. What benefits do I gain from writing software this way? How do I get started with this practice?

Are there any good grid-hosting companies out there that offer .NET stacks? Something like MediaTemple - which won't host the worlds fastest websites, but for the price is far better than "shared hosting". I've used Rackspace's Mosso, but it sucked - it never felt like a normal .NET stack (caching was odd, site recompilation was odd).

Is it "acceptable" to have an ASP.Net 2.0 application without the BLL (Business Logic Layer) as the following?


SQL Server Data Storage &amp; Stored Procedures
Data Link Layer (Strongly Typed Table Adapters) connecting to Stored Procs
Presentation Layer ASPX Pages with Code behind and ObjectDataSource for connection straight to the DLL


Is a BLL always preferable, even if business logic is entirely validatable in the presentation's code behind?  What are the potential drawbacks for not using a BLL?


Is there anyway to configure a WCF service with a failover endpoint if the primary endpoint dies? Kind of like being able to specify a failover server in a SQL cluster.

Specifically I am using the TCP/IP binding for speed, but on the rare occurrence that the machine is not available I would like to redirect traffic to the failover server. Not too bothered about losing messages. I'd just prefer not to write the code to handle re-routing.


Any suggestions?  Using visual studio in C#.

Are there any specific tools to use or methods to approach this?

Update:

Sorry, I should have been a little more specific. I am using ASP.Net 2.0 and was looking more for a tool like jUnit for Java. I took a look at NUnit and NUnitAsp and that looks very promising. And I didn't even know that Visual Studio Pro has a testing suite, so I'll look at all of these options (I've just started using Visual Studio/Asp.net/C# this summer).


I lost my MySQL username and password. How do I retrieve it?

What is the best documentation generator?  I want to something that will easily add templates for documenting functions, classes, etc.  I know there are several tools out there -- from Visual Studio plugins to external applications that take code files as input.


which is the best?  (If language-specific, specify)
are there any documentation tools that could be used for multiple
languages (e.g. VB.net and
JavaScript)


Bonus points if they're free / open source.

Is there available any tool for PHP which can be used to generate code for consuming a web service based on its WSDL? Something comparable to clicking "Add Web Reference" in Visual Studio or the Eclipse plugin which does the same thing for Java.


I'm writing a resource handling method where I control access to various files, and I'd like to be able to make use of the browser's cache.  My question is two-fold:


Which are the definitive HTTP headers that I need to check in order to know for sure whether I should send a 304 response, and what am I looking for when I do check them?
Additionally, are there any headers that I need to send when I initially send the file (like 'Last-Modified') as a 200 response?


Some psuedo-code would probably be the most useful answer.



What about the cache-control header?  Can the various possible values of that affect what you send to the client (namely max-age) or should only if-modified-since be obeyed?


I have the Xming X Window Server installed on a laptop running Windows XP to connect to some UNIX development servers. 

It works fine when I connect directly to the company network in the office. However, it does not work when I connect to the network remotely over a VPN.

When I start Xming when connected remotely none of my terminal Windows are displayed.

I think it may have something to do with the DISPLAY environment variable not being set correctly to the IP address of the laptop when it is connected.

I've noticed that when I do an ipconfig whilst connected remotely that my laptop has two IP addresses, the one assigned to it from the company network and the local IP address I've set up for it on my "local network" from my modem/router.

Are there some configuration changes I need to make in Xming to support its use through the VPN?


Has anyone tried installing SQL Server 2008 Developer on a machine that already has 2005 Developer installed?

I am unsure if I should do this, and I need to keep 2005 on this machine for the foreseeable future in order to test our application easily. Since I sometimes need to take backup files of databases and make available for other people in the company I cannot just replace 2005 with 2008 as I suspect (but do not know) that the databases aren't 100% backwards compatible.

What kind of issues would arise? Do I need to install the new version with an instance name, will that work? Can I use a different port number to distinguish them?

I found this entry on technet: http://forums.microsoft.com/TechNet/ShowPost.aspx?PostID=3496209&amp;SiteID=17

It doesn't say more than just yes you can do this and I kinda suspected that this was doable anyway, but I need to know if there are anything I need to know before I start installing.

Anyone?

How do I generate an ETag HTTP header for a resource file? 


To analyze lots of text logs I did some hackery that looks like this:


Locally import logs into Access 
Reprocess Cube link to previous mdb in Analisis Service 2000 (yes it is 2k)
Use Excel to visualize Cube (it is not big - up to milions raw entries)


My hackery is a succes and more people are demanding an access to my Tool. As you see I see more automating and easier deployment.

Do you now some tools/libraries that would give me the same but with easier deployment?
Kind of embedded OLAP service?

Edit: I heard of Mondrian but we don't do much with Java. Have you seen something similiar done for .Net/Win32 ? Comercial is also OK.


Im trying to create a bookmarklet for posting del.icio.us bookmarks to a seperate account.

I tested it from the command line like:

wget  -O - --no-check-certificate \
"https://seconduser:thepassword@api.del.icio.us/v1/posts/add?url=http://seet.dk&amp;description=test"


and this works great

I then wanted to create a bookmarklet in my firefox. I googled and found bits and pieces and ended up with:

javascript:void(
    open('https://seconduser:password@api.del.icio.us/v1/posts/add?url='
          +encodeURIComponent(location.href)
          +'&amp;description='+encodeURIComponent(document.title),
          'delicious','toolbar=no,width=500,height=250'
        )
    );


but all that happens is that I get this from del.icio.us:

&lt;?xml version="1.0" standalone="yes"?&gt;
&lt;result code="access denied" /&gt;
&lt;!-- fe04.api.del.ac4.yahoo.net uncompressed/chunked Thu Aug  7 02:02:54 PDT 2008 --&gt;  


If I then go to the address bar and press enter, it changes to:

&lt;?xml version='1.0' standalone='yes'?&gt;
&lt;result code="done" /&gt;
&lt;!-- fe02.api.del.ac4.yahoo.net uncompressed/chunked Thu Aug  7 02:07:45 PDT 2008 --&gt;


Any ideas how to get it to work directly from the bookmarks?


I've implemented an image/video transformation technique called discrete cosine transform.  This technique is used in MPEG video encoding.  I based my algorithm on the ideas presented at the following URL:

http://vsr.informatik.tu-chemnitz.de/~jan/MPEG/HTML/mpeg_tech.html

Now I can transform an 8x8 section of a black and white image, such as:


0140  0124  0124  0132  0130  0139  0102  0088  
0140  0123  0126  0132  0134  0134  0088  0117  
0143  0126  0126  0133  0134  0138  0081  0082  
0148  0126  0128  0136  0137  0134  0079  0130  
0147  0128  0126  0137  0138  0145  0132  0144  
0147  0131  0123  0138  0137  0140  0145  0137  
0142  0135  0122  0137  0140  0138  0143  0112  
0140  0138  0125  0137  0140  0140  0148  0143 


Into this an image with all the important information at the top right. The transformed block looks like this:


1041  0039  -023  0044  0027  0000  0021  -019  
-050  0044  -029  0000  0009  -014  0032  -010  
0000  0000  0000  0000  -018  0010  -017  0000  
0014  -019  0010  0000  0000  0016  -012  0000  
0010  -010  0000  0000  0000  0000  0000  0000  
-016  0021  -014  0010  0000  0000  0000  0000  
0000  0000  0000  0000  0000  0000  0000  0000  
0000  0000  -010  0013  -014  0010  0000  0000  


Now, I need to know how can I take advantage of this transformation?  I'd like to detect other 8x8 blocks in the same image ( or another image ) that represent a good match.

Also, What does this transformation give me? Why is the information stored in the top right of the converted image important?


How is it possible to make prototype methods in C#.Net?

In JavaScript, I can do the following to create a trim method for the string object:

String.prototype.trim = function() {
    return this.replace(/^\s+|\s+$/g,"");
}


How can I go about doing this in C#.Net?


I am currently aware that ASP.NET 2.0 is out and about and that there are 3.x versions of the .Net Framework.

Is it possible to upgrade my ASP.NET web server to version 3.x of the .Net Framework?



I have tried this, however, when selecting which version of the .Net framwork to use in IIS (the ASP.NET Tab), only version 1.1 and 2.0 show.

Is there a work around?


I have more than one OpenID as I have tried out numerous.  As people take up OpenID different suppliers are going to emerge I may want to switch provinders.  As all IDs are me, and all are authenticated against the same email address, shouldn't I be able to log into stack overflow with any of them and be able to hit the same account?


Should the folders in a solution match the namespace?

In one of my teams projects, we have a class library that has many sub-folders in the project.

Project Name and Namespace: MyCompany.Project.Section.

Within this project, there are several folders that match the namespace section:


Folder Vehicles has classes in the MyCompany.Project.Section.Vehicles namespace
Folder Clothing has classes in theMyCompany.Project.Section.Clothing namespace
etc.


Inside this same project, is another rogue folder


Folder BusinessObjects has classes in the MyCompany.Project.Section namespace


There are a few cases like this where folders are made for "organizational convenience".

My question is: What's the standard?  In class libraries do the folders usually match the namespace structure or is it a mixed bag?


I was working with quality yesterday doing some formal testing.  In their procedure they were verifying all files on the test machine were pulled from the release.  The way they were verifying these files were the same was by checking the size and the date/time stamp windows put on them in Windows Explorer.  These happened to be off for another reason which I was able to find out why.  

Is this a valid way to verify a file is the same?  I didn't think so and started to argue, but I am younger here so thought I shouldn't push it too far.  I wanted to argue they should do a binary compare on the file to verify its contents are exact.  In my experience time/date stamps and size attributes don't always act as expected.  Any thoughts???


I'm currently experimenting with build script, and since I have an ASP.net Web Part under source control, my build script should do that at the end:


Grab the "naked" Windows 2003 IIS VMWare or Virtual PC Image from the Network
Boot it up
Copy the Files from the Build Folder to the Server
Install it
Do whatever else is needed


I have never tried automating a Virtual Machine, but I saw that both VMWare and Virtual Server offer automation facilities. While I cannot use Virtual Server (Windows XP Home :-(), Virtual PC works.

Does anyone here have experience with either VMWare Server or Virtual PC 2007 SP1 in terms of automation?

Which one is better suited (I run windows, so the Platform-independence of VMWare does not count) and easier to automate?

I'm developing a data access component that will be used in a website that contains a mix of classic ASP and ASP.NET pages, and need a good way to manage its configuration settings.

I'd like to use a custom ConfigurationSection, and for the ASP.NET pages this works great.  But when the component is called via COM interop from a classic ASP page, the component isn't running in the context of an ASP.NET request and therefore has no knowledge of web.config.

Is there a way to tell the ConfigurationManager to just load the configuration from an arbitrary path (e.g. ..\web.config if my assembly is in the /bin folder)?  If there is then I'm thinking my component can fall back to that if the default ConfigurationManager.GetSection returns null for my custom section.

Any other approaches to this would be welcome!


In a installation of WebSphere Application Server with Network Deployment, a node is:


a physical machine
an instance of operative system
a logical set of WAS instances that is independent of physical machine or OS instance


I've written (most of) an application in Flex and I am concerned with protecting the source code. I fired up a demo of Trillix swf decompiler and opened up the swf file that was installed to my Program Files directory. I saw that all of the actionscript packages I wrote were there. I'm not too concerned with the packages, even though there is a substantial amount of code, because it still seems pretty unusable without the mxml files. I think they are converted to actionscript, or atleast I hope. However, I would still like to explore obfuscation.

Does anyone have any experience with Flash / Actionscript 3 / Flex obfuscators? Can you recommend a good product?

Is there a C library function that will return the index of a character in a string?

So far, all I've found are functions like strstr that will return the found char *, not it's location in the original string.

I'm writing a Telnet client of sorts in C# and part of what I have to parse are ANSI/VT100 escape sequences, specifically, just those used for colour and formatting (detailed here).

One method I have is one to find all the codes and remove them, so I can render the text without any formatting if needed:

    
public static string StripStringFormating(string formattedString)
{
    if (rTest.IsMatch(formattedString))
        return rTest.Replace(formattedString, string.Empty);
    else
        return formattedString;
}


I'm new to regular expressions and I was suggested to use this:

static Regex rText = new Regex(@"\e\[[\d;]+m", RegexOptions.Compiled);

However, this failed if the escape code was incomplete due to an error on the server. So then this was suggested, but my friend warned it might be slower (this one also matches another condition (z) that I might come across later):

static Regex rTest = 
              new Regex(@"(\e(\[([\d;]*[mz]?))?)?", RegexOptions.Compiled);

This not only worked, but was in fact faster to and reduced the impact on my text rendering. Can someone explain to a regexp newbie, why? :)


Is there a way to get MS virtual PC 2007 to support multiple displays? Or is there another virtual machine product available that will allow me to work with multiple displays?

At the company I work for we do all of our development in virtual machines.
We currently use MS Virtual PC 2007 for this. I would love to be able to spread my machine's display across multiple displays, but I don't know of any way to do this. Any advice would be appreciated.

I am currently creating a master ddl for our database.  Historically we have used backup/restore to version our database, and not maintained any ddl scripts.  The schema is quite large.

My current thinking:


Break script into parts (possibly in separate scripts):


table creation
add indexes
add triggers
add constraints

Each script would get called by the master script.
I might need a script to drop constraints temporarily for testing
There may be orphaned tables in the schema, I plan to identify suspect tables.


Any other advice?

Edit: Also if anyone knows good tools to automate part of the process, we're using MS SQL 2000 (old, I know).


I'm working on a project which uses .NET Remoting for communication between the client application and an object server. For development, the client, server, and MSSQL database are all running on my local development machine. 

When I'm working at the office, the responsiveness is just fine. 

However, when I work from home the speed is significantly slower. If I disconnect from the VPN, it speeds up (I believe, but maybe that's just wishful thinking). If I turn off my wireless connection completely it immediately speeds up to full throttle.

My assumption is that the remoting traffic is being routed through some point that is slowing everything down, albeit my home router and/or the VPN.

Does anyone have any ideas of how to force the remoting traffic to remain completely localized?

I have written an AIR Application that downloads videos and documents from a server. The videos play inside of the application, but I would like the user to be able to open the documents in their native applications.

I am looking for a way to prompt the user to Open / Save As on a local file stored in the Application Storage Directory. I have tried using the FileReference + URLRequest classes but this throws an exception that it needs a remote url.

My last resort is just copying the file to their desktop : \

I tried to follow a couple of googled up tutorials on setting up mod_python, but failed every time. Do you have a good, step-by step, rock-solid howto?

My dev box is OS X, production - Centos.

I'm working on a website which will switch to a new style on a set date. The site's built in semantic HTML and CSS, so the change should just require a CSS reference change. I'm working with a designer who will need to be able to see how it's looking, as well as a client who will need to be able to review content updates in the current look as well as design progress on the new look.

I'm planning to use a magic querystring value and / or a javascript link in the footer which writes out a cookie to select the new CSS page. We're working in ASP.NET 3.5. Any recommendations?

I should mention that we're using IE Conditional Comments for IE8,7,and 6 support. I may create a function that does a replacement:

&lt;link href="Style/&lt;% GetCssRoot() %&gt;.css" rel="stylesheet" type="text/css" /&gt;
&lt;!--[if lte IE 8]&gt;
    &lt;link type="text/css" href="Style/&lt;% GetCssRoot() %&gt;-ie8.css" rel="stylesheet" /&gt;
&lt;![endif]--&gt;
&lt;!--[if lte IE 7]&gt;
    &lt;link type="text/css" href="Style/&lt;% GetCssRoot() %&gt;-ie7.css" rel="stylesheet" /&gt;
&lt;![endif]--&gt;
&lt;!--[if lte IE 6]&gt;
    &lt;link type="text/css" href="Style/&lt;% GetCssRoot() %&gt;-ie6.css" rel="stylesheet" /&gt;
&lt;![endif]--&gt;



What is the best approach to write hooks for Subversion in Windows? As far as I know, only executable files can be used. So what is the best choice? 


Plain batch files (very limited but perhaps OK for very simple solutions)
Dedicated compiled executable applications (sledgehammer to crack a nutshell?)
Some other hybrid choice (like a batch file running a Powershell script)



A researcher has created a small simulation in MATLAB, and we want to make it accessible to others. My plan is to take the simulation, clean up a few things, and turn it into a set of functions. Then, I plan to compile it into a C library and use SWIG to create a Python wrapper. At that point, I should be able to call the simulation from a small Django app. At least, I hope so.

Do I have the right plan? Has anyone else done something similar? Can you let me know if there are some serious pitfalls I'm not aware of at the moment?

ASP.NET server-side controls postback to their own page.  This makes cases where you want to redirect a user to an external page, but need to post to that page for some reason (for authentication, for instance) a pain.

An HttpWebRequest works great if you don't want to redirect, and JavaScript is fine in some cases, but can get tricky if you really do need the server-side code to get the data together for the post.

So how do you both post to an external URL and redirect the user to the result from your ASP.NET codebehind code?


I am prototyping some C# 3 collection filters and came across this.
I have a collection of products:

public class MyProduct
{
    public string Name { get; set; }
    public Double Price { get; set; }
    public string Description { get; set; }
}

var MyProducts = new  List&lt;MyProduct&gt;
{            
    new  MyProduct
    {
        Name = "Surfboard",
        Price = 144.99,
        Description = "Most important thing you will ever own."
    },
    new MyProduct
    {
        Name = "Leash",
        Price = 29.28,
        Description = "Keep important things close to you."
    }
    ,
    new MyProduct
    {
        Name = "Sun Screen",
        Price = 15.88,
        Description = "1000 SPF! Who Could ask for more?"
    }
};


Now if I use LINQ to filter it works as expected:

var d = (from mp in MyProducts
             where mp.Price &lt; 50d
             select mp);


And if I use the Where extension method combined with a Lambda the filter works as well:

var f = MyProducts.Where(mp =&gt; mp.Price &lt; 50d).ToList();


Question: What is the difference, and why use one over the other?


I am currently in the process of creating my own blog and I have got to marking up the comments, but what is the best way to mark it up?

The information I need to present is:


Persons Name
Gravatar Icon
Comment Date
The Comment


Any idea's would be much appriciated.

PS: I'm only interested in semantic html markup.

The parent div needs to have a defined width, either in pixels or as a percentage. In Internet&nbsp;Explorer&nbsp;7, the parent div needs a defined width for child percentage divs to work correctly.


Here is one hack that may work. It isn't clean, but it looks like it might work:

Essentially, you just try to update a column that doesn't exist.


The problem is that the Distinct
 operator does not grant that it will
 maintain the original order of
 values.

So your query will need to work like this

var names = (from DataRow dr in dataTable.Rows
             select (string)dr["Name"]).Distinct().OrderBy( name =&gt; name );



It is very simple with the Skip and Take extension methods.

var query = from i in ideas
            select i;

var paggedCollection = query.Skip(startIndex).Take(count);



If you use a return type of IEnumerable, you can return your query variable directly.


I've had problems with JavaHL in Eclipse Ganymede, when it worked fine in Eclipse Europa.  I'm not sure how Aptana is different, but try either upgrading JavaHL or switching to the pure-java SVNKit implementation within the Subclipse config.

Rolling your own PHP solution will certainly work though I'm not sure if there is a good way to automatically duplicate the schema from one DB to the other (maybe this was your question).

If you are just copying data, and/or you need custom code anyway to convert between modified schemas between the two DB's, I would recommend using PHP 5.2+ and the PDO libraries.  You'll be able to connect using PDO ODBC (and use MSSQL drivers).  I had a lot of problems getting large text fields and multi-byte characters from MSSQL into PHP using other libraries.

Another tool to try would be the SQLMaestro suite - http://www.sqlmaestro.com It is a little tricky nailing down the precise tool, but they have a variety of tools, both free and for purchase that handle a wide variety of tasks for multiple database platforms. I'd suggest trying the Data Wizard tool first for MySQL, since I believe that will have the proper "import" tool you need.

The best way that I have found is the MySQL Migration Toolkit provided by MySQL.  I have used it successfully for some large migration projects.

You can do it by adding the following anywhere in your code 

$Id:$


So for example @Jeff did:

&lt;div id="svnrevision"&gt;svn revision: $Id:$&lt;/div&gt;


and when checked in the server replaced $Id:$ with the current revision number.  I also found this reference.

There is also $Date:$, $Rev:$, $Revision:$


Looks like Jeff is using CruiseControl.NET based on some leafing through the podcast transcripts.  This seems to have automated deployment capabilities from source control to production. Might this be where the insertion is happening?


Hey, I'm still stuck in that camp too. The third party application we have to support is FINALLY going to 2K5, so we're almost out of the wood. But I feel your pain 8^D

That said, from everything I heard from our DBA, the key is to convert the database to 8.0 format first, and then go to 2005. I believe they used the built in migration/upgrade tools for this. There are some big steps between 6.5 and 8.0 that are better solved there than going from 6.5 to 2005 directly.

Your BIGGEST pain, if you didn't know already, is that DTS is gone in favor of SSIS. There is a shell type module that will run your existing DTS packages, but you're going to want to manually recreate them all in SSIS. Ease of this will depend on the complexity of the packages themselves, but I've done a few at work so far and they've been pretty smooth.

I don't know the ASP.NET side of the problem but .NET has a pretty powerful class: System.Globalization.CultureInfo. You can use the following code to parse a string containing a double value:

double d = double.Parse("100.20", CultureInfo.CurrentCulture);//  -- OR --double d = double.Parse("100.20", CultureInfo.CurrentUICulture);

If ASP.NET somehow (i.e. using HTTP Request headers) passes current user's CultureInfo to either CultureInfo.CurrentCulture or CultureInfo.CurrentUICulture, these will work fine.

You can't please everyone.  If I enter ten as 10.000, and someone enters ten thousand as 10.000, you cannot handle that without some knowledge of the culture of the input.  Detect the culture somehow (browser, system setting - what is the use case?  ASP?  Internal app, or open to the world?), or provide an example of the expected formatting, and use the most lenient parser you can.  Probably something like:

double d = Double.Parse("5,000.00", NumberStyles.Any, CultureInfo.InvariantCulture);

The functionality you're looking for is traditionally called an UPSERT. Atleast knowing what it's called might help you find what you're looking for.

I don't think SQL Server 2005 has any great ways of doing this. 2008 introduces the MERGE statement that can be used to accomplish this as shown in: http://www.databasejournal.com/features/mssql/article.php/3739131 or http://blogs.conchango.com/davidportas/archive/2007/11/14/SQL-Server-2008-MERGE.aspx

Merge was available in the beta of 2005, but they removed it out in the final release.

What the upsert/merge is doing is something to the effect of...

IF EXISTS (SELECT * FROM [Table] WHERE Id = X)
   UPDATE [Table] SET...
ELSE
   INSERT INTO [Table]


So hopefully the combination of those articles and this pseudo code can get things moving.

I think the best you can do in this case is to take their input and then show them what you think they meant. If they disagree, show them the format you're expecting and get them to enter it again.

Any assemblies that are expected to be in the GAC should stay in the GAC. This includes System.web.dll or any other 3rd party dll that you'll deploy to the GAC in production. This means a new developer would have to install these assemblies.

All other 3rd party assemblies should be references through a relative path. My typical structure is:

-Project--Project.sln--References---StructureMap.dll---NUnit.dll---System.Web.Mvc.dll--Project.Web---Project.Web.Proj---Project.Web.Proj files--Project---Project.Proj---Project.Proj files

Project.Web and Project reference the assemblies in the root/References folder relatively. These .dlls are checked into subversion.

Aside from that, */bin  */bin/* obj  should be in your global ignore path.

With this setup, all references to assemblies are either through the GAC (so should work across all computers), or relative to each project within your solution.

Is this a .Net specific question?

Generally the best practice is to not check in anything which is built automatically from files that are already in SCM.  All of that is ideally created as part of your automatic build process.

If the bin directory you're referring to contains third-party binaries, rather than a build of your project, ignore (downvote?) this advice.

I know that Visual Studio itself (at least in 2003 version) references the IE dll directly to render the "Design View".

It may be worth looking into that.

Otherwise, I can't think of anything beyond the Web Browser control.

I was glad to find a good conversation on this subject, as I hadn't really given it much thought before.

In summary, signed is a good general choice - even when you're dead sure all the numbers are positive - if you're going to do arithmetic on the variable (like in a typical for loop case).  

If you're going to do bitwise things like masks, unsigned starts to make more sense.  Or, if you're desperate to get that extra positive range by taking advantage of the sign bit.

Personally, I like signed because I don't trust myself to stay consistent and avoid mixing the two types (like the article warns against).

In your example above, when 'i' will always be positive and a higher range would be beneficial, unsigned would be useful. Like if you're using 'declare' statements, such as: 

#declare BIT1 (unsigned int 1)#declare BIT32 (unsigned int reallybignumber)

Especially when these values will never change.

However, if you're doing an accounting program where the people are irresponsible with their money and are constantly in the red, you will most definitely want to use 'signed'.

I do agree with saint though that a good rule of thumb is to use signed, which C actually defaults to, so you're covered.

size_t is often a good choice for this, or size_type if you're using an STL class.


The difference between 12.345 in French and English is a factor of 1000. If you supply an expected range where max &lt; 1000*min, you can easily guess. 

Take for example the height of a person (including babies and children) in mm.

By using a range of 200-3000, an input of 1.800 or 1,800 can unambiguously be interpreted as 1 meter and 80 centimeters, whereas an input of 912.300 or 912,300 can unambiguously be interpreted as 91 centimeters and 2.3 millimeters.

open up a terminal (Applications-&gt;Utilities-&gt;Terminal) and type this in:

locate InsertFontHere

This will spit out every file that has the name you want.

Warning: there may be alot to wade through.

I haven't been able to find anything that does this directly.  I think you'll have to iterate through the various font folders on the system: /System/Library/Fonts, /Library/Fonts, and there can probably be a user-level directory as well ~/Library/Fonts.


Using fcgi with Ruby is known to be very buggy. 

Practically everybody has moved to Mongrel for this reason, and I recommend you do the same.

Doesn't this depend on the hardware as well as number of threads and stuff?

I would make a simple test and run it with increasing amounts of threads hammering and see what seems best.

Maven helps quite a lot with this problem when I'm coding java. We commit the pom.xml to the scs and the maven repository contains all our dependencies.
For me that seems like a nice way to do it.

One possibility is Hudson.  It's written in Java, but there's integration with Python projects:


  Hudson embraces Python


I've never tried it myself, however.

(Update, Sept. 2011: After a trademark dispute Hudson has been renamed to Jenkins.)


We run Buildbot - Trac at work, I haven't used it too much since my code base isn't part of the release cycle yet. But we run the tests on different environments (OSX/Linux/Win) and it sends emails --and it's written in python.

Interesting question!

I would do this by picking combinations, something like the following in python.  The hardest part is probably first pass verification, i.e. if f(1,2,3) returns true, is that a correct result?  Once you have verified that, then this is a good basis for regression testing.

Probably it's a good idea to make a set of test cases that you know will be all true (e.g. 3,4,5 for this triangle case), and a set of test cases that you know will be all false (e.g. 0,1,inf).  Then you can more easily verify the tests are correct.


# xpermutations from http://code.activestate.com/recipes/190465
from xpermutations import *

lengths=[-1,0,1,5,10,0,1000,'inf']
for c in xselections(lengths,3):        # or xuniqueselections
    print c



(-1,-1,-1);
(-1,-1,0);
(-1,-1,1);
(-1,-1,5);
(-1,-1,10);
(-1,-1,0);
(-1,-1,1000);
(-1,-1,inf);
(-1,0,-1);
(-1,0,0);
...



For C++, Boost.Random is probably what you're looking for. It has support for MT (among many other algorithms), and can collect entropy via the nondet_random class. Check it out! :-)

The Gnu Scientific Library (GSL) has a pretty extensive set of RN generators, test harness, etc.  If you're on linux, it's probably already available on your system.

It's been awhile since I used FCGI but I think a FCGI process could throw a SystemExit if the thread was taking too long.  This could be the web service not responding or even a slow DNS query.  Some google results show a similar error with Python and FCGI so moving to mongrel would be a good idea.  This post is my reference I used to setup mongrel and I still refer back to it.


With the "Built in" stuff, you can't, as using 1.0.* or 1.0.0.* will replace the revision and build numbers with a coded date/timestamp, which is usually also a good way. 

For more info, see the Assembly Linker Documentation in the /v tag.

As for automatically incrementing numbers, use the AssemblyInfo Task:

AssemblyInfo Task

This can be configured to automatically increment the build number.

There are 2 Gotchas:


Each of the 4 numbers in the Version string is limited to 65535. This is a Windows Limitation and unlikely to get fixed.

Why are build numbers limited to 65535?

Using with with Subversion requires a small change:

Using MSBuild to generate assembly version info at build time (including SubVersion fix)



Retrieving the Version number is then quite easy:

Version v = Assembly.GetExecutingAssembly().GetName().Version;
string About = string.Format(CultureInfo.InvariantCulture, @"YourApp Version {0}.{1}.{2} (r{3})", v.Major, v.Minor, v.Build, v.Revision);




And, to clarify: In .net or at least in C#, the build is actually the THIRD number, not the fourth one as some people (for example Delphi Developers who are used to Major.Minor.Release.Build) might expect.

In .net, it's Major.Minor.Build.Revision.


Second the Buildbot - Trac integration. You can find more information about the integration on the Buildbot website. At my previous job, we wrote and used the plugin they mention (tracbb).
What the plugin does is rewriting all of the Buildbot urls so you can use Buildbot from within Trac. (http://example.com/tracbb).

The really nice thing about Buildbot is that the configuration is written in Python. You can integrate your own Python code directly to the configuration. It's also very easy to write your own BuildSteps to execute specific tasks.

We used BuildSteps to get the source from SVN, pull the dependencies, publish test results to WebDAV, etcetera.

I wrote an X10 interface so we could send signals with build results. When the build failed, we switched on a red lava lamp. When the build succeeded, a green lava lamp switched on. Good times :-)

It depends on what kind of application you are building. Create a representative test scenario, and start hammering away. Then you will know the definitive answer.

Besides your use case, it also depends on CPU, memory, front-side bus, operating system, cache settings, etcetera.

Seriously, just test your own scenario.

If you need some numbers (that actually may mean nothing in your scenario):


Oracle Berkeley DB: 
Performance Metrics and 
Benchmarks
Performance Metrics 
&amp; Benchmarks: 
Berkeley DB


If you are choosing an encryption method for your login system then speed is not your friend, Jeff had a to-and-frow with Thomas Ptacek about passwords and the conclusion was that you should use the slowest, most secure encryption method you can afford to. 


  From Thomas Ptacek's blog:
  Speed is exactly what you don’t want in a password hash function.
  
  Modern password schemes are attacked with incremental password crackers.
  
  Incremental crackers don’t precalculate all possible cracked passwords. They consider each password hash individually, and they feed their dictionary through the password hash function the same way your PHP login page would. Rainbow table crackers like Ophcrack use space to attack passwords; incremental crackers like John the Ripper, Crack, and LC5 work with time: statistics and compute.
  
  The password attack game is scored in time taken to crack password X. With rainbow tables, that time depends on how big your table needs to be and how fast you can search it. With incremental crackers, the time depends on how fast you can make the password hash function run.
  
  The better you can optimize your password hash function, the faster your password hash function gets, the weaker your scheme is. MD5 and SHA1, even conventional block ciphers like DES, are designed to be fast. MD5, SHA1, and DES are weak password hashes. On modern CPUs, raw crypto building blocks like DES and MD5 can be bitsliced, vectorized, and parallelized to make password searches lightning fast. Game-over FPGA implementations cost only hundreds of dollars. 



I'm with Peter. Developer don't seem to understand passwords. We all pick (and I'm guilty of this too) MD5 or SHA1 because they are fast. Thinking about it ('cuz someone recently pointed it out to me) that doesn't make any sense. We should be picking a hashing algorithm that's stupid slow. I mean, on the scale of things, a busy site will hash passwords what? every 1/2 minute? Who cares if it take 0.8 seconds vs 0.03 seconds server wise? But that extra slowness is huge to prevent all types of common brute-forcish attacks.

From my reading, bcrypt is specifically designed for secure password hashing. It's based on blowfish, and there are many implementation.

For PHP, check out PHPPass http://www.openwall.com/phpass/

For anyone doing .NET, check out BCrypt.NET http://derekslager.com/blog/posts/2007/10/bcrypt-dotnet-strong-password-hashing-for-dotnet-and-mono.ashx

No, there are many applications built with VS2005 that have to support Windows XP, 2000, NT, the whole stack. The issue is that (by default) VS2005 wants to use libraries/exports not present on NT.
See this thread for some background.
Then start limiting your dependencies via preprocessor macros, and avoiding APIs which aren't supported on NT.

Here is my current method. Any suggestions?

Regex singleMToDoubleRegex = new Regex("(?&lt;!m)m(?!m)");Regex singleDToDoubleRegex = new Regex("(?&lt;!d)d(?!d)");CultureInfo currentCulture = CultureInfo.CurrentUICulture;// If the culture is netural there is no date pattern to use, so use the default.if (currentCulture.IsNeutralCulture){    currentCulture = CultureInfo.InvariantCulture;}// Massage the format into a more general user friendly form.string shortDatePattern = CultureInfo.CurrentUICulture.DateTimeFormat.ShortDatePattern.ToLower();shortDatePattern = singleMToDoubleRegex.Replace(shortDatePattern, "mm");shortDatePattern = singleDToDoubleRegex.Replace(shortDatePattern, "dd");

How about giving the format (mm/dd/yyyy or dd/mm/yyyy) followed by a printout of today's date in the user's culture. MSDN has an article on formatting a DateTime for the person's culture, using the CultureInfo object that might be helpful in doing this. A combination of the format (which most people are familiar with) combined with the current date represented in that format should be enough of a clue to the person on how they should enter the date. (Also include a calendar control for those who still cant figure it out).

You can print from the command line using the following:


  rundll32.exe
  %WINDIR%\System32\mshtml.dll,PrintHTML
  "%1"


Where %1 is the file path of the html file to be printed.

If you don't need to print from memory (or can afford to write to the disk in a temp file) you can use:

using (Process printProcess = new Process()){    string systemPath = Environment.GetFolderPath(Environment.SpecialFolder.System);    printProcess.StartInfo.FileName = systemPath + @"\rundll32.exe";    printProcess.StartInfo.Arguments = systemPath + @"\mshtml.dll,PrintHTML """ + fileToPrint + @"""";    printProcess.Start();}

N.B. This only works on Windows 2000 and above I think.

Just use ISO-8601. It's an international standard.

Date and time (current at page generation) expressed according to ISO 8601:
Date:                           2014-07-05
Combined date and time in UTC:  2014-07-05T04:00:25+00:00
                                2014-07-05T04:00:25Z
Week:                           2014-W27
Date with week number:          2014-W27-6
Ordinal date:                   2014-186



I don't have any experience with http://www.SiteGround.com as a web host personally.  

This is just a guess, but it's common for a shared host to support Python and MySQL with the MySQLdb module (e.g., GoDaddy does this).  Try the following CGI script to see if MySQLdb is installed.

#!/usr/bin/pythonmodule_name = 'MySQLdb'head = '''Content-Type: text/html%s is ''' % module_nametry:    __import__(module_name)    print head + 'installed'except ImportError:    print head + 'not installed'

What source control system are you using? 

Almost all of them have some form of $ Id $ tag that gets expanded when the file is checked in.

I usually use some form of hackery to display this as the version number.

The other alternative is use to use the date as the build number: 080803-1448

Absolutely, especially dealing with lots of these permutations/combinations I can definitely see that the first pass would be an issue.

Interesting implementation in python, though I wrote a nice one in C and Ocaml based on "Algorithm 515" (see below). He wrote his in Fortran as it was common back then for all the "Algorithm XX" papers, well, that assembly or c. I had to re-write it and make some small improvements to work with arrays not ranges of numbers. This one does random access, I'm still working on getting some nice implementations of the ones mentioned in Knuth 4th volume fascicle 2. I'll an explanation of how this works to the reader. Though if someone is curious, I wouldn't object to writing something up.

/** [combination c n p x] * get the [x]th lexicographically ordered set of [p] elements in [n] * output is in [c], and should be sizeof(int)*[p] */void combination(int* c,int n,int p, int x){    int i,r,k = 0;    for(i=0;i&lt;p-1;i++){        c[i] = (i != 0) ? c[i-1] : 0;        do {            c[i]++;            r = choose(n-c[i],p-(i+1));            k = k + r;        } while(k &lt; x);        k = k - r;    }    c[p-1] = c[p-2] + x - k;}

~"Algorithm 515: Generation of a Vector from the Lexicographical Index"; Buckles, B. P., and Lybanon, M. ACM Transactions on Mathematical Software, Vol. 3, No. 2, June 1977.

I uploaded it and got an internal error

Premature end of script headers

After much playing around, I found that if I had

import cgiimport cgitb; cgitb.enable()import MySQLdb

It would give me a much more useful answer and say that it was not installed, you can see it yourself -> http://woarl.com/db.py

Oddly enough, this would produce an error

import MySQLdbimport cgiimport cgitb; cgitb.enable()

I looked at some of the other files I had up there and it seems that library was one of the ones I had already tried.

I'm partway to my solution with this entry on MSDN (don't know how I couldn't find it before).

User/Machine Hive
Subkeys and values entered under this hive will be installed under the HKEY_CURRENT_USER hive when a user chooses "Just Me" or the HKEY_USERS hive or when a user chooses "Everyone" during installation.

Registry Editor


First: Yes, this is something that belongs in the Application for the exact reson you specified: What happens after new user profiles are created? Sure, if you're using a domain it's possible to have some stuff put in the registry on creation, but this is not really a use case. The Application should check if there are seetings and use the default settings if not.

That being said, it IS possible to change other users Keys through the HKEY_USERS Hive.

I have no experience with the Visual Studio 2003 Setup Project, so here is a bit of (totally unrelated) VBScript code that might just give you an idea where to look:

const HKEY_USERS = &amp;H80000003strComputer = "."Set objReg=GetObject("winmgmts:{impersonationLevel=impersonate}!\\" &amp; strComputer &amp; "\root\default:StdRegProv")strKeyPath = ""objReg.EnumKey HKEY_USERS, strKeyPath, arrSubKeysstrKeyPath = "\Software\Microsoft\Windows\CurrentVersion\WinTrust\Trust Providers\Software Publishing"For Each subkey In arrSubKeys    objReg.SetDWORDValue HKEY_USERS, subkey &amp; strKeyPath, "State", 146944Next

(Code Courtesy of Jeroen Ritmeijer)

I'm guessing that because you want to set it for all users, that you're on some kind of shared computer, which is probably running under a domain?

HERE BE DRAGONS

Let's say Joe and Jane regularly log onto the computer, then they will each have 'registries'.

You'll then install your app, and the installer will employ giant hacks and disgusting things to set items under HKCU for them.

THEN, bob will come along and log on (he, and 500 other people have accounts in the domain and so can do this). He's never used this computer before, so he has no registry. The first time he logs in, windows creates him one, but he won't have your setting. 

Your app then falls over or behaves incorrectly, and bob complains loudly about those crappy products from raynixon incorporated.

The correct answer is to just have some default settings in your app, which can write them to the registry if it doesn't find them. It's general good practice that your app should never depend on the registry, and should create things as needed, for any registry entry, not just HKCU, anyway

A solution, albeit one that defers handling of the null value to the code, could be:


  DateTime yesterday = DateTime.Now.Date.AddDays(-1);


var collection=
    from u in db.Universe
    select new
    {
        u.id,
        u.name,
        MaxDate =(DateTime?)
       (
           from h in db.History
           where u.Id == h.Id
           &amp;&amp; h.dateCol &lt; yesterday
           select h.dateCol 
       ).Max()
    };


This does not produce exactly the same SQL, but does provide the same logical result. Translating "complex" SQL queries to LINQ is not always straightforward.


You're looking for Sandcastle

Project Page: Sandcastle Releases

Blog: Sandcastle Blog

NDoc Code Documentation Generator for .NET used to be the tool of choice, but support has all but stopped.


Have a look at Sandcastle, which does exactly that. It's also one of the more simpler solutions out there, and it's more or less the tool of choice, so in the long run, maybe we could help you to set up Sandcastle if you specify what issues you encountered during setup?

Despite what the MSDN article  says about User/Machine Hive, it doesn't write to HKEY_USERS. Rather it writes to HKCU if you select Just Me and HKLM if you select Everyone.

So my solution is going to be to use the User/Machine Hive, and then in the application it checks if the registry entries are in HKCU and if not, copies them from HKLM. I know this probably isn't the most ideal way of doing it, but it has the least amount of changes.

If you have the ability to use WScript.Shell then you can just execute pscp.exe from the Putty package. Obviously this is less then ideal but it will get the job done and let you use SCP/SFTP in classic ASP.

I am by no means authoritative, but I believe the only supported path is from 6.5 to 7. Certainly that would be the most sane route, then I believe you can migrate from 7 directly to 2005 pretty painlessly.

As for scripting out all the objects - I would advise against it as you will inevitably miss something (unless you database is truly trivial).

If you can find a professional or some other super-enterprise version of Visual Studio 6.0 - it came with a copy of MSDE (Basically the predecessor to SQL Express). I believe MSDE 2000 is still available as a free download from Microsoft, but I don't know if you can migrate directly from 6.5 to 2000.

I think in concept, you won't likely face any danger. Years of practice however tell me that you will always miss some object, permission, or other database item that won't manifest itself immediately. If you can script out the entire dump, the better as you will be less likely to miss something - and if you do miss something, it can be easily added to the script and fixed. I would avoid any manual steps (other than hitting the enter key once) like the plague.

Does doubling the \ work?

insert into EscapeTest (text) values ('This will be inserted \\n This will not be');

Partially. The text is inserted, but the warning is still generated.

I found a discussion that indicated the text needed to be preceded with 'E', as such:

insert into EscapeTest (text) values (E'This is the first part \n And this is the second');


This suppressed the warning, but the text was still not being returned correctly. When I added the additional slash as Michael suggested, it worked.

As such:

insert into EscapeTest (text) values (E'This is the first part \\n And this is the second');



Very roughly and from memory since I don't have code on this laptop:

using (OleDBConnection conn = new OleDbConnection())
{
  conn.ConnectionString = "Whatever connection string";

  using (OleDbCommand cmd = new OleDbCommand())
  {
    cmd.Connection = conn;
    cmd.CommandText = "Select * from CoolTable";

    using (OleDbDataReader dr = cmd.ExecuteReader())
    {
      while (dr.Read())
      {
        // do something like Console.WriteLine(dr["column name"] as String);
      }
    }
  }
}



Cool.

I also found the documentation regarding the E:

http://www.postgresql.org/docs/8.3/interactive/sql-syntax-lexical.html#SQL-SYNTAX-STRINGS


  PostgreSQL also accepts "escape" string constants, which are an extension to the SQL standard. An escape string constant is specified by writing the letter E (upper or lower case) just before the opening single quote, e.g. E'foo'. (When continuing an escape string constant across lines, write E only before the first opening quote.) Within an escape string, a backslash character (\) begins a C-like backslash escape sequence, in which the combination of backslash and following character(s) represents a special byte value. \b is a backspace, \f is a form feed, \n is a newline, \r is a carriage return, \t is a tab. Also supported are \digits, where digits represents an octal byte value, and \xhexdigits, where hexdigits represents a hexadecimal byte value. (It is your responsibility that the byte sequences you create are valid characters in the server character set encoding.) Any other character following a backslash is taken literally. Thus, to include a backslash character, write two backslashes (\\). Also, a single quote can be included in an escape string by writing \', in addition to the normal way of ''.


You can upgrade 6.5 to SQL Server 2000.  You may have an easier time getting a hold of SQL Server or the 2000 version of the MSDE.  Microsoft has a page on going from 6.5 to 2000.  Once you have the database in 2000 format, SQL Server 2005 will have no trouble upgrading it to the 2005 format.  

If you don't have SQL Server 2000, you can download the MSDE 2000 version directly from Microsoft.

@Goyuix -- that's excellent for something written from memory.
tested it here -- found the connection wasn't opened. Otherwise very nice.

using System.Data.OleDb;
...

using (OleDbConnection conn = new OleDbConnection())
{
    conn.ConnectionString = "Provider=sqloledb;Data Source=yourServername\\yourInstance;Initial Catalog=databaseName;Integrated Security=SSPI;";

    using (OleDbCommand cmd = new OleDbCommand())
    {
        conn.Open();
        cmd.Connection = conn;
        cmd.CommandText = "Select * from yourTable";

        using (OleDbDataReader dr = cmd.ExecuteReader())
        {
            while (dr.Read())
            {
                Console.WriteLine(dr["columnName"]);
            }
        }
    }
}



is this a console app or a winforms app? If it's a .NET 1.1 console app this is, sadly, by design -- it's confirmed by a MSFT dev in the second blog post you referenced:


  BTW, on my 1.1 machine the example from MSDN does have the expected output; it's just that the second line doesn't show up until after you've attached a debugger (or not). In v2 we've flipped things around so that the UnhandledException event fires before the debugger attaches, which seems to be what most people expect.


Sounds like .NET 2.0 does this better (thank goodness), but honestly, I never had time to go back and check.

It's a WinForms app. The exceptions that are caught by Application.ThreadException work fine, and I don't get the ugly .NET exception box (OK to terminate, cancel to debug? who came up with that??).

I was getting some exceptions that weren't being caught by that and ended up going to the AppDomain.UnhandledException event that were causing problems. I think I've caught most of those exceptions and I am displaying them in our nice error box now.

So I'll just have to hope there are not some other circumstances that would cause exceptions to not be caught by the Application.ThreadException handler.

Oh, in WinForms you definitely should be able to get it to work. The only thing you have to watch out for is things happening on different threads.

I have an old CodeProject article here which should help:

http://www.codeproject.com/KB/exception/ExceptionHandling.aspx

That's definitely a good way to do it.  But you if you happen to be using a database that supports LINQ to SQL, it can be a lot more fun.  It can look something like this:

MyDB db = new MyDB("Data Source=...");var q = from db.MyTable        select c;foreach (var c in q)  Console.WriteLine(c.MyField.ToString());

You can use this program, Handle, to find which process has the lock on your file. It's a command-line tool, so I guess you use the output from that... I'm not sure about finding it programmatically.

If deleting the file can wait, you could specify it for deletion when your computer next starts up:


Start REGEDT32 (W2K) or REGEDIT (WXP) and navigate to:

HKEY_LOCAL_MACHINE\System\CurrentControlSet\Control\Session Manager

W2K and WXP


W2K:EditAdd Value...Data Type: REG_MULTI_SZValue Name: PendingFileRenameOperationsOK
WXP:EditNewMulti-String Valueenter
PendingFileRenameOperations

In the Data area, enter "\??\" + filename to be deleted. LFNs may
be entered without being embedded in quotes. To delete C:\Long Directory Name\Long File Name.exe, enter the following data:

\??\C:\Long Directory Name\Long File Name.exe


Then press OK.
The "destination file name" is a null (zero) string. It is entered
as follows:


W2K:EditBinaryselect Data Format: Hexclick at the end of the hex stringenter 0000 (four zeros)OK
WXP:Right-click the valuechoose "Modify Binary Data"click at the end of the hex stringenter 0000 (four zeros)OK

Close REGEDT32/REGEDIT and reboot to delete the file.


(Shamelessly stolen from some random forum, for posterity's sake.)


Killing other processes is not a healthy thing to do. If your scenario involves something like uninstallation, you could use the MoveFileEx API function to mark the file for deletion upon next reboot.

If it appears that you really need to delete a file in use by another process, I'd recommend re-considering the actual problem before considering any solutions.


If you want to do it programatically. I'm not sure... and I'd really recommend against it.
If you're just troubleshooting stuff on your own machine, SysInternals Process Explorer can help you

Run it, use the Find Handle command (I think it's either in the find or handle menu), and search for the name of your file. Once the handle(s) is found, you can forcibly close them.

You can then delete the file and so on.

Beware, doing this may cause the program which owns the handles to behave strangely, as you've just pulled the proverbial rug out from under it, but it works well when you are debugging your own errant code, or when visual studio / windows explorer is being crap and not releasing file handles even though you told them to close the file ages ago... sigh :-)

Oh, one big hack I employed years ago, is that Windows won't let you delete files, but it does let you move them.

Pseudo-sort-of-code:

mv %WINDIR%\System32\mfc42.dll %WINDIR\System32\mfc42.dll.old
Install new mfc42.dll
Tell user to save work and restart applications


When the applications restarted (note we didn't need to reboot the machine), they loaded the new mfc42.dll, and all was well. That, coupled with PendingFileOperations to delete the old one the next time the whole system restarted, worked pretty well.


The typical method is as follows. You've said you want to do this in C# so here goes...

If you don't know which process has the file locked, you'll need to examine each process's handle list, and query each handle to determine if it identifies the locked file. Doing this in C# will likely require P/Invoke or an intermediary C++/CLI to call the native APIs you'll need.
Once you've figured out which process(es) have the file locked, you'll need to safely inject a small native DLL into the process (you can also inject a managed DLL, but this is messier, as you then have to start or attach to the .NET runtime).
That bootstrap DLL then closes the handle using CloseHandle etc.
Essentially: the way to unlock a "locked" file is to inject a DLL into the offending process's address space and close it yourself. You can do this using native or managed code. No matter what, you're going to need a small amount of native code or at least P/Invoke into the same.
Helpful links:

http://www.codeproject.com/KB/threads/winspy.aspx
http://damianblog.com/2008/07/02/net-code-injection/
Good luck!

What I did when working against a database of unknown performance was to measure turnaround time on my queries.  I kept upping the thread count until turn-around time dropped, and dropping the thread count until turn-around time improved (well, it was processes in my environment, but whatever).  

There were moving averages and all sorts of metrics involved, but the take-away lesson was: just adapt to how things are working at the moment.  You never know when the DBAs will improve performance or hardware will be upgraded, or perhaps another process will come along to load down the system while you're running.  So adapt.

Oh, and another thing: avoid process switches if you can - batch things up.



Oh, I should make this clear: this all happened at run time, not during development.


The trouble with international standards is that pretty much noone uses them. I try where I can, but I am forced to use dd/mm/yyyy almost everywhere in real life, which means I am so used to it it's always a conscious process to use ISO-8601. For the majority of people who don't even try to use ISO-8601 it's even worse. If you can internationalize where you can, I think it's a great advantage.

Yes, you are right to lock at the VSZ.

ps u will give you the VSZ and RSS, which are the virtual memory size and resident set size.  The RSS is how much physical memory has been allocated to the process, and the VSZ is the virtual memory size of the process.  If you have several copies of a program running, a lot of the memory in the VSZ will be shared between those processes.


Maybe https://ci-bayes.dev.java.net/ or http://www.cs.cmu.edu/~javabayes/Home/node2.html?

I never played with it either.


Here is an implementation of Bayesian filtering in C#: A Naive Bayesian Spam Filter for C# (hosted on CodeProject).

No.

See A Pragmatic Look at Exception Specifications for reasons why not. 

The only way you can "help" this is to document the exceptions your function can throw, say as a comment in the header file declaring it. This is not enforced by the compiler or anything. Use code reviews for that purpose.


AppDomain.UnhandledException is an event, not a global exception handler. This means, by the time it is raised, your application is already on its way down the drain, and there is nothing you can do about it, except for doing cleanup and error logging.

What happened behind the scenes is this: The framework detected the exception, walked up the call stack to the very top, found no handlers that would recover from the error, so was unable to determine if it was safe to continue execution. So, it started the shutdown sequence, and fired up this event as a courtesy to you so you can pay your respects to your already-doomed process. This happens when an exception is left unhandled in the main thread.

There is no single-point solution to this kind of error. You need to put a real exception handler (a catch block) upstream of all places where this error occurs and forward it to (for example) a global handler method/class that will determine if it is safe to simply report and continue, based on exception type and/or content.

Edit: It is possible to disable (=hack) the error-reporting mechanism built into Windows so the mandatory "crash and burn" dialog does not get displayed when your app goes down. However, this becomes effective for all the applications in the system, not just your own.

I've just set up Sandcastle again. Try installing it (the May 2008 release) and search for SandcastleGui.exe or something similar (it's in the examples folder or so).

Click Add Assembly and add your Assembly or Assemblies, add any .xml Documentation files (the ones generated by the compiler if you enabled that option) and then Build.

It will take some time, but the result will be worth the effort. It will actually look up stuff from MSDN, so your resulting documentation will also have the Class Inheritance all the way down to System.Object with links to MSDN and stuff.

Sandcastle seems a bit complicated at first, especially when you want to use it in an automated build, but I am absolutely sure it will be worth the effort.

Also have a look at Sandcastle Help File Builder, this is a somewhat more advanced GUI for it.


Follow this simple 5 step article and you are pretty much done. As a bonus you can use H2Viewer to view Html Help 2.x files.

Why is it needed?

When data is stored on disk based storage devices, it is stored as blocks of data. These blocks are accessed in their entirety, making them the atomic disk access operation. Disk blocks are structured in much the same way as linked lists; both contain a section for data, a pointer to the location of the next node (or block), and both need not be stored contiguously.

Due to the fact that a number of records can only be sorted on one field, we can state that searching on a field that isn’t sorted requires a Linear Search which requires N/2 block accesses (on average), where N is the number of blocks that the table spans. If that field is a non-key field (i.e. doesn’t contain unique entries) then the entire table space must be searched at N block accesses.

Whereas with a sorted field, a Binary Search may be used, this has log2 N block accesses. Also since the data is sorted given a non-key field, the rest of the table doesn’t need to be searched for duplicate values, once a higher value is found. Thus the performance increase is substantial.

What is indexing?

Indexing is a way of sorting a number of records on multiple fields. Creating an index on a field in a table creates another data structure which holds the field value, and pointer to the record it relates to. This index structure is then sorted, allowing Binary Searches to be performed on it.

The downside to indexing is that these indexes require additional space on the disk, since the indexes are stored together in a table using the MyISAM engine, this file can quickly reach the size limits of the underlying file system if many fields within the same table are indexed.

How does it work?

Firstly, let’s outline a sample database table schema; 


Field name       Data type      Size on disk
id (Primary key) Unsigned INT   4 bytes
firstName        Char(50)       50 bytes
lastName         Char(50)       50 bytes
emailAddress     Char(100)      100 bytes


Note: char was used in place of varchar to allow for an accurate size on disk value. 
This sample database contains five million rows, and is unindexed. The performance of several queries will now be analyzed. These are a query using the id (a sorted key field) and one using the firstName (a non-key unsorted field).

Example 1

Given our sample database of r = 5,000,000 records of a fixed size giving a record length of R = 204 bytes and they are stored in a table using the MyISAM engine which is using the default block size B = 1,024 bytes. The blocking factor of the table would be bfr = (B/R) = 1024/204 = 5 records per disk block. The total number of blocks required to hold the table is N = (r/bfr) = 5000000/5 = 1,000,000 blocks. 

A linear search on the id field would require an average of N/2 = 500,000 block accesses to find a value given that the id field is a key field. But since the id field is also sorted a binary search can be conducted requiring an average of log2 1000000 = 19.93 = 20 block accesses. Instantly we can see this is a drastic improvement.

Now the firstName field is neither sorted nor a key field, so a binary search is impossible, nor are the values unique, and thus the table will require searching to the end for an exact N = 1,000,000 block accesses. It is this situation that indexing aims to correct.

Given that an index record contains only the indexed field and a pointer to the original record, it stands to reason that it will be smaller than the multi-field record that it points to. So the index itself requires fewer disk blocks than the original table, which therefore requires fewer block accesses to iterate through. The schema for an index on the firstName field is outlined below; 


Field name       Data type      Size on disk
firstName        Char(50)       50 bytes
(record pointer) Special        4 bytes


Note: Pointers in MySQL are 2, 3, 4 or 5 bytes in length depending on the size of the table.

Example 2

Given our sample database of r = 5,000,000 records with an index record length of R = 54 bytes and using the default block size B = 1,024 bytes. The blocking factor of the index would be bfr = (B/R) = 1024/54 = 18 records per disk block. The total number of blocks required to hold the table is N = (r/bfr) = 5000000/18 = 277,778 blocks. 

Now a search using the firstName field can utilise the index to increase performance. This allows for a binary search of the index with an average of log2 277778 = 18.08 = 19 block accesses. To find the address of the actual record, which requires a further block access to read, bringing the total to 19 + 1 = 20 block accesses, a far cry from the 277,778 block accesses required by the non-indexed table.

When should it be used?

Given that creating an index requires additional disk space (277,778 blocks extra from the above example), and that too many indexes can cause issues arising from the file systems size limits, careful thought must be used to select the correct fields to index.

Since indexes are only used to speed up the searching for a matching field within the records, it stands to reason that indexing fields used only for output would be simply a waste of disk space and processing time when doing an insert or delete operation, and thus should be avoided. Also given the nature of a binary search, the cardinality or uniqueness of the data is important. Indexing on a field with a cardinality of 2 would split the data in half, whereas a cardinality of 1,000 would return approximately 1,000 records. With such a low cardinality the effectiveness is reduced to a linear sort, and the query optimizer will avoid using the index if the cardinality is less than 30% of the record number, effectively making the index a waste of space.


In French, but you should be able to find the download link :)
PHP Naive Bayesian Filter

The following is SQL92 standard so should be supported by the majority of RDMBS that use SQL:

CREATE INDEX [index name] ON [table name] ( [column name] )

A dictionary may also contain overhead, depending on the actual implementation. A hashtable usually contain some prime number of available nodes to begin with, even though you might only use a couple of the nodes.

Judging by your example, "Property", would you be better of with a class approach for the final level and real properties? Or is the names of the properties changing a lot from node to node?

I'd say that what "efficient" means depends on a lot of things, like:


speed of updates (insert, update, delete)
speed of random access retrieval
speed of sequential retrieval
memory used


I think that you'll find that a data structure that is speedy will generally consume more memory than one that is slow. This isn't always the case, but most data structures seems to follow this.

A dictionary might be easy to use, and give you relatively uniformly fast access, it will most likely use more memory than, as you suggest, lists. Lists, however, generally tend to contain more overhead when you insert data into it, unless they preallocate X nodes, in which they will again use more memory.

My suggestion, in general, would be to just use the method that seems the most natural to you, and then do a "stress test" of the system, adding a substantial amount of data to it and see if it becomes a problem.

You might also consider adding a layer of abstraction to your system, so that you don't have to change the programming interface if you later on need to change the internal data structure.

The trick to that is to use URL rewriting so that name.domain.com transparently maps to something like domain.com/users/name on your server.  Once you start down that path, it's fairly trivial to implement.

Well, you didn't specify Rails, so I'm going to throw Shoes out there. First, building shoes apps is probably the best way to learn Ruby (Rails is great, but I find mastering Ruby far more fun/useful). Secondly, while I certainly don't think building crossplatform UI components is trivial, shoes is relatively new, and relatively small. There are no doubt countless additions that could be made.


Don't worry about DNS and URL rewriting

Your DNS record will be static, something like:

*.YOURDOMAIN.COM A 123.123.123.123

Ask your DNS provider to do it for you (if it's not done already) or do it by yourself if you have control over your DNS records. This will automatically point all your subdomains (current and future ones) into the same HTTP server.

Once it's done, you will only need to parse HOST header on every single http request to detect what hostname was used to access your server-side scripts on your http server.

Assuming you're using ASP.NET, this is kind of silly example I came up with but works and demonstrates simplicity of this approach:

&lt;%@ Language="C#" %&gt;&lt;%string subDomain = Request.Url.Host.Split('.')[0].ToUpper();if (subDomain == "CLIENTXXX") Response.Write("Hello CLIENTXXX, your secret number is 33");else if (subDomain == "CLIENTYYY") Response.Write("Hello CLIENTYYY, your secret number is 44");else Response.Write(subDomain+" doesn't exist");%&gt;

Making a class-based structure would probably have more overhead than the dict-based structure, since in python classes actually use dicts when they are implemented.

C and C++ compilers will generate a warning when you compare signed and unsigned types; in your example code, you couldn't make your loop variable unsigned and have the compiler generate code without warnings (assuming said warnings were turned on).

Naturally, you're compiling with warnings turned all the way up, right?

And, have you considered compiling with "treat warnings as errors" to take it that one step further?

The downside with using signed numbers is that there's a temptation to overload them so that, for example, the values 0-&gt;n are the menu selection, and -1 means nothing's selected - rather than creating a class that has two variables, one to indicate if something is selected and another to store what that selection is.  Before you know it, you're testing for negative one all over the place and the compiler is complaining about how you're wanting to compare the menu selection against the number of menu selections you have - but that's dangerous because they're different types.  So don't do that.

The way we do this is to have a 'catch all' for our domain name registered in DNS so that anything.ourdomain.com will point to our server.

With Apache you can set up a similar catch-all for your vhosts.  The ServerName must be a single static name but the ServerAlias directive can contain a pattern.

Servername www.ourdomain.com
ServerAlias *.ourdomain.com


Now all of the domains will trigger the vhost for our project.  The final part is to decode the domain name actually used so that you can work out the username in your code, something like (PHP):

list( $username ) = explode( ".", $_SERVER[ "HTTP_HOST" ] );


or a RewriteRule as already suggested that silently maps user.ourdomain.com/foo/bar to www.ourdomain.com/foo/bar?user=user or whatever you prefer.


When I needed to write an application with a client-server model where the clients could leave and enter whenever they want, (I assume that's also the case for your application as you use mobile devices) I made sure that the clients send an online message to the server, indicating they were connected and ready to do whatever they needed doing.

at that time the server could send messages back to the client trough the same open connection.

Also, but I don't know if that is applicable for you, I had some sort of heartbeat the clients sent to the server, letting it know it was still online. That way the server knows when a client was forcibly disconnected from the network and it could mark that client back as offline.

Why wouldn't you just use an array with a size of 2?  A Queue is supposed to be able to dynamically grow and shrink.

Or create a wrapper class around an instance of Queue&lt;T&gt; instance and each time one enqueues a &lt;T&gt; object, check the size of the queue.  If larger than 2, dequeue the first item.


You should create your own class, a ringbuffer would probably fit your needs.

The data structures in .NET that allows you to specify capacity, except for array, uses this to build the internal data structure used to hold the internal data.

For instance, for a list, capacity is used to size an internal array. When you start adding elements to the list, it'll start filling this array from index 0 and up, and when it reaches your capacity, it increases the capacity to a new higher capacity, and continues filling it up.

I've knocked up a basic version of what I'm looking for, it's not perfect but it'll do the job until something better comes along.

public class LimitedQueue&lt;T&gt; : Queue&lt;T&gt;{    private int limit = -1;    public int Limit    {        get { return limit; }        set { limit = value; }    }    public LimitedQueue(int limit)        : base(limit)    {        this.Limit = limit;    }    public new void Enqueue(T item)    {        if (this.Count &gt;= this.Limit)        {            this.Dequeue();        }        base.Enqueue(item);    }}

You might be able to get some implementation ideas from the Ruby/ProgressBar library, which generates text progress bars. I stumbled across it a couple of months back but haven't made any use of it.

In my opinion it is more a question of personal preference.  nAnt is a great framework and MSBuild is almost as capable.  With the ability to easily develop custom tasks (in both frameworks) you can accomplish almost anything that you need to do.

I cannot answer the "still supported" portion of your questions, but I would say if you are already comfortable with nAnt then it's probably viable.  If you (or someone in your group) is familiar with MSBuild then that is a fine way to go as well.

Honestly it depends on what fits in to your environment better.  If you are using a lot of Non-Microsoft tools, nunit, ccnet, ncover.  You will probably find better support with nant.  Alternatively if you are using MSTest, TFSBuild, you will probably find MSBuild a better environment.  I would learn both and use which every fits more smoothly with your environment.

If you've already got a bunch of custom tasks you use with nAnt, stick with it - you don't gain much with MSBuild.  That said, there doesn't seem to be anything that nAnt can do that MSBuild can't at its core.  Both can call external tools, both can run .Net-based custom tasks, and both have a bunch of community tasks out there.

We're using MSBuild here for the same reason you are - it's the default build system for VS now, and we didn't have any nAnt-specific stuff to worry about.

The MSBuildCommunityTasks are a good third-party task base to start with, and covers most of the custom stuff I ever did in nAnt, including VSS and Subversion support.

On windows, curses works out of the box, ncurses doesn't, and for a progress bar curses should be sufficient. So, use curses instead of ncurses.

Also, both curses and ncurses are wafer-thin wrappers around the c library - that means you don't really need Ruby-specific tutorials.

However, on the site for the PickAxe you can download all the code examples for the book. The file "ex1423.rb" contains a curses demo which plays Pong - that should give you plenty of material to get you going.


Personally I think curses is overkill in this case.  While the curses lib is nice (and I frequently use it myself) it's a PITA to relearn every time I haven't needed it for 12 months which has to be the sign of a bad interface design.

If for some reason you can't get on with the progress bar lib Joey suggested roll your own and release it under a pretty free licence for instant kudos :)


Here is a solution that works in XP / Vista, but is definitely expandable to OSX, linux, I'd still be interested in another way.

public static function GetCurrentOSUser():String{
    // XP &amp; Vista only.
    var userDirectory:String = File.userDirectory.resolvePath("").nativePath;
    var startIndex:Number = userDirectory.lastIndexOf("\\") + 1
    var stopIndex:Number = userDirectory.length;
    var user = userDirectory.substring(startIndex, stopIndex);

    return user;
}



Not sure how credible this source is, but:


  The Windows Server 2008 Core edition can:
  
  
  Run the file server role.
  Run the Hyper-V virtualization server role.
  Run the Directory Services role.
  Run the DHCP server role.
  Run the IIS Web server role.
  Run the DNS server role.
  Run Active Directory Lightweight Directory Services.
  Run the print server role.
  
  
  The Windows Server 2008 Core edition cannot:
  
  
  Run a SQL Server.
  Run an Exchange Server.
  Run Internet Explorer.
  Run Windows Explorer.
  Host a remote desktop session.
  Run MMC snap-in consoles locally.
  



Also I would try:

File.userDirectory.name

But I don't have Air installed so I can't really test this...

There are some parallel extensions to .NET that are currently in testing and available at Microsoft's Parallel Computing Developer Center. They have a few interesting items that you would expect like Parallel foreach and a parallel version of LINQ called PLINQ. Some of the best information about the extensions is on Channel 9.

When you're using .Net 2.0 and Ajax - you should use:

ScriptManager.RegisterClientScriptBlock


It will work better in Ajax environments then the old Page.ClientScript version


You can set Apache to serve pages from anywhere with any restrictions but it's normally distributed in a more secure form.

Editing your apache files (http.conf is one of the more common names) will allow you to set any folder so it appears in your webroot.

EDIT:

alias myapp c:\myapp\

I've edited my answer to include the format for creating an alias in the http.conf file which is sort of like a shortcut in windows or a symlink under un*x where Apache 'pretends' a folder is in the webroot.  This is probably going to be more useful to you in the long term.

You can relocate it by editing the DocumentRoot setting in XAMPP\apache\conf\httpd.conf.

It should currently be:


  C:/xampp/htdocs


Change it to:


  C:/projects/transitCalculator/trunk


Try changing the argument names to "sender" and "args".  And, after you have it working, switch the call over to ScriptManager.RegisterClientScriptBlock, regardless of AJAX use.

Ok, per pix0r's, Sparks' and Dave's answers it looks like there are three ways to do this:



Virtual Hosts


Open C:\xampp\apache\conf\extra\httpd-vhosts.conf.
Un-comment line 19 (NameVirtualHost *:80).
Add your virtual host (~line 36):

&lt;VirtualHost *:80&gt;
    DocumentRoot C:\Projects\transitCalculator\trunk
    ServerName transitcalculator.localhost
    &lt;Directory C:\Projects\transitCalculator\trunk&gt;
        Order allow,deny
        Allow from all
    &lt;/Directory&gt;
&lt;/VirtualHost&gt;

Open your hosts file (C:\Windows\System32\drivers\etc\hosts).
Add

127.0.0.1 transitcalculator.localhost #transitCalculator


to the end of the file (before the Spybot - Search &amp; Destroy stuff if you have that installed).
Save (You might have to save it to the desktop, change the permissions on the old hosts file (right click > properties), and copy the new one into the directory over the old one (or rename the old one) if you are using Vista and have trouble).
Restart Apache.


Now you can access that directory by browsing to http://transitcalculator.localhost/.



Make an Alias


Starting ~line 200 of your http.conf file, copy everything between &lt;Directory "C:/xampp/htdocs"&gt; and &lt;/Directory&gt; (~line 232) and paste it immediately below with C:/xampp/htdocs replaced with your desired directory (in this case C:/Projects) to give your server the correct permissions for the new directory.
Find the &lt;IfModule alias_module&gt;&lt;/IfModule&gt; section (~line 300) and add

Alias /transitCalculator "C:/Projects/transitCalculator/trunk"


(or whatever is relevant to your desires) below the Alias comment block, inside the module tags.




Change your document root


Edit ~line 176 in C:\xampp\apache\conf\httpd.conf; change DocumentRoot "C:/xampp/htdocs" to #DocumentRoot "C:/Projects" (or whatever you want).
Edit ~line 203 to match your new location (in this case C:/Projects).




Notes: 


You have to use forward slashes "/" instead of back slashes "\".
Don't include the trailing "/" at the end.
restart your server.



I think if you are attempting to learn a new language do something fun in that language.

I learned python by writing lots of web spiders and little toys and for ruby I'd take exactly the same path.  Instead of finding a project that needs input do a couple of little personal projects to get a feel for the language.  You learn more by doing something then by reading lots of examples and other peoples code in those first few months.

A language like Ruby is structured in such a way you can do something productive straight out of the box without much support so jump right in and do something fun rather than think you have to do something for somebody else right at the beginning of your experimentation with a new language.

have fun with Ruby, it's on my short list of things to play with :)

I think we could also include non-.NET-specific approaches to parallel processing if those are among the best options to consider.

Or you could start throwing critical exceptions. Surely, an access violation exception will catch your users' attention.

@Larsenal

If you want to branch outside of .NET there has been a lot of discussion about Intel's Threading Building Blocks which is a parallel library for C++.

Outside the scope of your question so I debated not posting this but in Java there are actually 2 types of exceptions, checked and unchecked. The basic difference is that, much like in c[++], you dont have to catch an unchecked exception.

For a good reference try this 


This isn't much better:

public function createShipment($startZip, $endZip, $weight=null){    $weight = !$weight ? $this-&gt;getDefaultWeight() : $weight;}// or...public function createShipment($startZip, $endZip, $weight=null){    if ( !$weight )        $weight = $this-&gt;getDefaultWeight();}

The way I typically go about coding permission systems is having 6 tables.


Users - this is pretty straight forward it is your typical users table
Groups - this would be synonymous to your departments
Roles - this is a table with all permissions generally also including a human readable name and a description
Users_have_Groups - this is a many-to-many table of what groups a user belongs to
Users_have_Roles - another many-to-many table of what roles are assigned to an individual user
Groups_have_Roles - the final many-to-many table of what roles each group has


At the beginning of a users session you would run some logic that pulls out every role they have assigned, either directory or through a group. Then you code against those roles as your security permissions.

Like I said this is what I typically do but your millage may vary.

An approach I've used in various applications is to have a generic PermissionToken class which has a changeable Value property.  Then you query the requested application, it tells you which PermissionTokens are needed in order to use it.

For example, the Shipping application might tell you it needs:

new PermissionToken(){    Target = PermissionTokenTarget.Application,    Action = PermissionTokenAction.View,    Value = "ShippingApp"};

This can obviously be extended to Create, Edit, Delete etc and, because of the custom Value property, any application, module or widget can define its own required permissions.  YMMV, but this has always been an efficient method for me which I have found to scale well.


  Is there a way one can ensure that the
  exceptions thrown are always caught
  using try/catch by the calling
  function?


I find it rather funny, that the Java crowd - including myself - is trying to avoid checked Exceptions. They are trying to work their way around being forced to catch Exceptions by using RuntimeExceptions.

I agree with John Downey.

Personally, I sometimes use a flagged enumeration of permissions. This way you can use AND, OR, NOT and XOR bitwise operations on the enumeration's items.

"[Flags]public enum Permission{    VIEWUSERS = 1, // 2^0 // 0000 0001    EDITUSERS = 2, // 2^1 // 0000 0010    VIEWPRODUCTS = 4, // 2^2 // 0000 0100    EDITPRODUCTS = 8, // 2^3 // 0000 1000    VIEWCLIENTS = 16, // 2^4 // 0001 0000    EDITCLIENTS = 32, // 2^5 // 0010 0000    DELETECLIENTS = 64, // 2^6 // 0100 0000}"

Then, you can combine several permissions using the AND bitwise operator. 
For example, if a user can view &amp; edit users, the binary result of the operation is 0000 0011 which converted to decimal is 3. 
You can then store the permission of one user into a single column of your DataBase (in our case it would be 3).
Inside your application, you just need another bitwise operation (OR) to verify if a user has a particular permission or not. 

As far as I can tell Python, up through 2.5, only supports hexadecimal &amp; octal literals.  I did find some discussions about adding binary to future versions but nothing definite.

I am pretty sure this is one of the things due to change in Python 3.0 with perhaps bin() to go with hex() and oct().

EDIT:
lbrandy's answer is correct in all cases.


&gt;&gt;&gt; print int('01010101111',2)
687
&gt;&gt;&gt; print int('11111111',2)
255


Another way.


In addition to John Downey and jdecuyper's solutions, I've also added an "Explicit Deny" bit at the end/beginning of the bitfield, so that you can perform additive permissions by group, role membership, and then subtract permissions based upon explicit deny entries, much like NTFS works, permission-wise.

I have to agree with the OP 'wrong' dates really jar with my DD/MM/YYYY upbringing and I find ISO 8601 dates and times extremely easy to work with.  For once the standard got it right and engtech has the obvious answer that doesn't require localisation.

I was going to report the birthday input form on stack overflow as a bug because of how much of a sore thumb it is to the majority of the world.

From the Apple Developer Connection Safari Web Content Guide for iPhone page Specifying a Webpage Icon for Web Clip...


  The user can add a web application or
  webpage link to the Home screen. These
  links, represented by an icon, are
  called web clips. Follow these simple
  steps to specify an icon to represent
  your web application or webpage on
  iPhone.
  
  To specify an icon for the entire
  website (every page on the website),
  place an icon file in PNG format in
  the root document folder called
  apple-touch-icon.png.
  
  To specify an icon for a single
  webpage, or replace the website icon
  with a webpage-specific icon, add a
  link element to the webpage as in:
  
  &lt;link rel="apple-touch-icon" href="/custom_icon.png"/&gt;
  
  In the above example, replace
  custom_icon.png with your icon
  filename.
  
  See "Create an Icon for Your Web
  Application or Webpage" in iPhone
  Human Interface Guidelines in iPhone
  Human Interface Guidelines for webpage
  icon metrics.
  
  Note: The web clip feature is
  available in iPhone 1.1.3 and later.


And for the sake of completeness, a link to Scott Hanselman's posting, which contains some additional tips as well:

Add Home Screen iPhone Icons and Adjust the ViewPort

You should be able to check out the current version of the code and then create a git repository around it. Updating that and committing it to your local git repository should be painless. As should cloning it.

The only catch is that you need to have them both ignore each other (I've done something similar with SVN) by messing with the appropriate ignore files. I'm presuming SourceSafe let's you ignore things. And you'll need to do certain operations twice (like telling both that you are deleting a file).

I've made a suggestion that Stack Overflow implement an apple-touch-icon:

Add an apple-touch-icon for Safari on iPhone

A short form is convenient and helps avoid spelling mistakes. Localize as applicable, but be sure to display the expected format (do not leave the user blind). Provide a date-picker control as an optional aide to filling in the field.

As an extra, on-the-fly parsing and display of the date in long form might help too.

Well... KernelTrap has something on this. Looks like you can use vss2svn to pipe the Source Safe repo into a Subversion repository, then use the very nice git-svn to pull into a local git repo.
I would assume the commits back to VSS would not be a smooth, automatic process using this method.

While I would still like an answer to why my JS wasn't being recognized, the solution I found in the meantime (and should have done in the first place) is to use an Asp:CompareValidator instead of an Asp:CustomValidator.

While you cannot prevent usage of those inherited members to my knowledge, you should be able to hide them from IntelliSense using the EditorBrowsableAttribute:

Using System.ComponentModel;

[EditorBrowsable(EditorBrowsableState.Never)]
private string MyHiddenString = "Muahahahahahahahaha";


Edit: Just saw this in the documentation comments, which makes it kinda useless for this purpose:


  There is a prominent note that states that this attribute "does not suppress members from a class in the same assembly". That is true but not complete. Actually, the attribute does not suppress members from a class in the same solution.



I think you're best least hackish way is to consider composition as opposed to inheritance.

Or, you could create an interface that has the members you want, have your derived class implement that interface, and program against the interface.

One potential thing you can do is contain the object rather than extend from the other class.  This will give you the most flexibility in terms of exposing what you want to expose, but if you absolutely need the object to be of that type it is not the ideal solution (however you could expose the object from a getter).

Thus:

public class MyClass : BaseClass{    // Your stuff here}

Becomes:

public class MyClass{    private BaseClass baseClass;    public void ExposeThisMethod()    {        baseClass.ExposeThisMethod();    }}

Or:

public class MyClass{    private BaseClass baseClass;    public BaseClass BaseClass    {        get        {            return baseClass;        }    }}

Some time ago I wrote a quick and dirty exe that would update the version #'s in an assemblyinfo.{cs/vb} - I also have used rxfind.exe (a simple and powerful regex-based search replace tool) to do the update from a command line as part of the build process.  A couple of other helpfule hints:


separate the assemblyinfo into product parts (company name, version, etc.) and assembly specific parts (assembly name etc.).  See here
Also - i use subversion, so I found it helpful to set the build number to subversion revision number thereby making it really easy to always get back to the codebase that generated the assembly (e.g. 1.4.100.1502 was built from revision 1502).


Just wondering, what do you feel your method leaves to be desired?  You could replace the anonymous delegate with a.. named? delegate, something like

    public delegate void IoOperation(params string[] parameters);    public void FileDeleteOperation(params string[] fileName)    {        File.Delete(fileName[0]);    }    public void FileCopyOperation(params string[] fileNames)    {        File.Copy(fileNames[0], fileNames[1]);    }    public void RetryFileIO(IoOperation operation, params string[] parameters)    {        RetryTimer fileIORetryTimer = new RetryTimer(TimeSpan.FromHours(10));        bool success = false;        while (!success)        {            try            {                operation(parameters);                success = true;            }            catch (IOException e)            {                if (fileIORetryTimer.HasExceededRetryTimeout)                {                    throw;                }                fileIORetryTimer.SleepUntilNextRetry();            }        }    }    public void Foo()    {        this.RetryFileIO(FileDeleteOperation, "L:\file.to.delete" );        this.RetryFileIO(FileCopyOperation, "L:\file.to.copy.source", "L:\file.to.copy.destination" );    }

Override them like Michael Suggests above and to prevent folks from using the overridden (sp?) methods, mark them as obsolete:

[Obsolete("These are not supported in this class.", true)]public override  void dontcallmeanymore(){}

If the second parm is set to true, a compiler error will be generated if anyone tries to  call that method and the string in the first parm is the message.  If parm2 is false only a compiler warning will be generated.

I assume you don't mean cygwin, right?

How about powershell, then?


Are you asking about Linux shell as in an environment to work in? For that CygWin I think has been around the longest and is pretty robust: http://www.cygwin.com/

A while ago I found a windows port of all the popular linux commands I use (ls, grep, diff) and I simply unzip those to a file, add it to my PATH environment and then can run from there: http://unxutils.sourceforge.net/

Or are you talking about executing shell commands from within your code? If you're in the .net sphere, there is the Process.Start() method that will give you a lot of options.

Hope this helps!

If you're referring to simply accessing your IIS server from a remote location, remote desktop generally solves that problem.  Assuming your server has a static IP address or a host name you can access from the internet, remote desktop is a simple and relatively secure solution.

Is there a problem with this answer?  Now I have negative reputation...

The best way I can think of would be to use Cygwin over an OpenSSH connection.
Here's a document that explains how to do just that: 

http://www.ucl.ac.uk/cert/openssh_rdp_vnc.pdf

It is possible to skip the step of creating the empty database. You can create the new database as part of the restore process.

This is actually the easiest and best way I know of to clone a database. You can eliminate errors by scripting the backup and restore process rather than running it through the SQL Server Management Studio

There are two other options you could explore:


Detach the database, copy the .mdf file and re-attach.
Use SQL Server Integration Services (SSIS) to copy all the objects over


I suggest sticking with backup and restore and automating if necessary.

MySQLdb is what I have used before.

If you host is using Python version 2.5 or higher, support for sqlite3 databases is built in (sqlite allows you to have a relational database that is simply a file in your filesystem).  But buyer beware, sqlite is not suited for production, so it may depend what you are trying to do with it.

Another option may be to call your host and complain, or change hosts.  Honestly these days, any self respecting web host that supports python and mysql ought to have MySQLdb pre installed.

I usually do a project for the GUI a project for the business logic a project for data access and a project for unit tests.

But sometimes it is prudent to have separation based upon services (if you are using a service oriented architecture) Such as Authentication, Sales, etc. 

I guess the rule of thumb that I work off of is that if you can see it as a component that has a clear separation of concerns then a different project could be prudent. But I would think that folders versus projects could just be a preference or philosophy.

I personally feel that if reusable code is split into projects it is simpler to use other places than if it is just in folders.

Here's a dynamic sql script I've used in the past.  It can be further modified but it will give you the basics.  I prefer scripting it to avoid the mistakes you can make using the Management Studio:

Declare @OldDB varchar(100)Declare @NewDB varchar(100)Declare @vchBackupPath varchar(255)Declare @query varchar(8000)/*Test code to implement Select @OldDB = 'Pubs'Select @NewDB = 'Pubs2'Select @vchBackupPath = '\\dbserver\C$\Program Files\Microsoft SQL Server\MSSQL.1\MSSQL\Backup\pubs.bak'*/SET NOCOUNT ON;Select @query = 'Create Database ' + @NewDBexec(@query)Select @query = 'Declare @vBAKPath varchar(256)declare @oldMDFName varchar(100)declare @oldLDFName varchar(100)declare @newMDFPath varchar(100)declare @newLDFPath varchar(100)declare @restQuery varchar(800)select @vBAKPath = ''' + @vchBackupPath + '''select @oldLDFName = name from ' + @OldDB +'.dbo.sysfiles where filename like ''%.ldf%''select @oldMDFName = name from  ' + @OldDB +'.dbo.sysfiles where filename like ''%.mdf%''select @newMDFPath = physical_name from ' + @NewDB +'.sys.database_files where type_desc = ''ROWS''select @newLDFPath = physical_name from ' + @NewDB +'.sys.database_files where type_desc = ''LOG''select @restQuery = ''RESTORE DATABASE ' + @NewDB + ' FROM DISK = N'' + '''''''' + @vBAKpath + '''''''' + '' WITH MOVE N'' + '''''''' + @oldMDFName + '''''''' +  '' TO N'' + '''''''' + @newMDFPath + '''''''' +  '', MOVE N'' + '''''''' + @oldLDFName + '''''''' +  '' TO N'' + '''''''' + @newLDFPath + '''''''' +  '', NOUNLOAD, REPLACE, STATS = 10''exec(@restQuery)--print @restQuery'exec(@query)

denny wrote:


  I personally feel that if reusable code is split into projects it is simpler to use other places than if it is just in folders.


I really agree with this - if you can reuse it, it should be in a separate project.  With that said, it's also very difficult to reuse effectively :)

Here at SO, we've tried to be very simple with three projects:


MVC Web project (which does a nice job of separating your layers into folders by default)
Database project for source control of our DB
Unit tests against MVC models/controllers


I can't speak for everyone, but I'm happy with how simple we've kept it - really speeds the builds along!

By default, always just create new folder within the same project


You will get single assembly (without additional ILMerge gymnastic)
Easier to obfuscate (because you will have less public types and methods, ideally none at all)


Separating your source code into multiple projects makes only sense if you...


Have some portions of the source code that are part of the project but not deployable by default or at all (unit tests, extra plugins etc.)
More developers involved and you want to treat their work as consumable black box. (not very recommended)
If you can clearly separate your project into isolated layers/modules and you want to make sure that they can't cross-consume internal members. (also not recommended because you will need to decide which aspect is the most important)


If you think that some portions of your source code could be reusable, still don't create it as a new project. Just wait until you will really want to reuse it in another solution and isolate it out of original project as needed. Programming is not a lego, reusing is usually very difficult and often won't happen as planned.

If you are quite happy with MSBuild, then I would stick with MSBuild.  This may be one of those cases where the tool you learn first is the one you will prefer.  I started with NAnt and can't quite get used to MSBuild.  I'm sure they will both be around for quite some time.

There are some fundamental differences between the two, probably best highlighted by this conversation between some NAnt fans and a Microsoftie.

Interestingly, Jeremy Miller asked the exact opposite question on his blog last year.  

I'm not necessarily looking for the fastest but a nice balance, some of the server that this code is being developed for are fairly slow, the script that hashes and stores the password is taking 5-6 seconds to run, and I've narrowed it down to the hashing (if I comment the hashing out it runs, in 1-2 seconds).

It doesn't have to be the MOST secure, I'm not codding for a bank (right now) but I certainly WILL NOT store the passwords as plain-text.

In my experience its best to keep an the info in the profile to a bare minimum, only put the essentials in there that are directly needed for authentication. Other information such as addresses should be saved in your own database by your own application logic, this approach is more extensible and maintainable.

I think that depends on how many fields you need. To my knowledge, Profiles are essentially a long string that gets split at the given field sizes, which means that they do not scale very well if you have many fields and users.

On the other hand, they are built in, so it's an easy and standardized way, which means there is not a big learning curve and you can use it in future apps as well without needing to tweak it to a new table structure.

Rolling your own thing allows you to put it in a properly normalized database, which drastically improves performance, but you have to write pretty much all the profile managing code yourself.

Edit: Also, Profiles are not cached, so every access to a profile goes to the database first (it's then cached for that request, but the next request will get it from the database again)

If you're thinking about writing your own thing, maybe a custom Profile Provider gives you the best of both worlds - seamless integration, yet the custom stuff you want to do.

The active projects on Rubyforge are a great place to start.  What would be a good starter project is to pick one that is pretty popular but not a lot of developers.

If you are interested in Ruby on Rails, I'm working on Redmine right now.  It's been one of the most active projects and only has 5 developers.  Open Source Rails also has a good collection of projects.

I've found doing a Refactotum a great way to get started on a project.  Use the fact that you are new to your advantage, most people who have been on a project forget about simple things like gem dependencies and documentation


This website might help you out a bit more.  Also this one.

I'm working from a fairly rusty memory of a statistics course, but here goes nothing:

When you're doing analysis of variance (ANOVA), you actually calculate the F statistic as the ratio from the mean-square variances "between the groups" and the mean-square variances "within the groups".  The second link above seems pretty good for this calculation.

This makes the F statistic measure exactly how powerful your model is, because the "between the groups" variance is explanatory power, and "within the groups" variance is random error.  High F implies a highly significant model.

As in many statistical operations, you back-determine Sig. using the F statistic.  Here's where your Wikipedia information comes in slightly handy.  What you want to do is - using the degrees of freedom given to you by SPSS - find the proper P value at which an F table will give you the F statistic you calculated.  The P value where this happens [F(table) = F(calculated)] is the significance.

Conceptually, a lower significance value shows a very strong ability to reject the null hypothesis (which for these purposes means to determine your model has explanatory power).

Sorry to any math folks if any of this is wrong.  I'll be checking back to make edits!!!

Good luck to you.  Stats is fun, just maybe not this part.  =)

You need to put the match in parentheses, like this:

preg_replace("/([A-Z])/", "&lt;span class=\"initial\"&gt;$1&lt;/span&gt;", $str)


The value of 'false' is true. You need to remove the quotes:

$canMerge = ($condition1 &amp;&amp; $condition2) ? true : false;

This is broken because 'false' as a string will evaluate to true as a boolean.

However, this is an unneeded ternary expression, because the resulting values are simple true and false.  This would be equivalent:

$canMerge = ($condition1 &amp;&amp; $condition2);

The exact definitions of "vsize," "rss," "rprvt," "rshrd," and other obscure-looking abbreviations vary from OS to OS.  The manual pages for the "top" and "ps" commands will have some sort of description, but all such descriptions are simplified greatly (or are based on long-extinct kernel implementations).  "Process size" as a concept is fiendishly difficult to pin down in the general case.  Answers in specific instances depend heavily on the actual memory management implementation in the OS, and are rarely as satisfying as the tidy "process size" concept that exists in the minds of most users (and most developers).

For example, none of those numbers (nor, likely, any combination of them) can be used to tell you exactly how many such processes can run at once in a given amount of free memory.  But really, your best bet is to come at it from that end: why do you want this number, and what will you use it for?  Given that information, I think you'll get more useful answers.

The Publish to Provider functionality has worked great for me.  See Scott Gu's Blog Entry.

If you need something really robust look  at redgate software's tools here...if you are doing much SQL at all, these are worth the $$.

If you are looking for user facing interaction, stick with xml. It has more support, understanding, and general acceptance currently. If it's internal, I would say that protocol buffers are a great idea.
Maybe in a few years as more tools come out to support protocol buffers, then start looking towards that for a public facing api. Until then... JSON?

Backup and Restore is the most straight-forward way I know.  You have to be careful between servers as security credentials don't come with the restored database.

Sounds like you might like Git. There's a Google Talk explaining all about it.

From the preg_replace documentation on php.net:


  replacement  may contain references of
  the form \n or (since PHP 4.0.4) $n,
  with the latter form being the
  preferred one. Every such reference
  will be replaced by the text captured
  by the n'th parenthesized pattern.


See Flubba's example.

Its probably not exactly what your looking for, but you may be able to implement OS level clustering.

On Linux:


set raw mode
select and read the keystroke
restore normal settings



import sys
import select
import termios
import tty

def getkey():
    old_settings = termios.tcgetattr(sys.stdin)
    tty.setraw(sys.stdin.fileno())
    select.select([sys.stdin], [], [], 0)
    answer = sys.stdin.read(1)
    termios.tcsetattr(sys.stdin, termios.TCSADRAIN, old_settings)
    return answer

print """Menu
1) Say Foo
2) Say Bar"""

answer=getkey()

if "1" in answer: print "foo"
elif "2" in answer: print "bar"




for C# 3.5

without notifyicon therefore you will need to terminate this application in task manager manually

using System;using System.Drawing;using System.Windows.Forms;static class Program{    static void Main()    {        Timer timer = new Timer();        // timer.Interval = 4 minutes        timer.Interval = (int)(TimeSpan.TicksPerMinute * 4 / TimeSpan.TicksPerMillisecond);        timer.Tick += (sender, args) =&gt; { Cursor.Position = new Point(Cursor.Position.X + 1, Cursor.Position.Y + 1); };        timer.Start();        Application.Run();    }}

Something like this should work (though, you will want to change the interval).

public Form1()
{
    InitializeComponent();
    Timer Every4Minutes = new Timer();
    Every4Minutes.Interval = 10;
    Every4Minutes.Tick += new EventHandler(MoveNow);
    Every4Minutes.Start();
}

void MoveNow(object sender, EventArgs e)
{
    Cursor.Position = new Point(Cursor.Position.X - 1, Cursor.Position.Y - 1);
}



On Windows:

import msvcrt
answer=msvcrt.getch()



This is an active area of research. There are literally oodles of academic papers on the subject. It's going to be difficult to give you assistance especially w/o more deatails. Are you looking for specific types of text? Fonts? English-only? Are you familiar with the academic literature? 

"Text detection" is a standard problem in any OCR (optical character recognition) system and consequently there are lots of bits of code on the interwebs that deal with it. 

I could start listing piles of links from google but I suggest you just do a search for "text detection" and start reading :). There is ample example code available as well.

Wow, that took forever. Ok, here's what I've ended up with 

#!C:\python25\python.exeimport msvcrtprint """Menu1) Say Foo 2) Say Bar"""while 1:    char = msvcrt.getch()    if char == chr(27): #escape        break    if char == "1":        print "foo"        break    if char == "2":        print "Bar"        break

It fails hard using IDLE, the python...thing...that comes with python. But once I tried it in DOS (er, CMD.exe), as a real program, then it ran fine.

No one try it in IDLE, unless you have Task Manager handy.

I've already forgotten how I lived with menus that arn't super-instant responsive.


  Without the extra quotes around the input string parameter, the Javascript function thinks I'm passing in an integer.


Can you do some rudimentary string function to force JavaScript into changing it into a string? Like 

value = value + ""

Try putting the extra text inside the server-side script block and concatenating.

onclick='&lt;%# "ToggleDisplay(""" &amp;  DataBinder.Eval(Container.DataItem, "JobCode") &amp; """);" %&gt;'

Edit: I'm pretty sure you could just use double quotes outside the script block as well.

The  reason msvcrt fails in IDLE is because IDLE is not accessing the library that runs msvcrt. Whereas when you run the program natively in cmd.exe it works nicely. For the same reason that your program blows up on Mac and Linux terminals.

But I guess if you're going to be using this specifically for windows, more power to ya.

I had recently similar problem and the only way to solve it was to use plain old HTML codes for single (&amp;#39;) and double quotes (&amp;#34;).  

Source code was total mess of course but it worked.

Try

&lt;a id="aShowHide" onclick='ToggleDisplay(&amp;#34;&lt;%# DataBinder.Eval(Container.DataItem, "JobCode") %&gt;&amp;#34;);'&gt;Show/Hide&lt;/a&gt;


or

&lt;a id="aShowHide" onclick='ToggleDisplay(&amp;#39;&lt;%# DataBinder.Eval(Container.DataItem, "JobCode") %&gt;&amp;#39;);'&gt;Show/Hide&lt;/a&gt;



Chris' probably has the best pure answer to the question:

However, I'm curious about the root of the question.  If the user should always wrap the call in a try/catch block, should the user-called function really be throwing exceptions in the first place?

This is a difficult question to answer without more context regarding the code-base in question.  Shooting from the hip, I think the best answer here is to wrap the function up such that the recommended (if not only, depending on the overall exception style of the code) public interface does the try/catch for the user.  If you're just trying to ensure that there are no unhandled exceptions in your code, unit tests and code review are probably the best solution.


the problem is that the sphere can be distorted a number of ways, and having all those points known on the equator, lets say, wont help you map points further away.

You need better 'close' points, then you can assume these three points are on a plane with the fourth and do the interpolation --knowing that the distance of longitudes is a function, not a constant.

Ummm.  Maybe I am missing something about the question here, but if you have long/lat info, you also have the direction of north?

It seems you need to map geodesic coordinates to a projected coordinates system. For example osgb to wgs84.

The maths involved is non-trivial, but the code comes out a only a few lines. If I had more time I'd post more but I need a shower so I will be boring and link to the wikipedia entry which is pretty good.

Note: Post shower edited.

I'm not clear on whether or not you're wanting to add the asynchronous bits to the server in C# or the client in C++.

If you're talking about doing this in C++, desktop Windows platforms can do socket I/O asynchronously through the API's that use overlapped I/O.  For sockets, WSASend, WSARecv both allow async I/O (read the documentation on their LPOVERLAPPED parameters, which you can populate with events that get set when the I/O completes).

I don't know if Windows Mobile platforms support these functions, so you might have to do some additional digging. 

The solution is to use the TempData property to store the desired Request components.

For instance:

public ActionResult Send()
{
    TempData["form"] = Request.Form;
    return this.RedirectToAction(a =&gt; a.Form());
}


Then in your "Form" action you can go:

public ActionResult Form()
{
    /* Declare viewData etc. */

    if (TempData["form"] != null)
    {
        /* Cast TempData["form"] to 
        System.Collections.Specialized.NameValueCollection 
        and use it */
    }

    return View("Form", viewData);
}



Are there any more specific details on the kind of distortion?  If, for example, your latitudes and longitudes are "distorted" onto your 2D map using a Mercator projection, the conversion math is readily available.

If the map is distorted truly arbitrarily, there are lots of things you could try, but the simplest would probably be to compute a weighted average from your existing point mappings.  Your weights could be the squared inverse of the x/y distance from your new point to each of your existing points.

Some pseudocode:

estimate-latitude-longitude (x, y)

    numerator-latitude := 0
    numerator-longitude := 0
    denominator := 0

    for each point,
        deltaX := x - point.x
        deltaY := y - point.y
        distSq := deltaX * deltaX + deltaY * deltaY
        weight := 1 / distSq

        numerator-latitude += weight * point.latitude
        numerator-longitude += weight * point.longitude
        denominator += weight

    return (numerator-latitude / denominator, numerator-longitude / denominator)


This code will give a relatively simple approximation.  If you can be more precise about the way the projection distorts the geographical coordinates, you can probably do much better.

Here is some sample code. I think this is what you are looking for. The following displays exactly the same in Firefox 3 (mac) and IE7.



#absdiv {
  position: absolute; 
  left: 100px; 
  top: 100px; 
  width: 80%; 
  height: 60%; 
  background: #999;
}

#pctchild {
  width: 60%; 
  height: 40%; 
  background: #CCC;
}

#reldiv {
  position: relative;
  left: 20px;
  top: 20px;
  height: 25px;
  width: 40%;
  background: red;
}
&lt;div id="absdiv"&gt;
    &lt;div id="reldiv"&gt;&lt;/div&gt;
    &lt;div id="pctchild"&gt;&lt;/div&gt;
&lt;/div&gt;





recognizing text inside an image is indeed a hot topic for researchers in that field, but only begun to grow out of control when captcha's became the "norm" in terms of defense against spam bots. Why use captcha's as protection? well because it is/was very hard to locate (and read) text inside an image!

The reason why I mention captcha's is because the most advancement* is made within that tiny area, and I think that your solution could be best found there.
especially because captcha's are indeed about locating text (or something that resembles text) inside a cluttered image and afterwards trying to read the letters correctly.

so if you can find yourself a good open source captcha breaking tool you probably have all you need to continue your quest... 
You could probably even throw away the most dificult code that handles the character recognition itself, because those OCR's are used to read distorted text, something you don't have to do.

*: advancement in terms of visible, usable, and practical information for a "non-researcher"

I can't think of any cross platform way of what you want (or any guaranteed way full stop) but as you are using GetTickCount perhaps you aren't interested in cross platform :)

I'd use interprocess communications and set the intensive processes nice levels to get what you require but I'm not sure that's appropriate for your situation.

EDIT:
I agree with Bernard which is why I think a process rather than a thread might be more appropriate but it just might not suit your purposes.

If the data modification is not too time consuming (meaning, if the main purpose of the background thread is not the actual data modification), try moving the section that modifies the data to a delegate and Invoke'ing that delegate.

If the actual heavy work is on the data, you'll probably have to create a deep copy of this data to pass to the background thread, which will send the processed data back to the UI thread via Invoke again.

You should be able to do something like:

if (control.InvokeRequired){    control.Invoke(delegateWithMyCode);}else{    delegateWithMyCode();}

InvokeRequired is a property on Controls to see if you are on the correct thread, then Invoke will invoke the delegate on the correct thread.

UPDATE: Actually, at my last job we did something like this:

private void SomeEventHandler(Object someParam){    if (this.InvokeRequired)    {        this.Invoke(new SomeEventHandlerDelegate(SomeEventHandler), someParam);    }    // Regular handling code}

which removes the need for the else block and kind of tightens up the code.

Would use double.TryParse, it has performance benefits.


As I don't have a test case to go from I can't guarantee this solution, but it seems to me that a scenario similar to the one used to update progress bars in different threads (use a delegate) would be suitable here.

public delegate void DataBindDelegate();public DataBindDelegate BindData = new DataBindDelegate(DoDataBind);public void DoDataBind(){    DataBind();}

If the data binding needs to be done by a particular thread, then let that thread do the work!

I'd personally use int.tryparse, then double.tryparse.  Performance on those methods is quite fast.  They both return a Boolean.  If both fail then you have a string, per how you defined your data. 

On linux, you can change the scheduling priority of a thread with nice().

I would say, don't worry so much about such micro performance.  It is much better to just get something to work, and then make it as clear and concise and easy to read as possible.  The worst thing you can do is sacrifice readability for an insignificant amount of performance.

In the end, the best way to deal with performance issues is to save them for when you have data that indicates there is an actual performance problem... otherwise you will spend a lot of time micro-optimizing and actually cause higher maintenance costs for later on.

If you find this parsing situation is really the bottleneck in your application, THEN is the time to try and figure out what the fastest way to solve the problem is.  I think Jeff (and many others) have blogged about this sort of thing a lot.

Re: 2004

No, you will only move the code that changes the data into the delegate function (because the data change is what triggers the control update). Other than that, you should not have to write anything extra.


If the thread call is "illegal" (i.e. the DataBind call affects controls that were not created in the thread it is being called from) then you need to create a delegate so that even if the decision / preparation for the DataBind is not done in the control-creating thread, any resultant modification of them (i.e. DataBind()) will be.

You would call my code from the worker thread like so:

this.BindData.Invoke();

This would then cause the original thread to do the binding, which (presuming it is the thread that created the controls) should work.

This is more or less the perfect use case for SVK.  SVK is a command line front end for subversion that works with an entire local copy of the repository.  So your commits, updates, etc. work on the local repository and you can then sync with a master.  I would generally recommend SVK over plain subversion anyway as it makes a lot of things nicer.  No .svn folders, better branching and merging, better conflict resolution.


This looks like an excellent opportunity to have a look at Aspect Oriented Programming. Here is a good article on AOP in .NET. The general idea is that you'd extract the cross-functional concern (i.e. Retry for x hours) into a separate class and then you'd annotate any methods that need to modify their behaviour in that way. Here's how it might look (with a nice extension method on Int32)[RetryFor( 10.Hours() )]
public void DeleteArchive()
{
  //.. code to just delete the archive
}


In my experience with LINQ to SQL and LINQ to Entities a DataContext is synonymous to a connection to the database. So if you were to use multiple data stores you would need to use multiple DataContexts. My gut reaction is you wouldn't notice to much of a slow down with a DataContext that encompasses a large number of tables. If you did however you could always split the database logically at points where you can isolate tables that don't have any relationship to other sets of tables and create multiple contexts.

You can accomplish this with Section Handlers. There is a basic overview of how to write one at http://www.codeproject.com/KB/aspnet/ConfigSections.aspx however it refers to app.config which would be pretty much the same as writing one for use in web.config. This will allow you to essentially have your own XML tree in the config file and do some more advanced configuration.

Assuming you have the correct assemblies and a C# compiler you in theory can use whatever you want to edit the code and then just run the compiler by hand or using a build script. That being said it is a real pain doing .NET development without Visual Studio/SharpEdit/Monodevelop in my opinion.

I haven't done SWING development since my early CS classes but if it wasn't built in you could just inherit javax.swing.AbstractButton and create your own. Should be pretty simple to wire something together with their existing framework.

You could always try the Synth look &amp; feel. You provide an xml file that acts as a sort of stylesheet, along with any images you want to use. The code might look like this:

try {
    SynthLookAndFeel synth = new SynthLookAndFeel();
    Class aClass = MainFrame.class;
    InputStream stream = aClass.getResourceAsStream("\\default.xml");

    if (stream == null) {
        System.err.println("Missing configuration file");
        System.exit(-1);                
    }

    synth.load(stream, aClass);

    UIManager.setLookAndFeel(synth);
} catch (ParseException pe) {
    System.err.println("Bad configuration file");
    pe.printStackTrace();
    System.exit(-2);
} catch (UnsupportedLookAndFeelException ulfe) {
    System.err.println("Old JRE in use. Get a new one");
    System.exit(-3);
}


From there, go on and add your JButton like you normally would. The only change is that you use the setName(string) method to identify what the button should map to in the xml file.

The xml file might look like this:

&lt;synth&gt;
    &lt;style id="button"&gt;
        &lt;font name="DIALOG" size="12" style="BOLD"/&gt;
        &lt;state value="MOUSE_OVER"&gt;
            &lt;imagePainter method="buttonBackground" path="dirt.png" sourceInsets="2 2 2 2"/&gt;
            &lt;insets top="2" botton="2" right="2" left="2"/&gt;
        &lt;/state&gt;
        &lt;state value="ENABLED"&gt;
            &lt;imagePainter method="buttonBackground" path="dirt.png" sourceInsets="2 2 2 2"/&gt;
            &lt;insets top="2" botton="2" right="2" left="2"/&gt;
        &lt;/state&gt;
    &lt;/style&gt;
    &lt;bind style="button" type="name" key="dirt"/&gt;
&lt;/synth&gt;


The bind element there specifies what to map to (in this example, it will apply that styling to any buttons whose name property has been set to "dirt").

And a couple of useful links:

http://javadesktop.org/articles/synth/

http://docs.oracle.com/javase/tutorial/uiswing/lookandfeel/synth.html


This will allow you to pass a weight of 0 and still work properly.  Notice the === operator, this checks to see if weight matches "null" in both value and type (as opposed to ==, which is just value, so 0 == null == false).

PHP:

public function createShipment($startZip, $endZip, $weight=null){    if ($weight === null)        $weight = $this-&gt;getDefaultWeight();}

Thxm, Mc! http://asm.objectweb.org/ is another one. Excellent documentation on byte code instrumentation, but nothing "directly" aimed at writing a coverage tool - just some hints or ideas.

Quick'n Dirty:

First create your ConfigurationSection and ConfigurationElement classes:

public class MyStuffSection : ConfigurationSection
{
    ConfigurationProperty _MyStuffElement;

    public MyStuffSection()
    {
        _MyStuffElement = new ConfigurationProperty("MyStuff", typeof(MyStuffElement), null);

        this.Properties.Add(_MyStuffElement);
    }

    public MyStuffElement MyStuff
    {
        get
        {
            return this[_MyStuffElement] as MyStuffElement;
        }
    }
}

public class MyStuffElement : ConfigurationElement
{
    ConfigurationProperty _SomeStuff;

    public MyStuffElement()
    {
        _SomeStuff = new ConfigurationProperty("SomeStuff", typeof(string), "&lt;UNDEFINED&gt;");

        this.Properties.Add(_SomeStuff);
    }

    public string SomeStuff
    {
        get
        {
            return (String)this[_SomeStuff];
        }
    }
}


Then let the framework know how to handle your configuration classes in web.config:

&lt;configuration&gt;
  &lt;configSections&gt;
    &lt;section name="MyStuffSection" type="MyWeb.Configuration.MyStuffSection" /&gt;
  &lt;/configSections&gt;
  ...


And actually add your own section below:

  &lt;MyStuffSection&gt;
    &lt;MyStuff SomeStuff="Hey There!" /&gt;
  &lt;/MyStuffSection&gt;


Then you can use it in your code thus:

MyWeb.Configuration.MyStuffSection configSection = ConfigurationManager.GetSection("MyStuffSection") as MyWeb.Configuration.MyStuffSection;

if (configSection != null &amp;&amp; configSection.MyStuff != null)
{
    Response.Write(configSection.MyStuff.SomeStuff);
}


Yes, this is possible. One of the main pros for using Swing is the ease with which the abstract controls can be created and manipulates.

Here is a quick and dirty way to extend the existing JButton class to draw a circle to the right of the text.

package test;

import java.awt.Color;
import java.awt.Container;
import java.awt.Dimension;
import java.awt.FlowLayout;
import java.awt.Graphics;

import javax.swing.JButton;
import javax.swing.JFrame;

public class MyButton extends JButton {

    private static final long serialVersionUID = 1L;

    private Color circleColor = Color.BLACK;

    public MyButton(String label) {
        super(label);
    }

    @Override
    protected void paintComponent(Graphics g) {
        super.paintComponent(g);

        Dimension originalSize = super.getPreferredSize();
        int gap = (int) (originalSize.height * 0.2);
        int x = originalSize.width + gap;
        int y = gap;
        int diameter = originalSize.height - (gap * 2);

        g.setColor(circleColor);
        g.fillOval(x, y, diameter, diameter);
    }

    @Override
    public Dimension getPreferredSize() {
        Dimension size = super.getPreferredSize();
        size.width += size.height;
        return size;
    }

    /*Test the button*/
    public static void main(String[] args) {
        MyButton button = new MyButton("Hello, World!");

        JFrame frame = new JFrame();
        frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
        frame.setSize(400, 400);

        Container contentPane = frame.getContentPane();
        contentPane.setLayout(new FlowLayout());
        contentPane.add(button);

        frame.setVisible(true);
    }

}


Note that by overriding paintComponent that the contents of the button can be changed, but that the border is painted by the paintBorder method. The getPreferredSize method also needs to be managed in order to dynamically support changes to the content. Care needs to be taken when measuring font metrics and image dimensions.

For creating a control that you can rely on, the above code is not the correct approach. Dimensions and colours are dynamic in Swing and are dependent on the look and feel being used. Even the default Metal look has changed across JRE versions. It would be better to implement AbstractButton and conform to the guidelines set out by the Swing API. A good starting point is to look at the javax.swing.LookAndFeel and javax.swing.UIManager classes.

http://docs.oracle.com/javase/8/docs/api/javax/swing/LookAndFeel.html

http://docs.oracle.com/javase/8/docs/api/javax/swing/UIManager.html

Understanding the anatomy of LookAndFeel is useful for writing controls:
Creating a Custom Look and Feel


When I was first learning Java we had to make Yahtzee and I thought it would be cool to create custom Swing components and containers instead of just drawing everything on one JPanel.  The benefit of extending Swing components, of course, is to have the ability to add support for keyboard shortcuts and other accessibility features that you can't do just by having a paint() method print a pretty picture.  It may not be done the best way however, but it may be a good starting point for you.

Edit 8/6 - If it wasn't apparent from the images, each Die is a button you can click. This will move it to the DiceContainer below.  Looking at the source code you can see that each Die button is drawn dynamically, based on its value.





Here are the basic steps:


Create a class that extends JComponent
Call parent constructor super() in your constructors
Make sure you class implements MouseListener
Put this in the constructor:

enableInputMethods(true);   
addMouseListener(this);

Override these methods:

public Dimension getPreferredSize()  
public Dimension getMinimumSize()  
public Dimension getMaximumSize()

Override this method:

public void paintComponent(Graphics g)



The amount of space you have to work with when drawing your button is defined by getPreferredSize(), assuming getMinimumSize() and getMaximumSize() return the same value.  I haven't experimented too much with this but, depending on the layout you use for your GUI your button could look completely different.

And finally, the source code.  In case I missed anything.  


in 1.x there used to be things DataTables couldn't do which DataSets could (don't remember exactly what). All that was changed in 2.x.  My guess is that's why a lot of examples still use DataSets.  DataTables should be quicker as they are more lightweight. If you're only pulling a single resultset, its your best choice between the two.

It really depends on the sort of data you're bringing back.  Since a DataSet is (in effect) just a collection of DataTable objects, you can return multiple distinct sets of data into a single, and therefore more manageable, object.  

Performance-wise, you're more likely to get inefficiency from unoptimized queries than from the "wrong" choice of .NET construct.  At least, that's been my experience.


One feature of the DataSet is that if you can call multiple select statements in your stored procedures, the DataSet will have one DataTable for each.

Try the Managed Fusion Url Rewriter and Reverse Proxy:

http://urlrewriter.codeplex.com

The rule for rewriting this would be:

# clean up old rules and forward to new URL
RewriteRule ^/?user=(.*)  /users/$1 [NC,R=301]

# rewrite the rule internally
RewriteRule ^/users/(.*)  /?user=$1 [NC,L]



Scott Guthrie covers how to do this natively in .Net pretty extensively here.

I've used the httpmodule approach and it works well.  It's basically what ManagedFusion is doing for you.

if its .net on both sides:
think you should use binary serialization and send the byte[] result.
trusting your struct to be fully blittable can be trouble.
you will pay in some overhead (both cpu and network) but will be safe.

Whatever your [things] are need to be written outside of VBScript.

In VB6, you can write a Custom Collection class, then you'll need to compile to an ActiveX DLL and register it on your webserver to access it.

Make sure that the file has svn:keywords "Rev Id" and then put $Rev$ somewhere in there.

See this question and the answers to it.

If you need to populate each member variable by hand you can generalize it a bit as far as the primitives are concerned by using FormatterServices to retrieve in order the list of variable types associated with an object. I've had to do this in a project where I had a lot of different message types coming off the stream and I definitely didn't want to write the serializer/deserializer for each message. 

Here's the code I used to generalize the deserialization from a byte[].

public virtual bool SetMessageBytes(byte[] message)    {        MemberInfo[] members = FormatterServices.GetSerializableMembers(this.GetType());        object[] values = FormatterServices.GetObjectData(this, members);        int j = 0;        for (int i = 0; i &lt; members.Length; i++)        {            string[] var = members[i].ToString().Split(new char[] { ' ' });            switch (var[0])            {                case "UInt32":                    values[i] = (UInt32)((message[j] &lt;&lt; 24) + (message[j + 1] &lt;&lt; 16) + (message[j + 2] &lt;&lt; 8) + message[j + 3]);                    j += 4;                    break;                case "UInt16":                    values[i] = (UInt16)((message[j] &lt;&lt; 8) + message[j + 1]);                    j += 2;                    break;                case "Byte":                    values[i] = (byte)message[j++];                    break;                case "UInt32[]":                    if (values[i] != null)                    {                        int len = ((UInt32[])values[i]).Length;                        byte[] b = new byte[len * 4];                        Array.Copy(message, j, b, 0, len * 4);                        Array.Copy(Utilities.ByteArrayToUInt32Array(b), (UInt32[])values[i], len);                        j += len * 4;                    }                    break;                case "Byte[]":                    if (values[i] != null)                    {                        int len = ((byte[])values[i]).Length;                        Array.Copy(message, j, (byte[])(values[i]), 0, len);                        j += len;                    }                    break;                default:                    throw new Exception("ByteExtractable::SetMessageBytes Unsupported Type: " + var[1] + " is of type " +  var[0]);            }        }        FormatterServices.PopulateObjectMembers(this, members, values);        return true;    }

There is nothing VS specific with the MVC framework - it is just a bunch of DLLs that you can use. The wizards in VS just build you a quick-start framework.
ASP.NET MVC is "bin-deployable" - there is nothing too clever to set up on the server either - just point the wildcard ISAPI filter to ASP.NET

The search seems to be an "OR" instead of an "AND" :-( If you do a search for ASP.NET SVN on the search bar, you will not find it on Page 6 or so.

it definitely should get some refining.

Go to System Preferences &gt; Keyboard and Mouse, then choose Keyboard Shortcuts. At the bottom, ensure Full Keyboard Access is set to "All controls". It's a long time since I turned it on but I think that's all you need to do

Apple Menu &gt; System Preferences &gt; Keyboard &amp; Mouse &gt; Keyboard Shortcuts:

Change the radio button at the bottom from "Text boxes and lists only" to "All controls."

Edit: Dammit. We're a fast group around here aren't we? :-)

This is an alternative way (DataReader is faster than this one):

string s = "";SqlConnection conn = new SqlConnection("Server=192.168.1.1;Database=master;Connect Timeout=30;User ID=foobar;Password=raboof;");SqlDataAdapter da = new SqlDataAdapter("SELECT TOP 5 name, dbid FROM sysdatabases", conn);DataTable dt = new DataTable();da.Fill(dt);for (int i = 0; i &lt; dt.Rows.Count; i++){    s += dt.Rows[i]["name"].ToString() + " -- " + dt.Rows[i]["dbid"].ToString() + "\n";}MessageBox.Show(s);

It's in the System Preferences - this blog post shows where the setting is.

Probably it comes from VB6. Because with Option Base statement in VB6, you can alter the lower bound of arrays like this:

Option Base 1

Also in VB6, you can alter the lower bound of a specific array like this:

Dim myArray(4 To 42) As String

Check out this blog post: Tweaking the ICallbackEventHandler and Viewstate. The author seems to be addressing the very situation that you are experiencing: 


  So when using ICallbackEventHandler you have two obstacles to overcome to have updated state management for callbacks. First is the problem of the read-only viewstate. The other is actually registering the changes the user has made to the page before triggering the callback.


See the blog post for his suggestions on how to solve this. Also check out this forum post which discusses the same problem as well.

I have found the solution else where:

SELECT SUBSTRING(master.dbo.fn_varbintohexstr(HashBytes('MD5', 'HelloWorld')), 3, 32) 

Why not use For Each?  That way you don't need to care what the LBound and UBound are.

Dim x, y, z
x = Array(1, 2, 3)

For Each y In x
    z = DoSomethingWith(y)
Next



You should make your choice of server platform based on the environment as a whole, and that includes the admin/management interfaces supplied.

I'm afraid that if you don't like the way Windows implements management of IIS, then that's too bad.  Having said that, a bit of delving around in the WMI interfaces will generally yield a solution that you should find usable.  I used to do quite a bit of WMI scripting (mostly via PowerShell) in order to have a reliable environment rebuild capability.

I actually found both of those links you provided, but as noted they are simply describing the problem, not solving it. The author of the blog post suggests a workaround by using a different ViewState provider, but unfortunately that isn't a possibility in this case...I really need to leave the particulars of the ViewState alone and just hook on to what is being done out-of-the-box.

Try this:

using (FileStream stream = new FileStream(fileName, FileMode.Open)){    BinaryFormatter formatter = new BinaryFormatter();    StructType aStruct = (StructType)formatter.Deserialize(filestream);}

A VirtualHost would also work for this and may work better for you as you can host several projects without the need for subdirectories.  Here's how you do it:

httpd.conf (or extra\httpd-vhosts.conf relative to httpd.conf. Trailing slashes "\" might cause it not to work):

NameVirtualHost *:80# ...&lt;VirtualHost *:80&gt;      DocumentRoot C:\projects\transitCalculator\trunk\    ServerName transitcalculator.localhost    &lt;Directory C:\projects\transitCalculator\trunk\&gt;          Order allow,deny          Allow from all      &lt;/Directory&gt;&lt;/VirtualHost&gt; 

HOSTS file (c:\windows\system32\drivers\etc\hosts usually):

# localhost entries127.0.0.1 localhost transitcalculator.localhost

Now restart XAMPP and you should be able to access http://transitcalculator.localhost/ and it will map straight to that directory.

This can be helpful if you're trying to replicate a production environment where you're developing a site that will sit on the root of a domain name.  You can, for example, point to files with absolute paths that will carry over to the server:

&lt;img src="/images/logo.png" alt="My Logo" /&gt;

whereas in an environment using aliases or subdirectories, you'd need keep track of exactly where the "images" directory was relative to the current file.

I don't see any problem with your code.

just out of my head, what if you try to do it manually? does it work?

BinaryReader reader = new BinaryReader(stream);StructType o = new StructType();o.FileDate = Encoding.ASCII.GetString(reader.ReadBytes(8));o.FileTime = Encoding.ASCII.GetString(reader.ReadBytes(8));.........

also try

StructType o = new StructType();byte[] buffer = new byte[Marshal.SizeOf(typeof(StructType))];GCHandle handle = GCHandle.Alloc(buffer, GCHandleType.Pinned);Marshal.StructureToPtr(o, handle.AddrOfPinnedObject(), false);handle.Free();

then use buffer[] in your BinaryReader instead of reading data from FileStream to see whether you still get AccessViolation exception.


  I had no luck using the
  BinaryFormatter, I guess I have to
  have a complete struct that matches
  the content of the file exactly.


That makes sense, BinaryFormatter has its own data format, completely incompatible with yours.

I suppose you could compare the ad prints with the page views on your website (which you can get from your analytics software).

Since programs like AdBlock actually never request the advert, you would have to look the server logs to see if the same user accessed a webpage but didn't access an advert. This is assuming the advert is on the same server.

If your adverts are on a separate server, then I would suggest it's impossible to do so.

The best way to stop users from blocking adverts, is to have inline text adverts which are generated by the server and dished up inside your html.

Add the user id to the request for the ad:

&lt;img src="./ads/viagra.jpg?{user.id}"/&gt;

that way you can check what ads are seen by which users.

You need to think about the different ways that ads are blocked. The first thing to look at is whether they are running noscript, so you could add a script that would check for that. 

The next thing is to see if they are blocking flash, a small movie should do that.

If you look at the adblock site, there is some indication of how it does blocking:
How does element hiding work?

If you look further down that page, you will see that conventional chrome probing will not work, so you need to try and parse the altered DOM.


Most people use .NET serialization (there is faster binary and slower XML formatter, they both depend on reflection and are version tolerant to certain degree)

However, if you want the fastest (unsafe) way - why not:

Writing:

YourStruct o = new YourStruct();
byte[] buffer = new byte[Marshal.SizeOf(typeof(YourStruct))];
GCHandle handle = GCHandle.Alloc(buffer, GCHandleType.Pinned);
Marshal.StructureToPtr(o, handle.AddrOfPinnedObject(), false);
handle.Free();


Reading:

handle = GCHandle.Alloc(buffer, GCHandleType.Pinned);
o = (YourStruct)Marshal.PtrToStructure(handle.AddrOfPinnedObject(), typeof(YourStruct));
handle.Free();



Alright. From a theoretical point of view, given that the distortion is "arbitrary", and any solution requires you to model this arbitrary distortion, you obviously can't get an "answer". However, any solution is going to involve imposing (usually implicitly) some model of the distortion that may or may not reflect the reality of the situation.

Since you seem to be most interested in models that presume some sort of local continuity of the distortion mapping, the most obvious choice is the one you've already tried: linear interpolaton between the nearest points. Going beyond that is going to require more sophisticated mathematical and numerical analysis knowledge.

You are incorrect, however, in presuming you cannot expand this to more points. You can by using a least-squared error approach. Find the linear answer that minimizes the error of the other points. This is probably the most straight-forward extension. In other words, take the 5 nearest points and try to come up with a linear approximation that minimizes the error of those points. And use that. I would try this next.

If that doesn't work, then the assumption of linearity over the area of N points is broken. At that point you'll need to upgrade to either a quadratic or cubic model. The math is going to get hectic at that point.

The two self-balancing BSTs I'm most familiar with are red-black and AVL, so I can't say for certain if any other solutions are better, but as I recall, red-black has faster insertion and slower retrieval compared to AVL. 

So if insertion is a higher priority than retrieval, red-black may be a better solution.


Red-black is better than AVL for insertion-heavy applications. If you foresee relatively uniform look-up, then Red-black is the way to go. If you foresee a relatively unbalanced look-up where more recently viewed elements are more likely to be viewed again, you want to use splay trees.


You can add a mouse click event to the TreeView, then select the correct node using GetNodeAt given the mouse coordinates provided by the MouseEventArgs.

void treeView1MouseUp(object sender, MouseEventArgs e)
{
    if(e.Button == MouseButtons.Right)
    {
        // Select the clicked node
        treeView1.SelectedNode = treeView1.GetNodeAt(e.X, e.Y);

        if(treeView1.SelectedNode != null)
        {
            myContextMenuStrip.Show(treeView1, e.Location);
        }
    }
}



As always, Google is your friend:
http://nixbit.com/cat/programming/libraries/c-generic-library/
specifically:
http://nixbit.com/cat/programming/libraries/generic-data-structures-library/

Flash is certainly the most ubiquitous and portable solution.  98% of browsers have Flash installed.  Other alternatives are Quicktime, Windows Media Player, or even Silverlight (Microsoft's Flash competitor, which can be used to embed several video formats).

I would recommend using Flash (and it's FLV video file format) for embedding your video unless you have very specific requirements as far as video quality or DRM.

Flash is usually the product of choice: Everyone has it, and using the JW FLV Player makes it relatively easy on your side.

As for other Video Formats, there are WMV and QuickTime, but the players are rather "heavy", not everyone might have them and they feel so 1990ish...

Real Player... Don't let me even start ranting about that pile of ...

The only other alternative of Flash that I would personally consider is Silverlight, which allows streaming WMV Videos. I found the production of WMV much better and easier than FLV because all Windows FLV Encoders I tried are not really good and stable, whereas pretty much every tool can natively output WMV. The problem with Silverlight is that no one has that Browser Plugin (yet?). There is also a player from JW.

I have worked for a company that developed a system for distributing media content to dedicated "players". It was web based and used ASP.NET technology and have tried almost every possible media format you can think of and your choice really comes down to asking yourself:

does it needs to play directly out of the box, or can I make sure that the components required to play the videos can be installed beforehand?

If your answer is that it needs to play out of the box then really your only option is flash (I know that it is not installed by default, but most will already have it installed)

If it is not a big issue that extra components are needed then you can go with formats that are supported by windows media player

The reason why windows media player falls into the second option is because for some browsers and some formats extra components must be installed.

We had the luxury that the "players" were provided by us, so we could go for the second option, however even we tried to convert as much as possible back to flash because it handles way better than windows media player

I'm no expert on Agile development, but I would imagine that integrating some basic automated pen-test software into your build cycle would be a good start.  I have seen several software packages out there that will do basic testing and are well suited for automation.

Not an answer, but a warning: my company bought the 2007 Infragistics ASP.NET controls just for the Grid, and we regret that choice. 

The quality of API is horrible (in our opinion at least), making it very hard to program against the grid (for example, inconsistent naming conventions, but this is just an inconvenience, we have complaints about the object model as well).

So I can't say that I know of a better option, I just know I will give a try to something else before paying for Infragistics products again (and the email support we got was horrible as well).

I'm not a security expert, but I think the most important fact you should be aware of, before testing security, is what you are trying to protect. Only if you know what you are trying to protect, you can do a proper analysis of your security measures and only then you can start testing those implemented measures.

Very abstract, I know. However, I think it should be the first step of every security audit.

Cerberus - it's more a full featured Help Desk/Issue Tracking system but it has a nice KB solution built in.  It can be free but they do have a low cost pay version that is also very good.

I think Drupal is a very possible choice. It has a lot of built-in support for book-type information capturing.

And there is a rich collection of user generated modules which you can use to enhance the features.

I think it has almost all the features you ask for out of the box.
Drupal CMS Benefits


I've also been investigating wiki software for use as a KB, but it is tricky to find something that is easy to use for non-technical people.  There are many wikis that attempt to provide WYSIWYG editing, but most of the software I've found generates nasty inefficient html markup from the WYSIWYG editor.  

One notable exception to this is Confluence which generates wiki syntax from a WYSIWYG editor. This still isn't perfect (show me a WYSIWYG editor that is) but is a pretty good compromise between retaining simple wiki syntax for those who like it and allowing non-technical users to contribute content.  The only problem is that Confluence isn't free ($1,200 for 25 user license).

Edit: I also tried DekiWiki and while the UI is nice it doesn't seem to be quite ready for primetime (suffers terribly from the bad WYSIWYG output disease mentioned above). Also seems like they lack direction as there are so many different ways of accomplishing the same task.


The Glib library used on the Gnome project may also be some use. Moreover it is pretty well tested.

IBM developer works has a good tutorial on its use: Manage C data using the GLib collections


Counting the number of times tags are used might be a google example:

Select TagName,Count(*) As TimesUsed From Tags Group By TagName Order TimesUsed

If your simply wanting a distinct value of tags, I would prefer to use the distant statement.

Select Distinct TagName From Tags Order By TagName Asc

SQLite databases exist independently, so there's not way to do this from the database level.

You will have to write your own code to do this.

GROUP BY also helps when you want to generate a report that will average or sum a bunch of data. You can GROUP By the Department ID and the SUM all the sales revenue or AVG the count of sales for each month.

Group By forces the entire set to be populated before records are returned (since it is an implicit sort).
For that reason (and many others), never use a Group By in a subquery.

I second Luke's answer.

I can Recommend Confluence and here is why:
I tested extensively many commercial and free Wiki based solutions. Not a single one is a winner on all accounts, including confluence. Let me try to make your quest a little shorter by summarizing what I have learned to be a pain and what is important:


WYSIWYG is a most have feature for the Enterprise. A wiki without it, skip it
Saying that, in reality, WYSIWYG doesn't work perfectly. It is more of a feature you must have to get the casual users not be afraid of the monster, and start using it. But you and anyone that wants to seriously create content, will very quickly get used to the wiki markup. it is faster and more reliable. 
You need good permissions controls (who can see, edit etc' a page). confluence has good, but I have my complaints (to complicated to be put here)
You will want a good export feature. Most will give you a single page "PDF" export, but you need much more. For example, lets say you have an FAQ, you want to export the entire FAQ right? will that work? 
Macros: you want a community creating macros. You asked for example about the ability to rate pages, here is a link to a Macro for Confluence that lets you do that
Structure: you want to be able to say that a page is a child of a different page, and be able to browse the data. The wikipedia model, of orphaned pages with no sturcture will not work in the Enterprise. (think FAQ, you want to have a hierarchy no?)
Ability to easily attache picture to be embedded in the body of the page/article. In confluence, you need to upload the image and then can embed it, it could be a little better (CTR+V) but I guess this is easy enough for 80% of the users.


At the end of the day, remember that a Wiki will be valuable to you the more flexible it is. It needs to be a "blank" canvas, and your imagination is then used to "build" the application. In Confluence, I found 3 different "best practices" on how to create a FAQ. That means I can implement MANY things. 

Some examples (I use my Wiki for)


FAQ: any error, problem is logged. Used by PS and ENG. reduced internal support time dramatically
Track account status: I implemetned sophisticated "dashboard" that you can see at a glance which customer is at what state, the software version they have, who in the company 'owns" the custoemr etc'
Product: all documentation, installation instructions, the "what's new" etc
Technical documentation, DB structure and what the tables mean 
HR: contact list, Document repository 


My runner up (15 month ago) was free Deki_Wiki, time has passed, so I don't know if this would be still my runner up.

good luck!

http://www.csszengarden.com/
The images are not Creative Commons, but the CSS is.

+1 for Zen garden.

I like the resources at inobscuro.com

The Open Design Community is a great resource.

http://www.opensourcetemplates.org/ has nice designs, just not enough selection.

To retrieve the number of widgets from each widget category that has more than 5 widgets, you could do this:

SELECT WidgetCategory, count(*)
FROM Widgets
GROUP BY WidgetCategory
HAVING count(*) &gt; 5


The "having" clause is something people often forget about, instead opting to retrieve all their data to the client and iterating through it there.


I was wrestling with this problem several years ago (2004 I think).  We ran into the problem that Firefox doesn't allow scripts to read the clipboard by default (but you can grant access to the clipboard).

There's other ways of reading the clipboard data as well...Flash, for instance, can read the clipboard.  There's a good article on ajaxian to explain how do to this behind the scenes.

In the end, we couldn't find a web-based Grid that fit the bill, so we had to create our own in a mixture of Actionscript and Javascript.

Check out:


Open Source Web Designs
CSS Remix
Best Web Gallery
CSS Based
CSS Beauty
CSS Genius



You need to handle the System.Windows.Forms.Application.ThreadException event for Windows Forms. This article really helped me: http://bytes.com/forum/thread236199.html.


Your CAST-FLOOR-CAST already seems to be the optimum way, at least on MS SQL Server 2005.

Some other solutions I've seen have a string-conversion, like Select Convert(varchar(11), getdate(),101) in them, which is slower by a factor of 10.


By no means is MVC the only design pattern for the web, but it is a useful one.

Adopting just the 'M' will pay dividends, in my opinion, even if you can't/won't adopt the 'V' or 'C'.

If you're using VB.NET, you can tap into the very convenient ApplicationEvents.vb.  This file comes for free with a VB.NET WinForms project and contains a method for handling unhandled exceptions.

To get to this nifty file, it's "Project Properties &gt;&gt; Application &gt;&gt; Application Events"

If you're not using VB.NET, then yeah, it's handling Application.ThreadException.

I'm currently using dhtmlxGrid and we have the Excel copy/paste functionality working.  dhtmlXGrid is the most full featured javascript grid package that I've found.

On their website, dhtmlXGrid claims to support Clipboard functionality in the Professional version.  (However, I noticed the Sample on their site isn't working on my Firefox.  EDIT: It's probably the permissions issue that Nathan mentioned.)

In any case, we had to do some extra work to get the exact Excel copy and paste functionality we wanted.  We essentially had to override some of their functionality to get the desired behavior.  Their support was pretty good in helping us come up with a solution.

So to answer your question, you should be able to get them to support copy and paste if you purchase the Professional version.  I'm just warning you that it may take some additional work to fine tune that behavior.  

Overall, I'm happy with dhtmlXGrid.  We use a lot of their features.  Their support is pretty good.  They usually take one day to respond since they are in Europe (I think).  And Javascript is by its very nature open source so I can always dive in when I need to.  

This question is loaded, data driven design vs domain driven design. For any application that has a good amount of behavior, then domain driven design should be preferred. Reporting, or utility applications tend to work better (or are quicker to develop) with data driven design.

What you're asking is "should my company make a fundamental shift in how we design our code". As a domain-freak, my gut reaction is to scream yes. However, by the simple nature of your question, I'm not sure you fully understand the scope of the change you are proposing. I think you should talk more to your team about it. 

Get some literature, such as Evan's DDD book, or the free foundations ebook, and then you'll be in a better position to judge which direction you should go.


You can add a timestamp field to that table and update that timestamp value with an update trigger.

Currently in my winforms app I have handlers for Application.ThreadException, as above, but also AppDomain.CurrentDomain.UnhandledException

Most exceptions arrive via the ThreadException handler, but the AppDomain one has also caught a few in my experience

Nice idea :)

I presume you've used wget's

--load-cookies (filename)

might help a little but it might be easier to use something like Mechanize (in Perl or python) to mimic a browser more fully to get a good spider.

Your status page is available now without logging in (click logout and try it). When the beta-cookie is disabled, there will be nothing between you and your status page.

For wget:

wget --no-cookies --header "Cookie: soba=(LookItUpYourself)" http://stackoverflow.com/users/30/myProfile.html



I couldn't figure out how to get the cookies to work either, but I was able to get to my status page in my browser while I was logged out, so I assume this will work once stackoverflow goes public.

This is an interesting idea, but won't you also pick up diffs of the underlying html code?  Do you have a strategy to avoid ending up with a diff of the html and not the actual content?

try

SELECT 1 AS Tag,
0 AS Parent,
AccountNumber AS [Root!1!AccountNumber!element]
FROM Location.LocationMDAccount
WHERE LocationID = 'long-guid-here'
FOR XML EXPLICIT


Try this, Chris:

SELECT    AccountNumber as [clientId]FROM    Location.Location rootWHERE    LocationId = 'long-guid-here'FOR    XML AUTO, ELEMENTS

TERRIBLY SORRY! I mixed up what you were asking for. I prefer the XML AUTO just for ease of maintainance, but I believe either one is effective. My apologies for the oversight ;-)

I got it with:

select
1 as tag,
null as parent,
AccountNumber as 'root!1!clientID!element'
from
Location.LocationMDAccount
where
locationid = 'long-guid-here'
for xml explicit



SQL Server 2000 does not keep track of this information for you. 
There may be creative / fuzzy ways to guess what this date was depending on your database model. But, if you are talking about 1 table with no relation to other data, then you are out of luck.

You can use the Row_Number() function.
Its used as follows:

SELECT Row_Number() OVER(ORDER BY UserName) As RowID, UserFirstName, UserLastName
FROM Users


From which it will yield a result set with a RowID field which you can use to page between.

SELECT * 
FROM 
    ( SELECT Row_Number() OVER(ORDER BY UserName) As RowID, UserFirstName, UserLastName
      FROM Users 
    ) As RowResults
WHERE RowID Between 5 AND 10


etc


OmniAudit is a commercial package which implments auditng across an entire database.

A free method would be to write a trigger for each table which addes entries to an audit table when fired.

I believe you'd need to perform a separate query to accomplish that unfortionately.

I was able to accomplish this at my previous position using some help from this page:
Paging in DotNet 2.0

They also have it pulling a row count seperately.

The way I have done this is to create a command script file and pass this on the command line via the  /b command to psftp.exe.  I have also tried this in Perl and have yet to find a neater way of doing it.

There is an issue with this method, in that you already have to have accepted the RSA finger-print.  If not, then the script will either wait for user input to accept it or will skip over it if you are running in full batch mode, with a failure.  Also, if the server changes so that it's RSA finger-print changes (e.g. a cluster) then you need to re-accept the finger-print again.

Not an ideal method, but the only one I know.

I shall be watching this question incase anyone knows another way.

And here's what works...

curl -s --cookie soba=. http://stackoverflow.com/users

If you have a byte[] you should be able to use the BinaryReader class and set values on NewStuff using the available ReadX methods.

From what I can see in that context, you don't need to copy SomeByteArray into a buffer. You simply need to get the handle from SomeByteArray, pin it, copy the IntPtr data using PtrToStructure and then release. No need for a copy.

That would be:

NewStuff ByteArrayToNewStuff(byte[] bytes)
{
    GCHandle handle = GCHandle.Alloc(bytes, GCHandleType.Pinned);
    NewStuff stuff = (NewStuff)Marshal.PtrToStructure(handle.AddrOfPinnedObject(), typeof(NewStuff));
    handle.Free();
    return stuff;
}


Generic version:

T ByteArrayToStructure&lt;T&gt;(byte[] bytes) where T: struct 
{
    GCHandle handle = GCHandle.Alloc(bytes, GCHandleType.Pinned);
    T stuff = (T)Marshal.PtrToStructure(handle.AddrOfPinnedObject(), typeof(T));
    handle.Free();
    return stuff;
}


...


You can't check for changes without some sort of audit mechanism.  You are looking to extract information that ha not been collected.  If you just need to know when a record was added or edited, adding a datetime field that gets updated via a trigger when the record is updated would be the simplest choice.

If you also need to track when a record has been deleted, then you'll want to use an audit table and populate it from triggers with a row when a record has been added, edited, or deleted.

If you use JavaScript to open the popup, you can use something like this:

var newWin = window.open(url);             

if(!newWin || newWin.closed || typeof newWin.closed=='undefined') 
{ 
     //POPUP BLOCKED
}



From Mark Harrison


  And here's what works...
  
  curl -s --cookie soba=. http://stackoverflow.com/users


And for wget:

wget --no-cookies --header "Cookie: soba=(LookItUpYourself)" http://stackoverflow.com/users/30/myProfile.html


Using attributes, child config sections and constraints

There is also the possibility to use attributes which automatically takes care of the plumbing, as well as providing the ability to easily add constraints.

I here present an example from code I use myself in one of my sites. With a constraint I dictate the maximum amount of disk space any one user is allowed to use.

MailCenterConfiguration.cs:

namespace Ani {

    public sealed class MailCenterConfiguration : ConfigurationSection
    {
        [ConfigurationProperty("userDiskSpace", IsRequired = true)]
        [IntegerValidator(MinValue = 0, MaxValue = 1000000)]
        public int UserDiskSpace
        {
            get { return (int)base["userDiskSpace"]; }
            set { base["userDiskSpace"] = value; }
        }
    }
}


This is set up in web.config like so

&lt;configSections&gt;
    &lt;!-- Mailcenter configuration file --&gt;
    &lt;section name="mailCenter" type="Ani.MailCenterConfiguration" requirePermission="false"/&gt;
&lt;/configSections&gt;
...
&lt;mailCenter userDiskSpace="25000"&gt;
    &lt;mail
     host="my.hostname.com"
     port="366" /&gt;
&lt;/mailCenter&gt;


Child elements

The child xml element mail is created created in the same .cs file as the one above. Here I've added constraints on the port. If the port is assigned a value not in this range the runtime will complain when the config is loaded.

MailCenterConfiguration.cs:

public sealed class MailCenterConfiguration : ConfigurationSection
{
    [ConfigurationProperty("mail", IsRequired=true)]
    public MailElement Mail
    {
        get { return (MailElement)base["mail"]; }
        set { base["mail"] = value; }
    }

    public class MailElement : ConfigurationElement
    {
        [ConfigurationProperty("host", IsRequired = true)]
        public string Host
        {
            get { return (string)base["host"]; }
            set { base["host"] = value; }
        }

        [ConfigurationProperty("port", IsRequired = true)]
        [IntegerValidator(MinValue = 0, MaxValue = 65535)]
        public int Port
        {
            get { return (int)base["port"]; }
            set { base["port"] = value; }
        }


Use

To then use it practically in code, all you have to do is instantiate the MailCenterConfigurationObject, this will automatically read the relevant sections from web.config.

MailCenterConfiguration.cs

private static MailCenterConfiguration instance = null;
public static MailCenterConfiguration Instance
{
    get
    {
        if (instance == null)
        {
            instance = (MailCenterConfiguration)WebConfigurationManager.GetSection("mailCenter");
        }

        return instance;
    }
}


AnotherFile.cs

public void SendMail()
{
    MailCenterConfiguration conf = MailCenterConfiguration.Instance;
    SmtpClient smtpClient = new SmtpClient(conf.Mail.Host, conf.Mail.Port);
}


Check for validity

I previously mentioned that the runtime will complain when the configuration is loaded and some data does not comply to the rules you have set up (e.g. in MailCenterConfiguration.cs). I tend to want to know these things as soon as possible when my site fires up. One way to solve this is load the configuration in _Global.asax.cx.Application_Start_ , if the configuration is invalid you will be notified of this with the means of an exception. Your site won't start and instead you will be presented detailed exception information in the Yellow screen of death.

Global.asax.cs

protected void Application_ Start(object sender, EventArgs e)
{
    MailCenterConfiguration.Instance;
}



I'd been wrangling over the same question whilst retro fitting LINQ to SQL over a legacy DB. Our database is a bit of a whopper (150 tables) and after some thought and experimentation I elected to use multiple DataContexts. Whether this is considered an anti-pattern remains to be seen, but for now it makes life manageable.


SetForegroundWindow is supposed to steal focus and there are certain cases where it will fail.

The SetForegroundWindow function puts the thread that created the specified window into the foreground and activates the window. Keyboard input is directed to the window
Try capturing the focus with SetCapture prior to making the call. Also look into different ways of bringing the window to the front: SetForeGroundWindow, SetActiveWindow, even simulating a mouse click can do this.

You can try the BringWindowToTop function to not steal focus. I haven't used it, but it seems to be what you're looking for.

Ive only built 2 applications that used the profile provider. Since then I have stayed away from using it. For both of the apps I used it to store information about the user such as their company name, address and phone number. 

This worked fine until our client wanted to be able to find a user by one of these fields.
Searching involved looping through every users profile and comparing the information to the search criteria. As the user base grew the search time became unacceptable to our client. The only solution was to create a table to store the users information. Search speed was increased immensely.

I would recommend storing this type of information in its own table.

How can anything on a website not have a UI? Could you maybe be more specific about what you mean by this? I think by definition a Turing test needs a UI -- it has to interact with a user. Did you mean a GUI? Does that mean a non-image based Captcha?


Unfortunately the only API that isn't deprecated is located in the ApplicationServices framework, which doesn't have a bridge support file, and thus isn't available in the bridge. If you're wanting to use ctypes, you can use ATSFontGetFileReference after looking up the ATSFontRef.

Cocoa doesn't have any native support, at least as of 10.5, for getting the location of a font.

I think you might be alluding to an "invisible" captcha. Check out the Subkismet project for an invisible captcha implementation.

http://www.codeplex.com/subkismet

Bottom line: no, because the overhead of interpretation overwhelms the overhead of method dispatching.

Maybe I'm crazy but worrying about speed in cases like this using an interpretive language is like trying to figure out what color to paint the shed. Let's not even get into the idea that this kind of optimization is entirely pre-mature.

You hit the nail on the head when you said 'maintainability'. I'd choose the approach that is the most productive and most maintainable. If you need speed later, it ain't gonna come from switching between procedural versus object oriented coding paradigms inside an interpreted language.

If you are using an interpreted language, the difference is irrelevant.  You should not be using an interpreted language if performance is an issue.  Both will perform about the same.

You need to have Sharepoint 2007 service pack 1 -- or else there's no chance.
(Sharepoint 2007 predates ajax 1.0 -- they built some support into service pack 1)

Next, from a trouble shooting point of view, test that the exact same code functions as expected when hosted in a regular asp.net page. (Literally copy and paste your code across into a fresh page, to rule out any typographical errors). Ruling sharepoint in or out of the problem area will help narrow down the solution space.

As I understand it, random access is in constant time for both Python's dicts and lists, the difference is that you can only do random access of integer indexes with lists.  I'm assuming that you need to lookup a node by its label, so you want a dict of dicts.

However, on the performance front, loading it into memory may not be a problem, but if you use too much you'll end up swapping to disk, which will kill the performance of even Python's highly efficient dicts.  Try to keep memory usage down as much as possible.  Also, RAM is amazingly cheap right now; if you do this kind of thing a lot, there's no reason not to have at least 4GB.

If you'd like advice on keeping memory usage down, give some more information about the kind of information you're tracking for each node.

Personally I use MediaWiki for this purpose.  I've tried a number of other free and paid wikis (including Confluence) and have always been impressed with MediaWiki's simplicity and ease of use.  

I have MediaWiki installed on a thumb drive (using XAMPP from PortableApps), which I use mostly as a personal knowledge base/code snippet repository. I can take it with me wherever I go, and view/edit it from any computer I'm using.

I am not aware of any plugin that does it natively, especially since Mono users seem to prefer MonoDevelop.

However, it should be possible to use Cygwin and a custom MSBuild Task or Batch file in order to achieve that by using the native .deb creation tools.

I've just tested this with:


#!/usr/bin/perl -w
use strict;

sub testit {

 my @ret = ();
 if (shift){
   push @ret,1;
   push @ret,2;
   push @ret,3;
}else{
  push @ret,"oneonly";
}

return \@ret;
}

foreach my $r (@{testit(1)}){
  print $r." test1\n";
}
foreach my $r (@{testit()}){
   print $r." test2\n";
}



And it seems to work ok, so I'm thinking it has something to do with the result getting returned from the service?
If you have no control over the returning service this might be hard one to crack


I used to do that with FTP on windows (create a file of commands and shell out FTP.exe)

Xetius I tried to upvote yours and accept your answer too and I received an warning that I needed 25 rep to do that.  Sorry. 

SQL Server 2008 has a new date data type and this simplifies this problem to:

SELECT CAST(CAST(GETDATE() AS date) AS datetime)



im not sure there's any other way than:


$result = [ $result ]   if ref($result) ne 'ARRAY';  
foreach .....



I think that it depends on what purpose the Extension methods serve.


Extension methods that relate to specific business needs of a project (whether they are connected to basic data types or custom objects) should not be included in a library that would be distributed across multiple projects.
Extension methods that relate to basic data types (int, string, etc) or generics that have a wider application could be packaged and distributed across projects.


Take care not to globally include Extension methods that have little application, as they just clog up intellisense and can lead to confusion and/or misuse. 

Itzik Ben-Gan in DATETIME Calculations, Part 1 (SQL Server Magazine, February 2007) shows three methods of performing such a conversion (slowest to fastest; the difference between second and third method is small):

SELECT CAST(CONVERT(char(8), GETDATE(), 112) AS datetime)

SELECT DATEADD(day, DATEDIFF(day, 0, GETDATE()), 0)

SELECT CAST(CAST(GETDATE() - 0.50000004 AS int) AS datetime)


Your technique (casting to float) is suggested by a reader in the April issue of the magazine. According to him, it has performance comparable to that of second technique presented above.


I've been including my extension methods in with my Core libraries in the Utils class because people who are working with my framework are likely to find the methods useful, but for mass deployment where the end developer might have a choice of extension method libraries, I would advise putting all of your extensions into their own namespace, even their own project file, so that people can choose to add a reference or a using statement or simply where required, like so:

Core.Extensions.Base64Encode(str);

My Utils class is my bestest friend in the whole world, it was before extension methods came along and they have only helped to strengthen our relationship.  The biggest rule I would go by is to give people choice over what extension framework they are using where possible.

If you used Server.MapPath, then you should already have the relative web path. According to the MSDN documentation, this method takes one variable, path, which is the virtual path of the Web server. So if you were able to call the method, you should already have the relative web path immediately accessible. 

I'm guessing Date.Parse() doesn't work?

I use it, especially since the hosted Version of FugBugz is free for up to 2 people. I found it a lot nicer than paper as I'm working on multiple projects, and my paper tends to get rather messy once you start making annotations or if you want to re-organize and shuffle tasks around, mark them as complete only to see that they are not complete after all...

Plus, the Visual Studio integration is really neat, something paper just cannot compete with. Also, if you lay the project to rest for 6 months and come back, all your tasks and notes are still there, whereas with paper you may need to search all the old documents and notes again, if you did not discard it.

But that is just the point of view from someone who is not really good at staying organized :-) If you are a really tidy and organized person, paper may work better for you than it does for me.

Bonus suggestion: Run Fogbugz on a second PC (or a small Laptop like the eeePC) so that you always have it at your fingertips. The main problem with Task tracking programs - be it FogBugz, Outlook, Excel or just notepad - is that they take up screen space, and my two monitors are usually full with Visual Studio, e-Mail, Web Browsers, some Notepads etc.


Well then method 2 seems the best way:

private function castMethod2(dateString:String):Date {    if ( dateString == null ) {        return null;    }    if ( dateString.indexOf("0000-00-00") != -1 ) {        return null;    }    dateString = dateString.split("-").join("/");    return new Date(Date.parse( dateString ));}

I had no luck using the BinaryFormatter, I guess I have to have a complete struct that matches the content of the file exactly. I realised that in the end I wasn't interested in very much of the file content anyway so I went with the solution of reading part of stream into a bytebuffer and then converting it using

Encoding.ASCII.GetString()


for strings and

BitConverter.ToInt32()


for the integers.

I will need to be able to parse more of the file later on but for this version I got away with just a couple of lines of code.


Try akismet from wp guys 

I think asking the user simple questions like:

"How many legs does a dog have?"

Would be much more effective that any CAPTCHA systems out there at the moment. Not only is it very difficult for the computer to answer that question, but it is very easy for a human to answer!

Perhaps this might work:

String RelativePath = AbsolutePath.Replace(Request.ServerVariables["APPL_PHYSICAL_PATH"], String.Empty);

(i'm using c# but could be adapted to vb)

(@Tags2k: I have editted the above answer)

I found that if you append the last modified timestamp of the file onto the end of the URL the browser will request the files when it is modified. For example in PHP:

function urlmtime($url) {
   $parsed_url = parse_url($url);
   $path = $parsed_url['path'];

   if ($path[0] == "/") {
       $filename = $_SERVER['DOCUMENT_ROOT'] . "/" . $path;
   } else {
       $filename = $path;
   }

   if (!file_exists($filename)) {
       // If not a file then use the current time
       $lastModified = date('YmdHis');
   } else {
       $lastModified = date('YmdHis', filemtime($filename));
   }

   if (strpos($url, '?') === false) {
       $url .= '?ts=' . $lastModified;
   } else {
       $url .= '&amp;ts=' . $lastModified;
   }

   return $url;
}

function include_css($css_url, $media='all') {
   // According to Yahoo, using link allows for progressive 
   // rendering in IE where as @import url($css_url) does not
   echo '&lt;link rel="stylesheet" type="text/css" media="' .
        $media . '" href="' . urlmtime($css_url) . '"&gt;'."\n";
}

function include_javascript($javascript_url) {
   echo '&lt;script type="text/javascript" src="' . urlmtime($javascript_url) .
        '"&gt;&lt;/script&gt;'."\n";
}



Some solutions I have seen involve adding a version number to the end of the file in the form of a query string.

&lt;script type="text/javascript" src="funkycode.js?v1"&gt;


You could use the SVN revision number to automate this for you by including the word LastChangedRevision in your html file after where v1 appears above. You must also setup your repository to do this.

I hope this further clarifies my answer?

Firefox also allows pressing CTRL + R to reload everything on a particular page.


Eric Meyer implemented a very similar thing as a WordPress plugin called WP-GateKeeper that asks human-readable questions like "What colour is an orange?". He did have some issues around asking questions that a non-native English speaker would be able to answer simply, though. 
There are a few posts on his blog about it.

I use it as well and quite frankly wouldn't want to work without it.

I've always had some kind of issue tracker available for the projects I work on and thus am quite used to updating it. With FB6 the process is now even better.

Since FB also integrates with Subversion, the source control tool I use for my projects, the process is really good and I have two-way links between the two systems now. I can click on a case number in the Subversion logs and go to the case in FB, or see the revisions bound to a case inside FB.

When I was working for myself doing my consulting business I signed up for a hosted account and honestly I couldn't have done without it. 
What I liked most about it was it took 30 seconds to sign up for an account and I was then able to integrate source control using sourcegear vault (which is an excellent source control product and free for single developers) set up projects, clients, releases and versions and monitor my progress constantly.
One thing that totally blew me away was that I ended up completely abandoning outlook for all work related correspondence. I could manage all my client interactions from within fogbugz and it all just worked amazingly well.
In terms of overhead, one of the nice things you could do was turn anything into a case. Anything that came up in your mind while you were coding, you simply created a new email, sent it to fogbugz and it was instantly added as an item for review later.
I would strongly recommend you get yourself one of the hosted accounts and give it a whirl

@KP
After your update to the original question, the only real option available to you is to do some jiggery-pokery in Javascript on the client. The only issue with that would be provicing graceful degredation for non-javascript enabled clients. 
e.g. You could add some AJAX-y goodness that reads a hidden form filed value, requests a verification key from the server, and sends that back along with the response, but that will never be populated if javascript is blocked/disabled. You could always implement a more traditional captcha type interface which could be disabled by javascript, and ignored by the server if the scripted field if filled in...
Depends how far you want to go with it, though. Good luck

You'd be looking to static link (as opposed to dynamically link)

I'm not sure how many of the MS redistributables statically link in.

The Array.zip function does an elementwise combination of arrays. It's not quite as clean as the Python syntax, but here's one approach you could use:

weights = [1, 2, 3]
data = [4, 5, 6]
result = Array.new
a.zip(b) { |x, y| result &lt;&lt; x * y } # For just the one operation

sum = 0
a.zip(b) { |x, y| sum += x * y } # For both operations



As the compiler is telling you, you need to change your return type to IEnumerable. That is how the yield return syntax works.

Ruby has a map method (a.k.a. the collect method), which can be applied to any Enumerable object. If numbers is an array of numbers, the following line in Ruby:

numbers.map{|x| x + 5}

is the equivalent of the following line in Python:

map(lambda x: x + 5, numbers)

For more details, see here or here.

// Generic function that gets all child controls of a certain type, 
// returned in a List collection
private static List&lt;T&gt; GetChildTextBoxes&lt;T&gt;(Control ctrl) where T : Control{
    List&lt;T&gt; tbs = new List&lt;T&gt;();
    foreach (Control c in ctrl.Controls) {
        // If c is of type T, add it to the collection
        if (c is T) { 
            tbs.Add((T)c);
        }
    }
    return tbs;
}

private static void SetChildTextBoxesHeight(Control ctrl, int height) {
    foreach (TextBox t in GetChildTextBoxes&lt;TextBox&gt;(ctrl)) {
        t.Height = height;
    }
}



If you return IEnumerator, it will be a different enumerator object each time call that method (acting as though you reset the enumerator on each iteration). If you return IEnumerable then a foreach can enumerate based on the method with the yield statement.


  What is the difference between SetForeGroundWindow, SetActiveWindow, and BringWindowToTop? It appears as if they all do the same thing.


According to MSDN, SetForeGroundWindow will activate the window and direct keyboard focus to it. This attempts to work even when your process is in the background. SetActiveWindow does the same thing as SetForeGroundWindow, but it doesn't do anything if your application isn't the frontmost application. Finally, BringWindowToTop only brings the window to the top, and doesn't change the keyboard focus.

The previous answer is fine, but I'm one of those guys that's firmly against mixing behavior with presentation (putting JavaScript in my HTML) so I prefer to put my event handling logic in my JavaScript files. Additionally, not all browsers implement event (or e) the same way. You may want to do a check prior to running any logic:

document.onkeydown = TabExample;function TabExample(evt) {    var evt = (evt) ? evt : ((event) ? event : null);    if(evt.keycode == 9) {        // do work    }}

I'd rather tab indentation not work than breaking tabbing between form items.

If you want to indent to put in code in the Markdown box, use Ctrl+K (or ⌘K on a Mac).

In terms of actually stopping the action, jQuery (which Stack Overflow uses) will stop an event from bubbling when you return false from an event callback. This makes life easier for working with multiple browsers.


I would advise against changing the default behaviour of a key.  I do as much as possible without touching a mouse, so if you make my tab key not move to the next field on a form I will be very aggravated.

A shortcut key could be useful however, especially with large code blocks and nesting.  Shift-TAB is a bad option because that normally takes me to the previous field on a form.  Maybe a new button on the WMD editor to insert a code-TAB, with a shortcut key, would be possible?

The quick and simple answer is No.

Javascript is quite a high level language and does not have access to this sort of information.

If this is for an intranet application and all of the clients use DHCP, you can query the DHCP server for the MAC address for a given IP address.

IE 8 is supposed to have better tools, but the IE Developer Toolbar is pretty good.


You might find Firebug Lite useful for that.  

Its bookmarklet should be especially useful when debugging on a user's machine.

You should take a look at the UNPIVOT clause.
Update1: GateKiller, strangely enough I read an article (about something unrelated) about it this morning and I'm trying to jog my memory where I saw it again, had some decent looking examples too. It'll come back to me I'm sure.
Update2: Found it: http://weblogs.sqlteam.com/jeffs/archive/2008/04/23/unpivot.aspx

UNION should be your friend:

SELECT Column1 FROM table WHERE idColumn = 1UNION ALLSELECT Column2 FROM table WHERE idColumn = 1UNION ALLSELECT Column3 FROM table WHERE idColumn = 1

but it can also be your foe on large result sets.

I would go with your first solution. This allows the user to focus on entering their credit card details.

You can then transfer them to another webpage which asks or provides them further information relating to the Google Map.

If you have a fixed set of columns and you know what they are, you can basically do a series of subselects 
(SELECT Column1 AS ResultA FROM TableA) as R1 
and join the subselects. All this in a single query.

http://www.easymaths.com/What_on_earth_is_Bodmas.htm:


  What do you think the answer to 2 + 3 x 5 is?
  
  Is it (2 + 3) x 5 = 5 x 5 = 25 ?
  
  or 2 + (3 x 5) = 2 + 15 = 17 ?
  
  BODMAS can come to the rescue and give us rules to follow so that we always get the right answer:
  
  (B)rackets (O)rder (D)ivision (M)ultiplication (A)ddition (S)ubtraction
  
  According to BODMAS, multiplication should always be done before addition, therefore 17 is actually the correct answer according to BODMAS and will also be the answer which your calculator will give if you type in 2 + 3 x 5 .


Why it is useful in programming? No idea, but i assume it's because you can get rid of some brackets? I am a quite defensive programmer, so my lines can look like this:

result = (((i + 4) - (a + b)) * MAGIC_NUMBER) - ANOTHER_MAGIC_NUMBER;


with BODMAS you can make this a bit clearer:

result = (i + 4 - (a + b)) * MAGIC_NUMBER - ANOTHER_MAGIC_NUMBER;


I think i'd still use the first variant - more brackets, but that way i do not have to learn yet another rule and i run into less risk of forgetting it and causing those weird hard to debug errors?

Just guessing at that part though.

Mike Stone EDIT: Fixed math as Gaius points out


I'm not sure of the SQL Server syntax for this but in MySQL I would do

SELECT IDColumn, ( IF( Column1 &gt;= 3, 1, 0 ) + IF( Column2 &gt;= 3, 1, 0 ) + IF( Column3 &gt;= 3, 1, 0 ) + ... [snip ] )
  AS NumberOfColumnsGreaterThanThree
FROM TableA;


EDIT: A very (very) brief Google search tells me that the CASE statement does what I am doing with the IF statement in MySQL.  You may or may not get use out of the Google result I found

FURTHER EDIT: I should also point out that this isn't an answer to your question but an alternative solution to your actual problem.


Another version of this (in middle school) was "Please Excuse My Dear Aunt Sally".


Parentheses
Exponents
Multiplication
Division
Addition
Subtraction


The mnemonic device was helpful in school, and still useful in programming today.

Order of operations in an expression, such as:

foo * (bar + baz^2 / foo)



Brackets first
Orders (ie Powers and Square Roots, etc.)
Division and Multiplication (left-to-right)
Addition and Subtraction (left-to-right)


source: http://www.mathsisfun.com/operation-order-bodmas.html


In addition to the benefits already mentioned, another nice feature of using FogBugz is BugzScout, which you can use to report errors from your app and log them into FogBugz automatically.  If you're a one person team, chances are there are some bugs in your code you've never seen during your own testing, so it's nice to have those bugs found "in the wild" automatically reported and logged for you.

When I learned this in grade school (in Canada) it was referred to as BEDMAS:

Brackets 
Exponents 
Division 
Multiplication 
Addition 
Subtraction

Just for those from this part of the world...

I'm not really sure how applicable to programming the old BODMAS mnemonic is anyways.  There is no guarantee on order of operations between languages, and while many keep the standard operations in that order, not all do.  And then there are some languages where order of operations isn't really all that meaningful (Lisp dialects, for example).  In a way, you're probably better off for programming if you forget the standard order and either use parentheses for everything(eg (a*b) + c) or specifically learn the order for each language you work in.

Look around on github for some open source projects.  Some of the more popular projects are:


Rails
Merb
Rubinius


Look on the Popular Forked list and you'll probably see something that interests you.


I had to do this for a project before. One of the major difficulties I had was explaining what I was trying to do to other people. I spent a ton of time trying to do this in SQL, but I found the pivot function woefully inadequate. I do not remember the exact reason why it was, but it is too simplistic for most applications, and it isn't full implemented in MS SQL 2000. I wound up writing a pivot function in .NET. I'll post it here in hopes it helps someone, someday. 

 ''' &lt;summary&gt;    ''' Pivots a data table from rows to columns    ''' &lt;/summary&gt;    ''' &lt;param name="dtOriginal"&gt;The data table to be transformed&lt;/param&gt;    ''' &lt;param name="strKeyColumn"&gt;The name of the column that identifies each row&lt;/param&gt;    ''' &lt;param name="strNameColumn"&gt;The name of the column with the values to be transformed from rows to columns&lt;/param&gt;    ''' &lt;param name="strValueColumn"&gt;The name of the column with the values to pivot into the new columns&lt;/param&gt;    ''' &lt;returns&gt;The transformed data table&lt;/returns&gt;    ''' &lt;remarks&gt;&lt;/remarks&gt;    Public Shared Function PivotTable(ByVal dtOriginal As DataTable, ByVal strKeyColumn As String, ByVal strNameColumn As String, ByVal strValueColumn As String) As DataTable        Dim dtReturn As DataTable        Dim drReturn As DataRow        Dim strLastKey As String = String.Empty        Dim blnFirstRow As Boolean = True        ' copy the original data table and remove the name and value columns        dtReturn = dtOriginal.Clone        dtReturn.Columns.Remove(strNameColumn)        dtReturn.Columns.Remove(strValueColumn)        ' create a new row for the new data table        drReturn = dtReturn.NewRow        ' Fill the new data table with data from the original table        For Each drOriginal As DataRow In dtOriginal.Rows            ' Determine if a new row needs to be started            If drOriginal(strKeyColumn).ToString &lt;&gt; strLastKey Then                ' If this is not the first row, the previous row needs to be added to the new data table                If Not blnFirstRow Then                    dtReturn.Rows.Add(drReturn)                End If                blnFirstRow = False                drReturn = dtReturn.NewRow                ' Add all non-pivot column values to the new row                For Each dcOriginal As DataColumn In dtOriginal.Columns                    If dcOriginal.ColumnName &lt;&gt; strNameColumn AndAlso dcOriginal.ColumnName &lt;&gt; strValueColumn Then                        drReturn(dcOriginal.ColumnName.ToLower) = drOriginal(dcOriginal.ColumnName.ToLower)                    End If                Next                strLastKey = drOriginal(strKeyColumn).ToString            End If            ' Add new columns if needed and then assign the pivot values to the proper column            If Not dtReturn.Columns.Contains(drOriginal(strNameColumn).ToString) Then                dtReturn.Columns.Add(drOriginal(strNameColumn).ToString, drOriginal(strValueColumn).GetType)            End If            drReturn(drOriginal(strNameColumn).ToString) = drOriginal(strValueColumn)        Next        ' Add the final row to the new data table        dtReturn.Rows.Add(drReturn)        ' Return the transformed data table        Return dtReturn    End Function

Creating an installer project, with a dependency on your EXE (which in turn depends on whatever it needs) is a fairly straightforward process - but you'll need at least VS Standard Edition for that.

Inside the installer project, you can create custom tasks and dialog steps that allow you to do anything you code up.

What's missing is the auto-upgrade and version-checking magic you get with ClickOnce.  You can still build it in, it's just not automatic.

I've worked on a SAN system in the past with telephony audio recordings which had issues with numbers of files in a single folder - that system became unusable somewhere near 5,000 (on Windows 2000 Advanced Server with an application in C#.Net 1.1)- the only sensible solution that we came up with was to change the folder structure so that there were a more reasonable number of files. Interestingly Explorer would also time out!

The convention we came up with was a structure that broke the structure up in years, months and days - but that will depend upon your system and whether you can control the directory structure...

I don't believe there is any easy way to make a Windows Installer project have the ease or upgradability of ClickOnce. I use ClickOnce for all the internal .NET apps I develop (with the exception of Console Apps). I find that in an enterprise environment, the ease of deployment outweighs the lack of flexibility.

Nope. The reason ActiveX can do it is because ActiveX is a little application that runs on the client's machine.

I would imagine access to such information via JavaScript would be a security vulnerability.

Have you seen WiX yet?

http://wix.sourceforge.net/

It builds windows installers using an XML file and has additional libraries to use if you want to fancify your installers and the like. I'll admit the learning curve for me was medium-high in getting things started, but afterwards I was able to build a second installer without any hassles. 

It will handle updates and other items if you so desire, and you can apply folder permissions and the like to the installers. It also gives you greater control on where exactly you want to install files and is compatible with all the standardized Windows folder conventions, so you can specify "PROGRAM_DATA" or something to that effect and the installer knows to put it in C:\Documents and Settings\All Users\Application Data or C:\ProgramData depending on if you're running XP or Vista.

The rumor is that Office 2007 and Visual Studio 2008 used WiX to create their installer, but I haven't been able to verify that anywhere. I do believe is is developed by some Microsoft folks on the inside.

None. .NET relies on underlying Windows API calls that really, really hate that amount of files themselves.
As Ronnie says: split them up. 

ClickOnce can be problematic if you have 3rd party components that need to be installed along with your product. You can skirt this to some extent by creating installers for the components however with ClickOnce deployment you have to create the logic to update said component installers.

I've in a previous life used Wise For Windows Installer to create installation packages. While creating upgrades with it were not automatic like ClickOnce is, they were more precise and less headache filled when it came to other components that needed to be registered/added.

Go to http://www.fogbugz.com/ then at the bottom under "Try It", sign up.

under Settings =&gt; Your FogBugz Hosted Account, it should either already say "Payment Information:    Using Student and Startup Edition." or there should be some option/link to turn on the Student and Startup Edition.

And yes, it's not only for Students and Startups, I asked their support :-)

Disclaimer: I'm not affiliated with FogCreek and Joel did not just deposit money in my account.

You could use DOS?

DIR /s/b &gt; Files.txt

Why not just use SQL Management Studio to create a complete script of your database and the objects?

Definitely split them up.  That said, stay as far away from the Indexing Service as you can.

Use a 3 step process:


Generate a script from the working database
Create a new database from that script
Create a backup of the new database


This answer is incomplete and flawed! It only works from TortoisSVN to Fogbugz, but not the other way around. I still need to know how to get it to work backwards from Fogbugz (like it's designed to) so that I can see the Revision number a bug is addressed in from Fogbugz while looking at a bug.



Helpful URLS

http://tortoisesvn.net/docs/release/TortoiseSVN_en/tsvn-dug-propertypage.html

http://tortoisesvn.net/issuetracker_integration



Set the "Hooks"


Go into your fogbugz account and click Extras > Configure Source Control Integration
Download "post-commit.bat" and the VBScript file for Subversion
Create a "hooks" directory in a common easily accessed location (preferably with no spaces in the file path)
Place a copy of the files in the hooks directories
Rename the files without the ".safe" extension
Right click on any directory.
Select "TortoiseSVN > Settings" (in the right click menu from the last step)
Select "Hook Scripts"





Click "Add"
Set the properties thus:


Hook Type: Post-Commit Hook 
Working Copy Path: C:\\Projects (or whatever your root directory for all of your projects is. If you have multiple you will need to do this step for each one.) 
Command Line To Execute: C:\\subversion\\hooks\\post-commit.bat (this needs to point to wherever you put your hooks directory from step 3)
I also selected the checkbox to Wait for the script to finish...



WARNING: Don't forget the double back-slash! "\\"

Click OK...



Note: the screenshot is different, follow the text for the file paths, NOT the screenshot...

At this point it would seem you could click "Issue Tracker Integration" and select Fogbugz. nope. It just returns "There are no issue-tracker providers available".


Click "OK" to close the whole
settings dialogue window


Configure the Properties


Once again, Right click on the root directory of the checked out
project you want to work with (you need to do this "configure the properties" step for each project -- See "Migrating Properties Between Projects" below)
Select "TortoiseSVN > Properties" (in the right click menu
from the last step)
Add five property value pairs by clicking "New..." and inserting the
following in "Property Name" and
"Property Value" respectively:



  bugtraq:label BugzID:
  bugtraq:message   BugzID: %%BUGID%%
  
  bugtraq:number    true
  
  bugtraq:url   http://[your fogbugz URL
  here]/default.asp?%BUGID%
  
  bugtraq:warnifnoissue false






Click "OK"


Commiting Changes and Viewing the Logs

Now when you are commiting, you can specify one bug that the commit addresses. This kind of forces you to commit after fixing each bug...



When you view the log (Right click root of project, TortoiseSVN > show log) you can see the bug id that each checking corresponds to (1), and you can click the bug id number to be taken to fogbugz to view that bug automatically if you are looking at the actual log message. Pretty nifty!





Migrating Properties Between Projects


Right click on a project that already has the proper Properties configuration
Select "TortoiseSVN > Properties" (from the right-click menu from step 1)
Highlight all of the desired properties
Click "Export"
Name the file after the property, and place in an easily accessible directory (I placed mine with the hooks files)





Right click on the root directory of the checked out project needing properties set for.
Click "Import"
Select the file you exported in step 4 above
Click Open



Wow. 

Not a lot of help here but, My Fiancée works for SAP here in latin america and constantly complains on the low out-SAP developer base SAP has. I suppose through purchases and whatnot (Bobj, all-in-one and whatnot) eventually something will spark interest but nothing comes to mind.

/mp

Warning: move_uploaded_file() [function.move-uploaded-file]: Unable to move 'C:\WINDOWS\Temp\phpA30E.tmp' to './people.xml' in E:\inetpub\vhosts\mywebsite.com\httpdocs\dump\upload.php on line 3

is the important line it says you can't put the file where you want it and this normally means a permissions problem

check the process running the app (normally the webservers process for php) has the rights to write a file there.

EDIT:

hang on a bit
I jumped the gun a little is the path to the file in the first line correct?

As it's Windows, there is no real 777. If you're using chmod, check the Windows-related comments.

Check that the IIS Account can access (read, write, modify) these two folders:

E:\inetpub\vhosts\mywebsite.com\httpdocs\dump\C:\WINDOWS\Temp\

I make heavy use of this tool:
SQLBalance for MySQL  

Unfortunately; its windows only... but works like a charm to move databases around, data or no data, merge or compare.


Have you tried the ADO driver for SQLite?

There is a great quick start guide (thanks to another thread here) that you can get here:
http://web.archive.org/web/20100208133236/http://www.mikeduncan.com/sqlite-on-dotnet-in-3-mins/


Something like this came to my mind

select @rownum:=@rownum+1 rownum, entries.* 
from (select @rownum:=0) r, entries 
where uid = ? and rownum % 150 = 0


I don't have MySQL at my hand but maybe this will help ...

Toad for SQL Server does this nicely, if you're considering a commercial product.

@Michal

For whatever reason, your example only works when the where @recnum uses a less than operator.  I think when the where filters out a row, the rownum doesn't get incremented, and it can't match anything else.

If the original table has an auto incremented id column, and rows were inserted in chronological order, then this should work:

select timefield from entries
where uid = ? and id % 150 = 0 order by timefield;


Of course that doesn't work if there is no correlation between the id and the timefield, unless you don't actually care about getting evenly spaced timefields, just 20 random ones.


The more the better. As programming languages start to become more complex and abstract, the more processing power that will be required.

Atleat Jeff believes Quadcore is better.

We use apache on linux, which forks a process to handle requests.  We've found that more cores help our throughput, since they reduce the latency of processes waiting to be placed on the run queue.  I don't have much experience with IIS, but I imagine the same scenario applies with its thread pool.

For something like a webserver, dividing up the tasks of handling each connection is (relatively) easy.  I say it's safe to say that web servers is one of the most common (and ironed out) uses of parallel code.  And since you are able to split up much of the processing into multiple discrete threads, more cores actually does benefit you.  This is one of the big reasons why shared hosting is even possible.  If server software like IIS and Apache couldn't run requests in parallel it would mean that every page request would have to be dished out in a queue fashion...likely making load times unbearably slow.

This also why high end server Operating Systems like Windows 2008 Server Enterprise support something like 64 cores and 2TB of RAM.  These are applications that can actually take advantage of that many cores.

Also, since each request is likely has low CPU load, you can probably (for some applications) get away with more slower cores.  But obviously having each core faster can mean being able to get each task done quicker and, in theory, handle more tasks and more server requests.

Mark Harrison said:


  I don't have much experience with IIS, but I imagine the same scenario applies with its thread pool.


Indeed - more cores = more threads running concurrently.  IIS is inherently multithreaded, and takes easy advantage of this.

Are there any built-in audit packages?  Oracle has a nice package, which will even send audit changes off to a separate server outside the access of any bad guy who is modifying the SQL.

Their example is awesome... it shows how to alert on anybody modifying the audit tables.

How much writing vs. reading of this table(s) do you expect?

I've used a single audit table, with columns for Table, Column, OldValue, NewValue, User, and ChangeDateTime - generic enough to work with any other changes in the DB, and while a LOT of data got written to that table, reports on that data were sparse enough that they could be run at low-use periods of the day.

Added:
If the amount of data vs. reporting is a concern, the audit table could be replicated to a read-only database server, allowing you to run reports whenever necessary without bogging down the master server from doing their work.

Can you do it as a subselect, something like

select * from (    SELECT A,B,C FROM table WHERE field LIKE 'query%'    UNION    SELECT A,B,C FROM table WHERE field LIKE '%query'    UNION    SELECT A,B,C FROM table WHERE field LIKE '%query%') ORDER BY B ASC LIMIT 5

If there isn't a sort that makes sense to order them you desire, don't union the results together - just return 3 separate recordsets, and deal with them accordingly in your data tier.

Add an additional column with hard-coded values that you will use to sort the overall resultset, like so:

SELECT A,B,C,1 as [order] FROM table WHERE field LIKE 'query%'UNIONSELECT A,B,C,2 as [order] FROM table WHERE field LIKE '%query'UNIONSELECT A,B,C,3 as [order] FROM table WHERE field LIKE '%query%'GROUP BY B ORDER BY [order] ASC, B ASC LIMIT 5

Maybe you should try including a fourth column, stating the table it came from, and then order and group by it:

SELECT A,B,C, "query 1" as origin FROM table WHERE field LIKE 'query%'UNIONSELECT A,B,C, "query 2" as origin FROM table WHERE field LIKE '%query'UNIONSELECT A,B,C, "query 3" as origin FROM table WHERE field LIKE '%query%'GROUP BY origin, B ORDER BY origin, B ASC LIMIT 5

Try this:

var request = new XMLHttpRequest();request.overrideMimeType( 'text/xml' );request.onreadystatechange = process;request.open ( 'GET', url );request.send( null );function process() {     if ( request.readyState == 4 &amp;&amp; request.status == 200 ) {        var xml = request.responseXML;    }}

Notice the overrideMimeType and responseXML.  The readyState == 4 is 'completed'.

I eventually (looking at all suggestions) came to this solution, its a bit of a compromise between what I need and time.

SELECT * FROM 
  (SELECT A, B, C, "1" FROM table WHERE B LIKE 'query%' LIMIT 3
   UNION
   SELECT A, B, C, "2" FROM table WHERE B LIKE '%query%' LIMIT 5)
AS RS
GROUP BY B
ORDER BY 1 DESC


it delivers 5 results total, sorts from the fourth "column" and gives me what I need; a natural result set (its coming over AJAX), and a wildcard result set following right after.

:)

/mp


Multiply by 100 and then convert to an int.

This seems like a buisness requirements/usability issue - do you have a good reason for putting the map on the credit card page? If so, maybe it's worth working through some technical problems.

You might try using Mapstraction, so you can switch to a provider that supports SSL, and switch back to Google if they support it in the future.

Could you be a little more specific about the use case? Removing the decimal point from the representation is a little unusual given that you'll lose all information about the scale. Are you assuming that there will always be two digits? If so, you could simplify multiply by 100 and then round before converting to a string.

SELECT distinct a,b,c  FROM (    SELECT A,B,C,1 as o FROM table WHERE field LIKE 'query%'    UNION    SELECT A,B,C,2 as o FROM table WHERE field LIKE '%query'    UNION    SELECT A,B,C,3 as o FROM table WHERE field LIKE '%query%')ORDER BY o ASC LIMIT 5

Would be my way of doing it. I dont know how that scales.

I don't understand the

GROUP BY B ORDER BY B ASC LIMIT 5

Does it apply only to the last SELECT in the union? 

Does mysql actually allow you to group by a column and still not do aggregates on the other columns?

EDIT: aaahh. I see that mysql actually does. Its a special version of DISTINCT(b) or something. I wouldnt want to try to be an expert on that area :)

Looking into the registry is perfectly valid, so long as you can be sure that the user of the application will always have access to what you need.

Take a look at Raymond Chens solution:

How to detect programmatically whether you are running on 64-bit Windows

and here's the PINVOKE for .NET:

IsWow64Process (kernel32)

Update: I'd take issue with checking for 'x86'. Who's to say what intel's or AMD's next 32 bit processor may be designated as. The probability is low but it is a risk. You should ask the OS to determine this via the correct API's, not by querying what could be a OS version/platform specific value that may be considered opaque to the outside world. Ask yourself the questions, 1 - is the registry entry concerned properly documented by MS, 2 - If it is do they provide a definitive list of possible values that is guaranteed to permit you as a developer to make the informed decision between whether you are running 32 bit or 64 bit. If the answer is no, then call the API's, yeah it's a but more long winded but it is documented and definitive. 


OmniAudit might be a good solution for you need.  I've never used it before because I'm quite happy writing my own audit routines, but it sounds good.

Try creating a div

document.createElement( 'div' );

And then set the tag soup HTML to the innerHTML of the div. The browser should process that into XML, which then you can parse.


  The innerHTML property takes a string
  that specifies a valid combination of
  text and elements. When the innerHTML
  property is set, the given string
  completely replaces the existing
  content of the object. If the string
  contains HTML tags, the string is
  parsed and formatted as it is placed
  into the document.


The easiest way to test for 64-bit under .NET is to check the value of IntPtr.Size.
EDIT: Doh! This will tell you whether or not the current process is 64-bit, not the OS as a whole. Sorry!


The easiest way to test for 64-bit under .NET is to check the value of IntPtr.Size.
I believe the value of IntPtr.Size is 4 for a 32bit app that's running under WOW, isn't it?
Edit: @Edit: Yeah. :)

So you want to download a webpage as an XML object using javascript, but you don't want to use a webpage? Since you have no control over what the user will do (closing tabs or windows or whatnot) you would need to do this in like a OSX Dashboard widget or some separate application. A Firefox extension would also work, unless you have to worry about the user closing the browser.

I use the approach described by Greg in his answer and populate the audit table with a stored procedure called from the table triggers. 

nProf is a free .Net profiler (ref). 

nProf is a good, free tool for .Net Profiling.

Ajaxian actually had a post on inserting / retrieving html from an iframe today. You can probably use the js snippet they have posted there.
As for handling closing of a browser / tab, you can attach to the onbeforeunload (http://msdn.microsoft.com/en-us/library/ms536907(VS.85).aspx) event and do whatever you need to do.

Keep in mind that the money data type can have up to 4 digits past the decimal.  Values with more than two digits might not work as expected for either your original solution or the x100 trick.

In my opinion, it is better to make the version number part of the file itself.  e.g. myscript.1.2.3.js  You can set your webserver to cache this file forever, and just add a new js file when you have a new version.

Yes resources are still the best way to support multiple languages in the .NET environment.  Because they are easy to reference and even easier to add new languages.

Site.resxSite.en.resxSite.en-US.resxSite.fr.resxetc...

So you are right still use the resource files.

You can also cache in memory which is much more efficient. Try memcached.

I don't have it entirely implemented yet, but this web site seems to give a good walkthrough of setting up the certificates and the code.

If you are talking about MS SQL Server tables, I like the diagram support in SQL Server Management Studio. You just drag the tables from the explorer onto the canvas, and they are laid out for you along with lines for relationships. You'll have to do some adjusting by hand for the best looking diagrams, but it is a decent way to get diagrams.

I am a big fan of Embarcadero's ER/Studio.  It is very powerful and produces excellent on-screen as well as printed results.  They have a free trial as well, so you should be able to get in and give it a shot without too much strife.

Good luck!


Toad Data Modeller from Quest does a nice job on this and is reasonably priced.  Embarcadero E/R studio is good too, as Bruce mentioned.

TeamCity watches the command line output from the build.  You can let it know how your tests are going by inserting certain markers into that output See http://www.jetbrains.net/confluence/display/TCD3/Build+Script+Interaction+with+TeamCity.  For example 

##teamcity[testSuiteStarted name='Test1']


will let TeamCity know that a set of tests started.  With MbUnit you can't output these markers while the tests are running, but you can transform the XML file that it outputs.  Here is the XSL that I am using:

&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;xsl:stylesheet version="1.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform"&gt;
    &lt;xsl:output method="text"/&gt;
    &lt;xsl:template match="/"&gt;

        &lt;xsl:apply-templates/&gt;

    &lt;/xsl:template&gt;

    &lt;xsl:template match="assemblies/assembly"&gt;
##teamcity[testSuiteStarted name='&lt;xsl:value-of select="@name" /&gt;']

        &lt;xsl:apply-templates select="//run" /&gt;

##teamcity[testSuiteFinished name='&lt;xsl:value-of select="@name" /&gt;']
    &lt;/xsl:template&gt;

    &lt;xsl:template match="run"&gt;

        &lt;xsl:choose&gt;
            &lt;xsl:when test="@result='ignore' or @result='skip'"&gt;
        ##teamcity[testIgnored name='&lt;xsl:value-of select="@name" /&gt;' message='Test Ignored']
            &lt;/xsl:when&gt;
            &lt;xsl:otherwise&gt;
        ##teamcity[testStarted name='&lt;xsl:value-of select="@name" /&gt;']
            &lt;/xsl:otherwise&gt;
        &lt;/xsl:choose&gt;


        &lt;xsl:if test="@result='failure'"&gt;
            ##teamcity[testFailed name='&lt;xsl:value-of select="@name" /&gt;' message='&lt;xsl:value-of select="child::node()/message"/&gt;' details='&lt;xsl:value-of select="normalize-space(child::node()/stack-trace)"/&gt;']
        &lt;/xsl:if&gt;


        &lt;xsl:if test="@result!='ignore' and @result!='skip'"&gt;
        ##teamcity[testFinished name='&lt;xsl:value-of select="@name" /&gt;']
        &lt;/xsl:if&gt;

    &lt;/xsl:template&gt;

&lt;/xsl:stylesheet&gt;



You could also look at either indexing the files yourself, or getting a third part app like google desktop or copernic to do it and then interface with their index. I know copernic has an API that you can use to search for any file in their index and it also supports mapping network drives.

What about checking out Microsoft Velocity?
Another option if you don't want to start using Microsoft CTP-ware is to check out Nache which allows distributed cache/session state management


OP asked about diagramming views and view dependencies, SQL Management Studio and Enterprise Manager doesn't allow you to diagram views. I can't vouch for the other tools.
The LINQ to SQL designer for Visual Studio does allow you to drop views on the design surface but there isn't a easy way to model the dependencies between the views. I'm not sure which tool has this type of diagramming functionality. You could take a look at Red Gate's SQLDoc tool but it just provides text based output.

http://www.danga.com/memcached/

worked awesome for me and have heard nothing but goodness about it

I would expect tools that need this simply walk the tree in a depth-first manner and when they hit a leaf, just process it (e.g. compile) and remove it from the graph (or mark it as processed, and treat nodes with all leaves processed as leaves).

As long as it's a DAG, this simple stack-based walk should be trivial.

I've come up with a fairly naive recursive algorithm (pseudocode):

Map&lt;Object, List&lt;Object&gt;&gt; source; // map of each object to its dependency listList&lt;Object&gt; dest; // destination listfunction resolve(a):    if (dest.contains(a)) return;    foreach (b in source[a]):        resolve(b);    dest.add(a);foreach (a in source):    resolve(a);

The biggest problem with this is that it has no ability to detect cyclic dependencies - it can go into infinite recursion (ie stack overflow ;-p). The only way around that that I can see would be to flip the recursive algorithm into an interative one with a manual stack, and manually check the stack for repeated elements.

Anyone have something better?

AdBlock forum says this is used to detect AdBlock. After some tweaking you could use this to gather some statistics.

&lt;script language="JavaScript" type="text/JavaScript"&gt;setTimeout('detect_abp()', 10000);var isFF = (navigator.userAgent.indexOf("Firefox") &gt; -1) ? true : false;var hasABP = false;function detect_abp(){   if(isFF)   {         if(Components.interfaces.nsIAdblockPlus != undefined)        {           hasABP = true;        }        else        {          var AbpImage = document.createElement("IMG");          AbpImage.id = 'abp_detector';          AbpImage.src = '/textlink-ads.jpg';            AbpImage.style.width = '0px';          AbpImage.style.height = '0px';          AbpImage.style.top = '-1000px';          AbpImage.style.left = '-1000px';          document.body.appendChild(AbpImage);          hasABP = (document.getElementById('abp_detector').style.display == 'none');             var e = document.getElementsByTagName("iframe");            for (var i = 0; i &lt; e.length; i++)            {               if(e[i].clientHeight == 0)                {                    hasABP = true;                }            }              if(hasABP == true)              {               history.go(1);                location = "http://www.tweaktown.com/supportus.html";              window.location(location);                    }        }      }    }    &lt;/script&gt; 

You can do this with any data type. Simply make it a pointer-to-pointer:

typedef struct {
  int myint;
  char* mystring;
} data;

data** array;


But don't forget you still have to malloc the variable, and it does get a bit complex:

//initialize
int x,y,w,h;
w = 10; //width of array
h = 20; //height of array

//malloc the 'y' dimension
array = malloc(sizeof(data*) * h);

//iterate over 'y' dimension
for(y=0;y&lt;h;y++){
  //malloc the 'x' dimension
  array[y] = malloc(sizeof(data) * w);

  //iterate over the 'x' dimension
  for(x=0;x&lt;w;x++){
    //malloc the string in the data structure
    array[y][x].mystring = malloc(50); //50 chars

    //initialize
    array[y][x].myint = 6;
    strcpy(array[y][x].mystring, "w00t");
  }
}


The code to deallocate the structure looks similar - don't forget to call free() on everything you malloced! (Also, in robust applications you should check the return of malloc().)

Now let's say you want to pass this to a function. You can still use the double pointer, because you probably want to do manipulations on the data structure, not the pointer to pointers of data structures:

int whatsMyInt(data** arrayPtr, int x, int y){
  return arrayPtr[y][x].myint;
}


Call this function with:

printf("My int is %d.\n", whatsMyInt(array, 2, 4));


Output:

My int is 6.



Dare Obasanjo has a pretty good blog post about this topic. You really need to assess what it is you're caching, why you're caching it and what your needs are before you can make a decision on a caching strategy.

PrcView seems to work off the command line as well:

http://www.teamcti.com/pview/prcview.htm

(Check the -ph parameter)

Maybe you want to explicit more precisely what you want to cache. You have all this opportunities to cache: 


Accessing the Data Base where you cache the data first correctly tuning your RDBMS, then using a layer to delegate the decision to detect multiple queries for the same data (with AdoDB for example.)
Extracting calculations from loops in the code so you don't compute the same value multiple times. Here your third way: storing results in the session for the user.
Precompiling the PHP code with an extension like APC Cache. This way you don't have to compile the same PHP code for every request.
The page sent to the user making sure you're setting the right META tags (do a good thing for the world and don't use ETL at least absolutly necessary); or maybe making dynamic pages completely static (having a batch process that generates .html pages); or by using a proxy cache like Squid.
Prefetching and by this I refer all those opportunities you have to improve the user experience just by doing things while the user don't look your way. For example, preloading IMG tags in the HTML file, tunning the RDBMS for prefectching, precomputing results storing complex computations in the database, etc. 


From my experience, I'd bet you that your code can be improved a lot before we start to talk about caching things. Consider, for example, how well structured is the navigation of your site and how well you control the user experience. Then check your code with a tool like XDebug. 

Verify also how well are you making your SQL queries and how well are you indexing your tables. Then check your code again to look for opportunities to apply the rule "read many times but write just once"

Use a simple tool like YSlow to hint other simple things to improve. Check your code again looking for opportunities to put logic in the browser (via JavaScript)

Seconding memcached, does the simple stuff well and can go distributive and all that jazz if you need it too

I know that if you use a paging repeater or gridview with the linqdatasource it will automatically optimize the number of results returned, but I'm also pretty sure in the datasource wizard you can go to advanced options and limit it to 

SELECT TOP 3 FROM 


which should allow you to do what you need


I appreciate the desire to find free software. However, in this case, I would strongly recommend looking at all options, including commercial products. I tried to play with nProf (which is at version 0.1 I think) and didn't have much luck. Even so, performance profiling an application is a subtle business and is best approached using a powerful, flexible tool. Unless you are working for free, I strongly believe the time you will save using a professional product will far outweigh the cost of a license. And of course, if you are only wanting to profile a single application, each commercial package has a 15 or 30 day trial, more than enough time to pinpoint any issues in an existing application. And if you need profiling support for more than just the one-off project, you're better buying a full strength tool anyway. 

We use the ANTS profiler from RedGate and have been very happy with it. I have also used .NET Memory Profiler with excellent results. The cool thing about .NET Memory Profiler is that it can attach to and profile running production applications, which really saved our butts when we had a memory leak in production we couldn't reproduce in our test lab.

The JetBrains folks have a profiler as well called dotTrace which I haven't tried, but I have to believe that if it comes from the JetBrains shop it is probably top notch as well.

Anyway, my advice is this: try to fix your app within the free trial window of one or an aggregated combination of the three of them (minimum of 45 days free use) and if that isn't enough time, pick your favorite and spring for one of them. You won't be sorry.

The DataGrid was originally in .NET 1.0.  The GridView was introduced (and replaced the DataGrid) in .NET 2.0.  They provide nearly identical functionality.

DataGrid was an ASP.NET 1.1 control, still supported. GridView arrived in 2.0, made certain tasks simpler added different databinding features:

This link has a comparison of DataGrid and GridView features -

https://msdn.microsoft.com/en-us/library/05yye6k9(v=vs.100).aspx


ActiveMQ works well with C# using the Spring.NET integrations and NMS. A post with some links to get you started in that direction is here. Also consider using MSMQ (The System.Messaging namespace) or a .NET based asynchronous messaging solution, with some options here.

Maybe you want to consider using ProcessTamer that "automatize" the process of downgrading or upgrading process priority based in your settings. 

I've been using it for two years. It's very simple but really effective! 

In Ruby 1.9:

weights.zip(data).map{|a,b| a*b}.reduce(:+)

In Ruby 1.8:

weights.zip(data).inject(0) {|sum,(w,d)| sum + w*d }

@Michiel de Mare

Your Ruby 1.9 example can be shortened a bit further:

weights.zip(data).map(:*).reduce(:+)

Also note that in Ruby 1.8, if you require ActiveSupport (from Rails) you can use:

weights.zip(data).map(&amp;:*).reduce(&amp;:+)

If you're running SQL 2005 you could do this in a CLR integration assembly and use the FTP classes in the System.Net namespace to build a simple FTP client.
You'd benefit from being able to trap and handle exceptions and reduce the security risk of having to use xp_cmdshell.
Just some thoughts.

Why not use a map of primitives (triangles, squares), distribute the starting points for the countries (the "capitals"), and then randomly expanding the countries by adding a random adjacent primitive to the country.

The best reference I've seen on them is Computational Geometry: Algorithms and Applications, which covers Voronoi diagrams, Delaunay triangulations (similar to Voronoi diagrams and each can be converted into the other), and other similar data structures. 

They talk about all the data structures you need but they don't give you the code necessary to implement it (which may be a good exercise). In terms of code, an Amazon search shows the book Computational Geometry in C, which presumably comes with the code (although since you're stuck in C, you'd mind as well get the other one and implement it in whatever language you want). I also don't have any experience with this book, only the first.

Sorry to have only books to recommend! The only decent online resource I've seen on them are the two Wikipedia articles, which doesn't really tell you implementation details. This link may be helpful though.

If you need to do FTP from within the database, then I would go with a .NET assembly as Kevin suggested.  That would provide the most control over the process, plus you would be able to log meaningful error messages to a table for reporting.

Another option would be to write a command line app that read the database for commands to run. You could then define a scheduled task to call that command line app every minutes or whatever the polling period needed to be.  That would be more secure than enabling CLR support in the database server.

Sorry if this isn't what you are asking for...
Have you considered some sort of cache behind the scenes that acts a bit like the "bucket system" when using asynchronous sockets in c/c++ using winsock?  Basicly, it works by accepting requests, and sends an immediate response back to the web app, and when it finally gets around to finding your record, it updates it on the app via AJAX or any other technology of your choice.  Since I'm not a C# programmer I can't provide any specific example.  Hope this helps!

Honestly the ASP.NET Membership / Roles features would work perfectly for the scenario you described. Writing your own tables / procs / classes is a great exercise and you can get very nice control over minute details, but after doing this myself I've concluded it's better to just use the built in .NET stuff. A lot of existing code is designed to work around it which is nice at well. Writing from scratch took me about 2 weeks and it was no where near as robust as .NETs. You have to code so much crap (password recovery, auto lockout, encryption, roles, a permission interface, tons of procs, etc) and the time could be better spent elsewhere.

Sorry if I didn't answer your question, I'm like the guy who says to learn c# when someone asks a vb question.

If you use PowerShell, you could write a script that let you change the priority of a process.  I found the following PowerShell function on the Monad blog:

function set-ProcessPriority {     param($processName = $(throw "Enter process name"), $priority = "Normal")    get-process -processname $processname | foreach { $_.PriorityClass = $priority }    write-host "`"$($processName)`"'s priority is set to `"$($priority)`""}

From the PowerShell prompt, you would do something line:

set-ProcessPriority SomeProcessName "High"

You should pass self() to the child as one of the arguments to the entry function.

spawn_link(?MODULE, child, [self()]).

Try gogrid.com they seem to have a very nice following in the cloud computing circles.

The benefits part has recently been covered, as for where to start....on a small enterprisey system where there aren't too many unknowns so the risks are low. If you don't already know a testing framework (like NUnit), start by learning that. Otherwise start by writing your first test :)

In my opinion, the single greatest thing is that it clearly allows you to see if your code does what it is supposed to.  This may seem obvious, but it is super easy to run astray of your original goals, as I have found out in the past :p

There are a lot of benefits:


You get immediate feedback on if your code is working, so you can find bugs faster
By seeing the test go from red to green, you know that you have both a working regression test, and working code
You gain confidence to refactor existing code, which means you can clean up code without worrying what it might break
At the end you have a suite of regression tests that can be run during automated builds to give you greater confidence that your codebase is solid


The best way to start is to just start.  There is a great book by Kent Beck all about Test Driven Development.  Just start with new code, don't worry about old code... whenever you feel you need to refactor some code, write a test for the existing functionality, then refactor it and make sure the tests stay green.  Also, read this great article.


Benefits


You figure out how to compartmentalize your code
You figure out exactly what you want your code to do
You know how it supposed to act and, down the road, if refactoring breaks anything
Gets you in the habit of making sure your code always knows what it is supposed to do


Getting Started

Just do it. Write a test case for what you want to do, and then write code that should pass the test. If you pass your test, great, you can move on to writing cases where your code will always fail (2+2 should not equal 5, for example).

Once all of your tests pass, write your actual business logic to do whatever you want to do. 

If you are starting from scratch make sure you find a good testing suite that is easy to use. I like PHP so PHPUnit or SimpleTest work well. Almost all of the popular languages have some xUnit testing suite available to help build and automate testing.

Try the following

var names = (from dr in dataTable.Rows
             select (string)dr["Name"]).Distinct().OrderBy(name =&gt; name);


this should work for what you need.


Like everything else it is environmental and depends on the use of the system.  The question you need to ask your self is:


Will this be actively developed
Is this going to be used over the course of many years and expanded on
Is the expansion of the application unknown and thus infinite


Really it comes down to laziness.  How much time to do you want to spend reworking the system from the UI?  Because having no business layer means duplication of rules in your UI across possibility many many pages.

Then again if this is a proof of concept or short demo or class project.  Take the easy way out.

It's acceptable as long as you understand the consequences. The main reason you'd have a BLL is to re-use that logic elsewhere throughout your application.

If you have all that validation logic in the presentation code, you're really making it difficult to re-use elsewhere within your application.

If you want to set priority when launching a process you could use the built-in start command:

START ["title"] [/Dpath] [/I] [/MIN] [/MAX] [/SEPARATE | /SHARED]
      [/LOW | /NORMAL | /HIGH | /REALTIME | /ABOVENORMAL | /BELOWNORMAL]
      [/WAIT] [/B] [command/program] [parameters]

Use the low through belownormal options to set priority of the launched command/program. Seems like the most straightforward solution. No downloads or script writing. The other solutions probably work on already running procs though.


Acceptable? Depends who you ask and what your requirements are. Is this app an internal one-off used by you and a few other people? Maybe this is good enough. If it's meant to be a production ready enterprise application that will grow and be maintained over the years, then you probably want to invest more effort up-front to build a maintainable app.
Separation of Concerns is a key design technique for building maintainable apps. By mixing presentation, business, and data access logic all together, you can end up with a very fragile difficult to change application architecture.

Without trying to sound too vague but I think Windows Network Load Balancing (NLB) should handle this for you.

If you're using the default view engines, then local resources work in the views. However, if you need to grab resource strings within a controller action, you can't get local resources, and have to use global resources.
This makes sense when you think about it because local resources are local to an aspx page and in the controller, you haven't even selected your view.

You can use the BIF register to give the spawning / parent process a name (an atom) then refer back to the registered name from other processes.


  FUNC() -&gt;
  
  
    %% Do something 
    %% Then send message to parent 
    parent ! MESSAGE.
  
  
  ...
  
  register(parent, self()), 
  spawn(MODULE, FUNC, [ARGS]).


See Getting Started With Erlang §3.3 and The Erlang Reference Manual §10.3.


  Stop the MySQL process.
  
  Start the MySQL process with the --skip-grant-tables option.
  
  Start the MySQL console client with the -u root option.


List all the users;

SELECT * FROM mysql.user;

Reset password;

UPDATE mysql.user SET Password=PASSWORD('[password]') WHERE User='[username]';



But DO NOT FORGET to


  Stop the MySQL process  
  
  Start the MySQL Process normally (i.e. without the --skip-grant-tables option)


when you are finished.  Otherwise, your database's security could be compromised.

Boy, that's a pretty general question.  I'll do my best, but be prepared to see me miss by a mile.

Assumptions


You are using ASP.NET, not plain ASP
You don't really want to test your web pages, but the logic behind them. Unit testing the actual .ASPX pages is rather painful, but there are frameworks out there to do it.  NUnitAsp is one.


The first thing to do is to organize (or plan) your code so that it can be tested.  The two most popular design patterns for this at the time seem to be MVP and MVC.  Both separate the logic of the application away from the view so that you can test the logic without the view (web pages) getting in your way.

Either MVP or MVC will be effective.  MVC has the advantage of having a Microsoft framework almost ready to go.  

Once you've selected a framework pattern that encourages testability, you need to use a unit testing tool.  NUnit is a good starting point.  Visual Studio Professional has a testing suite built it, but NUnit + TestDrive.NET also works in the IDE.

That's sort of a shotgun blast of information.  I hope some if it hits.  The Pragmatic Bookshelf has a good book covering the topic.

http://www.stack.nl/~dimitri/doxygen/ is probably a good place to start.

From the site: "Doxygen is a documentation system for C++, C, Java, Objective-C, Python, IDL (Corba and Microsoft flavors), Fortran, VHDL, PHP, C#, and to some extent D."

For JavaScript, there's http://jsdoc.sourceforge.net/, but it's not multi-language.

For VB.NET, there's http://vbdox.sourceforge.net/, also not multi-language.

I really like Doxygen. It works for "C++, C, Java, Objective-C, Python, IDL (Corba and Microsoft flavors), Fortran, VHDL, PHP, C#, and to some extent D."

It supports several output formats, and it will also generate hierarchical and "call" charts, using the dot program.

There was a screencast series a year or so ago on Polymorphic Podcast that did a pretty good intro walkthrough of an MVP implementation in ASP.NET. Implemented this way, unit tests fall into place much more naturally.
http://polymorphicpodcast.com/shows/mv-patterns/

Unfortunately your user password is irretrievable. It has been hashed with a one way hash which if you don't know is irreversible. I recommend go with Xenph Yan above and just create an new one.

You can also use the following procedure from the manual for resetting the password for any MySQL root accounts on Windows:  


Log on to your system as Administrator.  
Stop the MySQL server if it is running. For a server that is running as a Windows service, go to
the Services manager:          



  Start Menu -> Control Panel -> Administrative Tools -> Services


Then find the MySQL service in the list, and stop it. If your server is
not running as a service, you may need to use the Task Manager to force it to stop.


Create a text file and place the following statements in it. Replace the password with the password that you want to use.

UPDATE mysql.user SET Password=PASSWORD('MyNewPass') WHERE User='root';
FLUSH PRIVILEGES;


The UPDATE and FLUSH statements each must be written on a single line. The UPDATE statement resets the password for all existing root accounts, and the FLUSH statement tells the server to reload the grant tables into memory.

  Save the file. For this example, the file will be named C:\mysql-init.txt.
Open a console window to get to the command prompt:      


  Start Menu -> Run -> cmd

Start the MySQL server with the special --init-file option:

C:\&gt; C:\mysql\bin\mysqld-nt --init-file = C:\mysql-init.txt


If you installed MySQL to a location other than C:\mysql, adjust the command accordingly.

The server executes the contents of the file named by the --init-file option at startup, changing each root account password.

You can also add the --console option to the command if you want server output to appear in the console window rather than in a log file.

If you installed MySQL using the MySQL Installation Wizard, you may need to specify a --defaults-file option:

C:\&gt; "C:\Program Files\MySQL\MySQL Server 5.0\bin\mysqld-nt.exe" --defaults-file="C:\Program Files\MySQL\MySQL Server 5.0\my.ini" --init-file=C:\mysql-init.txt


The appropriate --defaults-file setting can be found using the Services Manager:


  Start Menu -> Control Panel -> Administrative Tools -> Services


Find the MySQL service in the list, right-click on it, and choose the Properties option. The Path to executable field contains the --defaults-file setting.

  After the server has started successfully, delete C:\mysql-init.txt.

  Stop the MySQL server, then restart it in normal mode again. If you run the server as a service, start it from the Windows Services window. If you start the server manually, use whatever command you normally use.


You should now be able to connect to MySQL as root using the new password. 


I use Doxygen too. I even created a shortcut to document methods with ctrl+shift+d in VAX.


Keep in mind that TempData stores the form collection in session. If you don't like that behavior, you can implement the new ITempDataProvider interface and use some other mechanism for storing temp data. I wouldn't do that unless you know for a fact (via measurement and profiling) that the use of Session state is hurting you.

There are several good options, but I also cast a vote for Doxygen.


As others have said, Doxygen is probably your best bet.  It works across many different languages, and just as importantly, it works with a number of different commenting styles.

For example, in C# you can continue to use the xml documentation in order to get the intellisense benefits that come with it as well as the ability to also use other tools such as the sandcastle help file builder.  In other languages, you're still free to use more concise styles.

It can generate output in a number of different ways, including html, man page, ps, pdf (with LaTex), and a few others, I believe.

If you are looking to find out which dll's your target machine is missing then use depends.exe which used to come with MSDev, but can also be found here. Testing this on a few target machines should tell you which dll's you need to package with your application.

Well, those features are specific to a tool that you are using for development in those languages.

You wouldn't have those tools if (for example) you were using notepad to write code. So, maybe you should ask the question for the tool you are using.

For PHP: http://webservices.xml.com/pub/a/ws/2004/03/24/phpws.html

You should send a 304 if the client has explicitly stated that it may already have the page in its cache. This is called a conditional GET, which should include the if-modified-since header in the request.

Basically, this request header contains a date from which the client claims to have a cached copy. You should check if content has changed after this date and send a 304 if it hasn't.

See http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.25 for the related section in the RFC.

A few months back I wrote a blog post about  Fluent Interfaces and LINQ which used an Extension Method on IQueryable&lt;T&gt; and another class to provide the following natural way of paginating a LINQ collection.

var query = from i in ideas
            select i;
var pagedCollection = query.InPagesOf(10);
var pageOfIdeas = pagedCollection.Page(2);


You can get the code from the MSDN Code Gallery Page: Pipelines, Filters, Fluent API and LINQ to SQL.


Here's how I implemented it.  The code has been working for a bit more than a year and with multiple browsers, so I think it's pretty reliable.  This is based on RFC 2616 and by observing what and when the various browsers were sending.

Here's the pseudocode:

server_etag = gen_etag_for_this_file(myfile)
etag_from_browser = get_header("Etag")

if etag_from_browser does not exist:
    etag_from_browser = get_header("If-None-Match")
if the browser has quoted the etag:
    strip the quotes (e.g. "foo" --&gt; foo)

set server_etag into http header

if etag_from_browser matches server_etag
    send 304 return code to browser


Here's a snippet of my server logic that handles this.

/* the client should set either Etag or If-None-Match */
/* some clients quote the parm, strip quotes if so    */
mketag(etag, &amp;sb);

etagin = apr_table_get(r-&gt;headers_in, "Etag");
if (etagin == NULL)
    etagin = apr_table_get(r-&gt;headers_in, "If-None-Match");
if (etag != NULL &amp;&amp; etag[0] == '"') {
    int sl; 
    sl = strlen(etag);
    memmove(etag, etag+1, sl+1);
    etag[sl-2] = 0;
    logit(2,"etag=:%s:",etag);
}   
... 
apr_table_add(r-&gt;headers_out, "ETag", etag);
... 
if (etagin != NULL &amp;&amp; strcmp(etagin, etag) == 0) {
    /* if the etag matches, we return a 304 */
    rc = HTTP_NOT_MODIFIED;
}   


If you want some help with etag generation post another question and I'll dig out some code that does that as well.  HTH!

regarding cache-control:

You shouldn't have to worry about the cache-control when serving out, other than setting it to a reasonable value.  It's basically telling the browser and other downstream entities (such as a proxy) the maximum time that should elapse before timing out the cache.

I believe that this is perfectly possible. I am currently running both SQL Server 2000 and SQL Server 2005 on my development server while I transfer applications over.

The only thing you will have to do is create a new instance which isn't already being used by SQL Server 2005.

As with anything new, there will probably be some bugs, however, it should generally "just work".

An etag is an arbitrary string that the server sends to the client that the client will send back to the server the next time the file is requested.

The etag should be computable on the server based on the file.  Sort of like a checksum, but you might not want to checksum every file sending it out.

 server                client

        &lt;------------- request file foo

 file foo etag: "xyz"  --------&gt;

        &lt;------------- request file foo
                       etag: "xyz" (what the server just sent)

 (the etag is the same, so the server can send a 304)


I built up a string in the format "file inode number/datestamp/file size".  So, if a file is changed on the server after it has been served out to the client, the newly regenerated etag won't match if the client re-requests it.


char *mketag(char *s, struct stat *sb)
{
    sprintf(s, "%d/%d/%d", sb->st_ino, sb->st_mtime, sb->st_size);
    return s;
}



Does calling the method twice work?

Seems to me that your authentication is being approved after the content arrives, so then a second attempt now works because you have the correct cookies.

Yes this is possible.  You will have to create a named instance not used by another version of SQL Server as per the previous answer and version 3.5 of .Net installed. Works great!!

Here the list of prerequisites:


.NET Framework 3.5 SP1
Windows Installer 4.5
Windows PowerShell 1.0



From http://developer.yahoo.com/performance/rules.html#etags:


  By default, both Apache and IIS embed data in the ETag that dramatically reduces the odds of the validity test succeeding on web sites with multiple servers. 
  
  ...
  
  If you're not taking advantage of the flexible validation model that ETags provide, it's better to just remove the ETag altogether. 


Haven't have the exact problem, but I think you need to look at the xhost and make sure that the vpn remote is allowed to send data to the x server. 

This link might help:
http://www.straightrunning.com/XmingNotes/trouble.php

Topological Sort (From Wikipedia):


  In graph theory, a topological sort or
  topological ordering of a directed
  acyclic graph (DAG) is a linear
  ordering of its nodes in which each
  node comes before all nodes to which
  it has outbound edges. Every DAG has
  one or more topological sorts.


Pseudo code:

L ← Empty list where we put the sorted elements
Q ← Set of all nodes with no incoming edges
while Q is non-empty do
    remove a node n from Q
    insert n into L
    for each node m with an edge e from n to m do
        remove edge e from the graph
        if m has no other incoming edges then
            insert m into Q
if graph has edges then
    output error message (graph has a cycle)
else 
    output message (proposed topologically sorted order: L)



If I remember correctly, this matrix allows you to save the data to a file with compression.

If you read further down, you'll find the zig-zag pattern of data to read from that final matrix. The most important data are in the top left corner, and least important in the bottom right corner. As such, if you stop writing at some point and just consider the rest as 0's, even though they aren't, you'll get a lossy approximation of the image.

The number of values you throw away increases compression at the cost of image fidelity.

But I'm sure someone else can give you a better explanation.

I'm not completely familiar with the implications of Step 2 in your approach above, but if you're looking for a more robust OLAP solution, it might be worth your while to check out Mondrian, the open-source OLAP / Analysis services module of Pentaho. 

I dont think that Mondrian is better than SSAS but I do know that its free and you independently distribute it. It uses XMLA and its cube definition XML file is almost the same as SSAS.

You could also use a more OO approach:


Create a base class that does the error handling and calls an abstract method to perform the concrete work. (Template Method pattern)
Create concrete classes for each operation.


This has the advantage of naming each type of operation you perform and gives you a Command pattern - operations have been represented as objects.

You need to create an extension method, which requires .NET 3.5. The method needs to be static, in a static class. The first parameter of the method needs to be prefixed with "this" in the signature.

public static string MyMethod(this string input){    // do things}

You can then call it like

"asdfas".MyMethod();

You can't dynamically add methods to existing objects or classes in .NET, except by changing the source for that class.

You can, however, in C# 3.0, use extension methods, which look like new methods, but are compile-time magic.

To do this for your code:

public static class StringExtensions{    public static String trim(this String s)    {        return s.Trim();    }}

To use it:

String s = "  Test  ";s = s.trim();

This looks like a new method, but will compile the exact same way as this code:

String s = "  Test  ";s = StringExtensions.trim(s);

What exactly are you trying to accomplish? Perhaps there are better ways of doing what you want?

Using the 3.5 compiler you can use an Extension Method:

public static void Trim(this string s){  // implementation}

You can use this on a CLR 2.0 targeted project (3.5 compiler) by including this hack:

namespace System.Runtime.CompilerServices{  [AttributeUsage(AttributeTargets.Method | AttributeTargets.Class | AttributeTargets.Assembly)]  public sealed class ExtensionAttribute : Attribute  {  }}

Try adding a path.  The following code works for me:

&lt;?php

if ( !empty($_FILES['file']) ) {
    $from = $_FILES['file']['tmp_name'];
    $to = dirname(__FILE__).'/'.$_FILES['file']['name'];

    if( move_uploaded_file($from, $to) ){
        echo 'Success';   
    } else {
        echo 'Failure';   
    }

    header('Location: http://www.mywebsite.com/dump/');
    exit;
}
?&gt;


It sounds like you're talking about C#'s Extension Methods. You add functionality to existing classes by inserting the "this" keyword before the first parameter. The method has to be a static method in a static class. Strings in .NET already have a "Trim" method, so I'll use another example.public static class MyStringEtensions
{
    public static bool ContainsMabster(this string s)
    {
        return s.Contains("Mabster");
    }
}

So now every string has a tremendously useful ContainsMabster method, which I can use like this:if ("Why hello there, Mabster!".ContainsMabster()) { /* ... */ }

Note that you can also add extension methods to interfaces (eg IList), which means that any class implementing that interface will also pick up that new method.
Any extra parameters you declare in the extension method (after the first "this" parameter) are treated as normal parameters.

you might want to take a look at http://www.codeplex.com/nxl and http://www.codeplex.com/umbrella which are both extension method libraries. I personally haven't had a look at the source code but I'm sure the guys there would be able to give you some good pointers.

Sure, download the 3.5 redistributable, install it on the servre, and you're good to go. .NET versions can be installed side-by-side, so it won't disrupt any "legacy" apps.

http://www.microsoft.com/downloads/details.aspx?FamilyId=333325FD-AE52-4E35-B531-508D977D32A6&amp;displaylang=en

The version you are selecting in IIS is the version of the CLR to use. There are only two versions of the CLR. The .NET Framework 3.5 runs on CLR 2.0

Unfortunately, the statement .NET versions can be installed side-by-side, so it won't disrupt any "legacy" apps isn't entirely true. If you install 3.5, it requires 2.0 SP1, which can disrupt legacy applications that uses 2.0 and connects to Oracle database servers.


  if I install 3.5 and have IIS setup to use 2.0. I will be able to use 3.5 features?


Yes, that is correct. You have IIS set to 2.0 for both 2.0 and 3.5 sites, as they both run on the same CLR. 3.5 uses a different compile method than 2.0. This is declared in the web.config for the site. See this post for more details on this. But the setup in IIS for both 3.5 and 2.0 ASP.net sites is identical.

Yes they should, only leads to confusion otherwise.

You should do a CRC check on each file... from the wiki:

Cyclic redundancy check, a type of hash function used to produce a checksum, in order to detect errors in transmission or storage.

It produces an almost unique value based on the contents of the file.

I'd say yes.

First, it will be easier to find the actual code files by following down the namespaces (say, when somebody e-mails you a naked exception call stack). If you let your folders go out of sync with namespaces, finding files in big codebases becomes getting tiring.

Second, VS will generate new classes you create in folders with the same namespace of its parent folder structure. If you decide to swim against this, it will be just one more plumbing job to do daily when adding new files.

Of course, this goes without saying that one should be conservative about how deep xis folder/namespace hierarchy goes.

I think the standard, within .NET, is to try to do it when possible, but not to create unnecessarily deep structures just to adhere to it as a hard rule. None of my projects follow the namespace == structure rule 100% of the time, sometimes its just cleaner/better to break out from such rules.

In Java you don't have a choice. I'd call that a classic case of what works in theory vs what works in practice.

Also, note that if you use the built-in templates to add classes to a folder, it will by default be put in a namespace that reflects the folder hierarchy.

The classes will be easier to find and that alone should be reasons good enough.

The rules we follow are:


Project/assembly name is the same as the root namespace, except for the .dll ending
Only exception to the above rule is a project with a .Core ending, the .Core is stripped off
Folders equals namespaces
One type per file (class, struct, enum, delegate, etc.) makes it easy to find the right file



I would do something like an md5sum hash on the files and compare that to the known hashes from the release.  They will be more accurate than just date/time comparisons and should be able to be automated more.

The normal way is to compute a hash of the two files and compare that. MD5 and SHA1 are typical hash algorithms. md5sum should be installed by default on most unix type machines, and Wikipedia's md5sum article has links to some windows implementations.

I had nothing but problems with Xming. When I could get it to work it was extremely slow (this is over a VPN). IMO X is not designed to run over slow connections its too chatty. And by slow connection I mean anything less then a LAN connection.

My solution was to use x11vnc. It lets you access your existing X11 session through VNC. I just ssh into my box through the VPN and launch:

$ x11vnc -display :0


That way I can access everything I had opened during the day. Then when I don't I just exit (Ctrl-C) in the terminal to close x11vnc.


The only 100% way to figure out if two files are equal is to do a binary comparison of the two.

If you can live with the risk of false positives (ie. two files which aren't 100% identical but your code says they are), then the digest and checksum algorithms can be used to lessen the work, particularly if the files lives on two different machines with less than optimal bandwidth so that a binary comparison is infeasible.

The digest and checksum algorithms all have chances of false positives, but the exact chance varies with the algorithm. General rule is that the more crypto-made it is, and the more bits it outputs, the less chance of a false positive.

Even the CRC-32 algorithm is fairly good to use and it should be easy to find code examples on the internet that implements it.

If you only do a size/timestamp comparison then I'm sorry to say that this is easy to circumvent and won't actually give you much of a certainty that the files are the same or different.

It depends though, if you know that in your world, timestamps are kept, and only changed when the file is modified, then you can use it, otherwise it holds no guarantee.

Chances are it's either X authentication, the X server binding to an interface, or your DISPLAY variable. I don't use Xming myself but there are some general phenomenon to check for. One test you can do to manually verify the DISPLAY variable is correct is:


Start your VPN. Run ipconfig to be sure you have the two IP addresses you mentioned (your local IP and your VPN IP).
Start Xming. Run 'netstat -n' to see how it's binding to the interface. You should see something that either says localIP:6000 or VPNIP:6000. It may not be 6000 but chances are it will be something like that. If there's no VPNIP:6000 it may be binding only to your localIP or even 127.0.0.1. That will probably not work over the VPN. Check if there are some Xming settings to make it bind to other or all interfaces.
If you see VPNIP:6000 or something similar, take note of what it says and remote shell into your UNIX host (hopefully something like ssh, if not whatever you have to get a text terminal).
On the UNIX terminal type 'echo $DISPLAY'. If there is nothing displayed try 'export DISPLAY=VPNIP:0.0' where VPNIP is your VPN IP address and 0.0 is the port you saw in step 3 minus 6000 with .0 at the end (i.e. 6000 = 0.0, 6010 = 10.0).
On the UNIX host run something like 'xclock' or 'xterm' to see if it runs. The error message should be informative. It will tell you that it either couldn't connect to the host (a connectivity problem) or authentication failed (you'll need to coordinate Xauth on your host and local machine or Xhosts on your local machine).


Opening Xhosts (with + for all hosts or something similar) isn't too bad if you have a locally protected network and you're going over a VPN. Hopefully this will get you started tracking down the problem. Another option that is often useful as it works over a VPN or simple ssh connectivity is ssh tunneling or X11 forwarding over ssh. This simulates connectivity to the X server on your local box by redirecting a port on your UNIX host to the local port on your X server box. Your display will typically be something like localhost:10.0 for the local 6010 port.

X can be ornery to set up but it usually works great once you get the hang of it.

Hashing is very good. But the other, slightly lower tech alternative is to run a diff tool like WinMerge or TextWrangler and compare the two versions of each file. Boring and there's room for human error.

Best of all, use version control to ensure the files you're testing are the files you edited and the ones you're going to launch. We have checkout folders from our repo as the staging and live sites, so once you've committed the changes from your working copy, you can be 100% sure that the files you test, push to staging and then live are the same, because you just run "svn update" on each box and check the revision number.

Oh, and if you need to roll back in a hurry (it happens to us all sometime or another) you just run svn update again with the -r switch and go back to a previous revision virtually instantly.

With VMWare, there is the Virtual Machine Automation APIs (VIX API).  You can find the reference guide here.  It works with VMWare Server and WorkStation, but AFAIK it's not available for ESX Server.

From the main page for VIX:


  The VIX API allows you to write
  scripts and programs that automate
  virtual machine operations. The API is
  high-level, easy to use, and practical
  for both script writers and
  application programmers. It runs on
  VMware Server and Workstation
  products, both Windows and Linux.
  Bindings are provided for C, Perl, and
  COM (Visual Basic, VBscript, C#).


Try this:

System.Configuration.ConfigurationFileMap fileMap = new ConfigurationFileMap(strConfigPath); //Path to your config fileSystem.Configuration.Configuration configuration = System.Configuration.ConfigurationManager.OpenMappedMachineConfiguration(fileMap);

You may have better luck doing X11 Forwarding through SSH rather than fiddling with your DISPLAY variable directly.  X11 Forwarding with SSH is secure and uses the existing SSH connection to tunnel, so working through a VPN should be no problem.

Fortunately this is fairly straightforward with Xming.  If you open your connection from within Xming (e.g. the plink option) I believe it sets up X11 forwarding by default.  If you connect using another SSH client (e.g. PuTTY) then you simply need to enable X11 forwarding (e.g. 'ssh -X user@host').  In PuTTY the option is under Connection -&gt; SSH -&gt; X11 -&gt; click on 'Enable X11 Forwarding'.

Make sure Xming is running in the background on your laptop and do the standard X test, 'xclock'.  If you get a message like 'X connection to localhost:19.0 broken (explicit kill or server shutdown).' then Xming is most likely not running.

Also, make sure you're not explicitly setting your DISPLAY variable in any startup scripts; SSH will set up an alias (something like localhost:10 or in the example above localhost:19) for the X11 tunnel and automatically set DISPLAY to that value.  Overwriting DISPLAY will obviously mean you will no longer be pointing to the correct X11 tunnel.  The flip side of this is that other terminals that don't have SSH X11 Forwarding set can use the same DISPLAY value and take advantage of the tunnel.

I tend to prefer the PuTTY option but several of my coworkers use plink from within Xming.

I think each site that implements OpenID would have to build their software to allow multiple entries for your OpenID credentials.  However, just because a site doesn't allow you to create multiple entries doesn't mean you can't swap out OpenID suppliers.

How to turn your blog into an OpenID

STEP 1: Get an OpenID. There a lots of servers and services out there you can use. I use http://www.myopenid.com

STEP 2: Add these two lines to your blog's main template in-between the &lt;HEAD&gt;&lt;/HEAD&gt; tags at the top of your template. Most all blog engines support editing your template so this should be an easy and very possible thing to do.

Example:


&lt;link rel="openid.server" href="http://www.myopenid.com/server" /&gt;  
&lt;link rel="openid.delegate" href=http://YOURUSERNAME.myopenid.com/ /&gt;


This will let you use your domain/blog  as your OpenID.

Credits to Scott Hanselman and Simon Willison for these simple instructions.

Switch Your Supplier

Now that your OpenID points to your blog, you can update your link rel href's to point to a new supplier and all the places that you've tied your blog's OpenID will use the new supplier.


You have multiple groups in Facebook being the mainly one the SAP Group. 

If you are not totally tied to pure C and can use string.h there is strchr()
See here

strstr returns a pointer to the found character, so you could use pointer arithmetic:  (Note: this code not tested for its ability to compile, it's one step away from pseudocode.)

char * source = "test string";         /* assume source address is */
                                       /* 0x10 for example */
char * found = strstr( source, "in" ); /* should return 0x18 */
if (found != NULL)                     /* strstr returns NULL if item not found */
{
  int index = found - source;          /* index is 8 */
                                       /* source[8] gets you "i" */
}



EDIT: strchr is better only for one char. 
Pointer aritmetics says "Hellow!":

char *pos = strchr (myString, '#');
int pos = pos ? pos - myString : -1;


Important: strchr () returns NULL if no string is found


You can use strstr to accomplish what you want. Example:

char *a = "Hello World!";char *b = strstr(a, "World");int position = b - a;printf("the offset is %i\n", position);

This produces the result:

the offset is 6

I think that


  size_t strcspn ( const char * str1, const char * str2 );


is what you want. Here is an example pulled from here:

/* strcspn example */
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;

int main ()
{
  char str[] = "fcba73";
  char keys[] = "1234567890";
  int i;
  i = strcspn (str,keys);
  printf ("The first number in str is at position %d.\n",i+1);
  return 0;
}



  @prakesh
  
  As long as you associate all of them
  to the same email address, i would
  think it would lead you to same
  account.
  
  But whats your experience?


When I tried it out I got a whole new account with 0 rep and no steenkin badges. So at the moment SO does not allow multiple OpenID's to be associated with the one account

Without doing detailed analysis, I'd guess that it's faster because of the question marks. These allow the regular expression to be "lazy," and stop as soon as they have enough to match, rather than checking if the rest of the input matches.

I'm not entirely happy with this answer though, because this mostly applies to question marks after * or +. If I were more familiar with the input, it might make more sense to me.

(Also, for the code formatting, you can select all of your code and press Ctrl+K to have it add the four spaces required.)


My Environment: Fedora 8; WAS 6.1 (as installed with Rational Application Developer 7)

The documentation is very poor in this area and there is a dearth of practical examples.

Using the WebSphere Application Server (WAS) Ant tasks

To run as described here, you need to run them from your server profile bin directory using the ws_ant.sh or ws_ant.bat commands.

&lt;?xml version="1.0"?&gt;
&lt;project name="project" default="wasListApps" basedir="."&gt;
    &lt;description&gt;
        Script for listing installed apps.
        Example run from:
        /opt/IBM/SDP70/runtimes/base_v61/profiles/AppSrv01/bin
    &lt;/description&gt;

    &lt;property name="was_home"
        value="/opt/IBM/SDP70/runtimes/base_v61/"&gt;
    &lt;/property&gt;
    &lt;path id="was.runtime"&gt;
        &lt;fileset dir="${was_home}/lib"&gt;
            &lt;include name="**/*.jar" /&gt;
        &lt;/fileset&gt;
        &lt;fileset dir="${was_home}/plugins"&gt;
            &lt;include name="**/*.jar" /&gt;
        &lt;/fileset&gt;
    &lt;/path&gt;
    &lt;property name="was_cp" value="${toString:was.runtime}"&gt;&lt;/property&gt;
    &lt;property environment="env"&gt;&lt;/property&gt;

    &lt;target name="wasListApps"&gt;
        &lt;taskdef name="wsListApp"
            classname="com.ibm.websphere.ant.tasks.ListApplications"
            classpath="${was_cp}"&gt;
        &lt;/taskdef&gt;
        &lt;wsListApp wasHome="${was_home}" /&gt;
    &lt;/target&gt;

&lt;/project&gt;


Command:

./ws_ant.sh -buildfile ~/IBM/rationalsdp7.0/workspace/mywebappDeploy/applist.xml


A Deployment Script

&lt;?xml version="1.0"?&gt;
&lt;project name="project" default="default" basedir="."&gt;
&lt;description&gt;
Build/Deploy an EAR to WebSphere Application Server 6.1
&lt;/description&gt;

    &lt;property name="was_home" value="/opt/IBM/SDP70/runtimes/base_v61/" /&gt;
    &lt;path id="was.runtime"&gt;
        &lt;fileset dir="${was_home}/lib"&gt;
            &lt;include name="**/*.jar" /&gt;
        &lt;/fileset&gt;
        &lt;fileset dir="${was_home}/plugins"&gt;
            &lt;include name="**/*.jar" /&gt;
        &lt;/fileset&gt;
    &lt;/path&gt;
    &lt;property name="was_cp" value="${toString:was.runtime}" /&gt;
    &lt;property environment="env" /&gt;
    &lt;property name="ear" value="${env.HOME}/IBM/rationalsdp7.0/workspace/mywebappDeploy/mywebappEAR.ear" /&gt;

    &lt;target name="default" depends="deployEar"&gt;
    &lt;/target&gt;

    &lt;target name="generateWar" depends="compileWarClasses"&gt;
        &lt;jar destfile="mywebapp.war"&gt;
            &lt;fileset dir="../mywebapp/WebContent"&gt;
            &lt;/fileset&gt;
        &lt;/jar&gt;
    &lt;/target&gt;

    &lt;target name="compileWarClasses"&gt;
        &lt;echo message="was_cp=${was_cp}" /&gt;
        &lt;javac srcdir="../mywebapp/src" destdir="../mywebapp/WebContent/WEB-INF/classes" classpath="${was_cp}"&gt;
        &lt;/javac&gt;
    &lt;/target&gt;

    &lt;target name="generateEar" depends="generateWar"&gt;
        &lt;mkdir dir="./earbin/META-INF"/&gt;
        &lt;move file="mywebapp.war" todir="./earbin" /&gt;
        &lt;copy file="../mywebappEAR/META-INF/application.xml" todir="./earbin/META-INF" /&gt;
        &lt;jar destfile="${ear}"&gt;
            &lt;fileset dir="./earbin" /&gt;
        &lt;/jar&gt;
    &lt;/target&gt;

    &lt;!-- http://publib.boulder.ibm.com/infocenter/wasinfo/v6r1/index.jsp?topic=/com.ibm.websphere.javadoc.doc/public_html/api/com/ibm/websphere/ant/tasks/package-summary.html --&gt;
    &lt;target name="deployEar" depends="generateEar"&gt;
        &lt;taskdef name="wsInstallApp" classname="com.ibm.websphere.ant.tasks.InstallApplication" classpath="${was_cp}"/&gt;
        &lt;wsInstallApp ear="${ear}" 
            failonerror="true" 
            debug="true" 
            taskname=""
            washome="${was_home}" /&gt;
    &lt;/target&gt;

&lt;/project&gt;


Notes:


You can only run this once! You cannot install if the app name is in use - see other tasks like wsUninstallApp
It probably won't start the app either
You need to run this on the server and the script is quite fragile


Alternatives

I would probably use Java Management Extensions (JMX). You could write a file-upload servlet that accepts an EAR and uses the deployment MBeans to deploy the EAR on the server. You would just POST the file over HTTP. This would avoid any WAS API dependencies on your dev/build machine and could be independent of any one project.


If you're already shuffling the ViewState around anyway, you might as well use an UpdatePanel.  Its partial postbacks will update the page's ViewState automatically.

The reason why #1 is slower is that [\d;]+ is a greedy quantifier. Using +? or *? is going to do lazy quantifing. See MSDN - Quantifiers for more info.

You may want to try:

"(\e\[(\d{1,2};)*?[mz]?)?"

That may be faster for you.

Here's what I would do.


Compile your application to a SWF file. Then encrypt the SWF using AES.
Make a "wrapper" application that loads the encrypted SWF into a ByteArray using URLLoader
Use the as3crypto library to decrypt the swf at runtime.
Once decrypted, use Loader.loadBytes to load the decrypted swf into the wrapper application.


This will make it a lot harder to get your code. Not impossible, but harder.

For AIR applications you could leave the SWF encrypted when delivering the application to the end-user. Then you could provide a registration key that contains the key used to decrypt the SWF.

Also, here is a link to an AS3 obfuscator. I am not sure how well it works though.
http://www.ambiera.com/irrfuscator/index.html

VMware Workstation 6 nicely handles multiple displays. At least it handles my two monitors nicely. And they don't even have to be the same size.

What you have there seems to be pretty good. My company has on occasion, for large enough databases, broken it down even further, perhaps to the individual object level. In this way each table/index/... has its own file. Can be useful, can be overkill. Really depends on how you are using it.

@Justin

By domain is mostly always sufficient. I agree that there are some complexities to deal with when doing it this way, but that should be easy enough to handle.  

I think this method provides a little more seperation (which in a large database you will come to appreciate) while still making itself pretty manageable. We also write Perl scripts that do a lot of the processing of these DDL files, so that might be an option of a good way to handle that.


Invest the time to write a generic "drop all constraints" script, so you don't have to maintain it. (A cursor over "Select * From Information_Schema.Table_Constraints" and "Select * From Information_Schema.Referential_Constraints" does the trick).

@Adam

Or how about just by domain -- a useful grouping of related tables in the same file, but separate from the rest?

Only problem is if some domains (in this somewhat legacy system) are tightly coupled.  Plus you have to maintain the dependencies between your different sub-scripts.

There are two main ways of running Python on Apache. The simplest would be to use CGI and write normal Python scripts while the second is using a web framework like Django or Pylons.

Using CGI is straightforward. Make sure your Apache config file has a cgi-bin set up. If not, follow their documentation (http://httpd.apache.org/docs/2.0/howto/cgi.html). At that point all you need to do is place your Python scripts in the cgi-bin directory and the standard output will become the HTTP response. Refer to Python's documentation for further info (https://docs.python.org/library/cgi.html).

If you want to use a web framework you'll need to setup mod_python or FastCGI. These steps are dependent on which framework you want to use. Django provides clear instructions on how to setup mod_python and Django with Apache (http://www.djangoproject.com/documentation/modpython/)


Depending on the complexity, each situation is different, If I am just simply moving files around, I'll write a quick batch file. If I want to do something more complex Ill normally just skip the scripting part and write a quick c# program that can handle it. 

The question then is do you put that c# program in svn and have it versioned :)

edit: The benefits of a dedicated c# application is that I can reuse code fragments to create new hooks later, including a simple log output I created to handle hook logging.

I would suggest storing the stylesheet selection in the session so you don't have to rely on the querystring key being present all the time. You can check the session in Page_Load and add the appropriate stylesheet reference. It sounds like this is a temporary/development situation, so go with whatever is easy and works.

if (!String.IsNullOrEmpty(Request.QueryString["css"]))  Session.Add("CSS",Request.QueryString["css"]);

In Asp.net 3.5, you should be able to set up the Link tag in the header as a server tag. Then in the codebehind you can set the href property for the link element, based on a cookie value, querystring, date, etc.
In your aspx file:&lt;head&gt;
  &lt;link id="linkStyles" rel="stylesheet" type="text/css" runat="server" /&gt;
&lt;/head&gt;

And in the Code behind:protected void Page_Load(object sender, EventArgs e) {
  string stylesheetAddress = // logic to determine stylesheet
  linkStyles.Href = stylesheetAddress;
}


You should look into ASP.NET themes, that's exactly what they're used for. They also allow you to skin controls, which means give them a set of default attributes.

I would do the following:

www.website.com/?stylesheet=new.css

Then in your ASP.NET code:

if (Request.Querystring["stylesheet"] != null) {    Response.Cookies["stylesheet"].Value = Request.QueryString["stylesheet"];    Response.Redirect(&lt;Current Page&gt;);}

Then where you define your stylesheets:

if (Request.Cookies["stylesheet"] != null) {    // New Stylesheet} else {    // Default}

Yes, mod_python is pretty confusing to set up.  Here's how I did it.

In httpd.conf:

LoadModule python_module modules/mod_python.so&lt;Directory "/serverbase/htdocs/myapp"&gt;  AddHandler mod_python .py  PythonHandler myapp  PythonDebug On

and in your application directory:

$ /serverbase/htdocs/myapp$ ls -ltotal 16-r-xr-xr-x 1 root sys        6484 May 21 15:54 myapp.py

Repeat the configuration for each python program you wish to have running under mod_python.

Are you running Python on UNIX or Windows?

An alternative to mod_python and FastCGI is mod_wsgi. You can find out more at modwsgi

I have built and installed this on Solaris without problems. I had previously tried mod_python but ran into problems with shared libraries as part of the build. There are good install docs available.


If you're using ASP.NET 2.0, you can do this with cross-page posting.
Edit: I missed the fact that you're asking about an external page. For that I think you'd need to have your ASP.NET page gen up an HTML form whose action is set to the remote URL and method is set to POST. (Using cross-page posting, this could even be a different page with no UI, only hidden form elements.) Then add a bit of javascript to submit the form as soon as the postback result was received on the client.

I have done this by rendering a form that auto-posts (using JavaScript) to the desired remote URL - gather whatever information you need for the post in the web form's postback and then build the HTML for the remote-posting form and render it back to the client.

I built a utility class for this that contains the remote URL and a collection of name/value pairs for the form.

Cross-page posting will work if you own both of the pages involved, but not if you need to post to another site (PayPal, for example).

LINQ turns into method calls like the code you have.

In other words, there should be no difference.

However, in your two pieces of code you are not calling .ToList in the first, so the first piece of code will produce an enumerable data source, but if you call .ToList on it, the two should be the same.

Here's I solved this problem today.  I started from this article on C# Corner, but found the example - while technically sound - a little incomplete.  Everything he said was right, but I needed to hit a few external sites to piece this together to work exactly as I wanted.

It didn't help that the user was not technically submitting a form at all; they were clicking a link to go to our support center, but to log them in an http post had to be made to the support center's site.

This solution involves using HttpContext.Current.Response.Write() to write the data for the form, then using a bit of Javascript on the 

&lt;body onload=""&gt; 

method to submit the form to the proper URL.

When the user clicks on the Support Center link, the following method is called to write the response and redirect the user:

public static void PassthroughAuthentication(){    System.Web.HttpContext.Current.Response.Write("&lt;body     onload=document.forms[0].submit();window.location=\"Home.aspx\";&gt;");    System.Web.HttpContext.Current.Response.Write("&lt;form name=\"Form\"     target=_blank method=post     action=\"https://external-url.com/security.asp\"&gt;");    System.Web.HttpContext.Current.Response.Write(string.Format("&lt;input        type=hidden name=\"cFName\" value=\"{0}\"&gt;", "Username"));    System.Web.HttpContext.Current.Response.Write("&lt;/form&gt;");    System.Web.HttpContext.Current.Response.Write("&lt;/body&gt;");}

The key to this method is in that onload bit of Javascript, which , when the body of the page loads, submits the form and then redirects the user back to my own Home page.  The reason for that bit of hoodoo is that I'm launching the external site in a new window, but don't want the user to resubmit the hidden form if they refresh the page.  Plus that hidden form pushed the page down a few pixels which got on my nerves.

I'd be very interested in any cleaner ideas anyone has on this one.

Eric Sipple

Here's one way you could do it:

&lt;div class='comment' id='comment_(comment id #)'&gt;  &lt;div class='comment_img'&gt;&lt;img src='...' alt='(Commenter Name)' /&gt;&lt;/div&gt;  &lt;div class='comment_text'&gt;  &lt;p&gt;Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Sed mauris. Morbi quis tellus sit amet eros ullamcorper ultrices. Proin a tortor. Praesent et odio. Duis mi odio, consequat ut, euismod sed, commodo vitae, nulla. Suspendisse potenti. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Etiam pede.&lt;/p&gt;  &lt;p&gt;Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Maecenas rhoncus accumsan velit. Donec varius magna a est. &lt;/p&gt;  &lt;/div&gt;  &lt;p class='comment_meta'&gt;By &lt;a href='#'&gt;Name&lt;/a&gt; on &lt;span class='comment_date'&gt;2008-08-21 11:32 AM&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;

With the following CSS to float the picture to the left of the contents:

.comment{    width: 400px;}.comment_img{    float: left;}.comment_text, .comment_meta{    margin-left: 40px;}.comment_meta{    clear: both;}

I don't know that there's markup that would necessarily represent the comment structure well without using divs or classes as well, but you could use definition lists. You can use multiple dt and dd tags in the context of a definition list - http://www.w3.org/TR/html401/struct/lists.html#edef-DL

&lt;dl&gt;  &lt;dt&gt;By [Name] at 2008-01-01&lt;dt&gt;  &lt;dd&gt;&lt;img src='...' alt=''/&gt;&lt;/dd&gt;  &lt;dd&gt;&lt;p&gt;Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Sed mauris. Morbi quis tellus sit amet eros ullamcorper ultrices. Proin a tortor. Praesent et odio. Duis mi odio, consequat ut, euismod sed, commodo vitae, nulla. Suspendisse potenti. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Etiam pede.&lt;/p&gt;  &lt;p&gt;Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Maecenas rhoncus accumsan velit. Donec varius magna a est. &lt;/p&gt;   &lt;/dd&gt;&lt;/dl&gt;

Update: The concern I'd have with an approach like this is that it could be difficult to uniquely identify the elements with CSS for styling purposes.  You could use JavaScript (jQuery would be great here) to find and apply styles.  Without full CSS selector support across browsers (IE), it would be tougher to style.

I was perhaps thinking of something like this:

&lt;ol class="comments"&gt;    &lt;li&gt;        &lt;a href=""&gt;            &lt;img src="" alt="" /&gt;        &lt;/a&gt;        &lt;cite&gt;Name&lt;br /&gt;Date&lt;/cite&gt;        &lt;blockquote&gt;Comment&lt;/blockquote&gt;    &lt;/li&gt;&lt;/ol&gt;

It's very semantic without using div's and only one class. The  list show the order the comments were made, a link to the persons website, and image for their gravatar, the cite tag to site who said the comment and blockquote to hold what they said.

What do people think?

Only way I could figure out how to do it without just moving the file and telling the user was to pass it off to the browser.

navigateToURL(new URLRequest(File.applicationStorageDirectory.nativePath + "/courses/" + fileName));



I think that your version with the cite, blockquote, etc. would definitely work , but if semantics is your main concern then I personally wouldn't use cite and blockquote as they have specific things that they are supposed to represent.

The blockquote tag is meant to represent a quotation taken from another source and the cite tag is meant to represent a source of information (like a magazine, newspaper, etc.).

I think an argument can certainly made that you can use semantic HTML with class names, provided they are meaningful. This article on Plain Old Semantic HTML makes a reference to using class names - http://www.fooclass.com/plain_old_semantic_html

I won't help much but I remember that I was able to wrap MATLAB simulation into DLL and then call it from Delphi app. It work really well.

Anyway: good luck!!!

I disagree with John's answer. The DataContext (or Linq to Entities ObjectContext) is more of a "unit of work" than a connection. It manages change tracking, etc. See this blog post for a description:

Lifetime of a LINQ to SQL DataContext

The four main points of this blog post are that DataContext:


Is ideally suited
for a "unit of work" approach 
Is also designed for
"stateless" server operation
Is not designed for
    Long-lived usage

    Should be used very carefully after
    any SumbitChanges() operation.


Considering that, I don't think using more than one DataContext would do any harm- in fact, creating different DataContexts for different types of work would help make your LinqToSql impelmentation more usuable and organized. The only downside is you wouldn't be able to use sqlmetal to auto-generate your dmbl. 


You need to set the run-time library (Under C/C++ -&gt; Code Generation) for ALL projects to static linkage, which correlates to the following default building configurations:

Multithreaded Debug/Release
Singlethreaded Debug/Release
As opposed to the "DLL" versions of those libraries.
Even if you do that, depending on the libraries you're using, you might have to install a Merge Module/framework/etc. It depends on whether static LIB versions of your dependencies are available.

I see your point. OK, after reading through that article, why don't you try something like this?

&lt;blockquote     cite="http://yoursite/comments/feederscript.php?id=commentid"     title="&lt;?php echo Name . " - " . Date ?&gt;" &gt;    &lt;?php echo Comment ?&gt;&lt;/blockquote&gt;

with some snazzy CSS to make it look nice.

feederscript.php would be something that could read from the database and echo only the commentid called for.

VirtualBox also has API's for automating their VM's.

I would do the form post in your code behind using HttpWebRequest class. Here is a good helper class to get your started:

http://geekswithblogs.net/rakker/archive/2006/04/21/76044.aspx

From there, you can just do a Response.Redirect, or perhaps you need to vary your action based on the outcome of the post (if there was an error, display it to the user or whatever). I think you already had the answer in your question to be honest - sounds like you think it is a post OR redirect when in reality you can do them both from your code behind.

I've heard good things about Mosso .

http://www.mosso.com/

I found a fairly elegant solution with Telerik's RadAjaxManager. It works quite nicely, essentially you register each control which might invoke a postback, and then register each control which should be re-drawn after that postback is performed asynchronously. The RadAjaxManager will update the DOM after the async postback and rewrite the ViewState and all affected controls. After taking a peek in Reflector, it looks a little kludgy under the hood, but it suits my purposes.

Other than the ToList difference, #2 is a lot more readable and natural IMO

I once saw a tool that converts Virtual PC to VMWare, never used it, so use at your own risk, but here is the link:

http://www.vmware.com/products/converter/faqs.html

From my experience VMWare is waaaay better than virtual PC, if you can convince your company to use it, you wont regret the switch.

I don't understand why you would use a custom control for that, when the builtin ASP.NET AJAX UpdatePanel does the same thing.

It just adds more complexity, gives you less support, and makes it more difficult for others to work on your app.

The Objective-C language has had "Categories" since the early 1990s; these are essentially the same thing as .NET Extension Methods.  When looking for best practices you might want to see what rules of thumb Objective-C (Cocoa &amp; NeXT) developers have come up with around them.

Brent Simmons (the author of the NetNewsWire RSS reader for Mac OS X and iPhone) just posted today about his new style rules for the use of categories and there's been a bit of discussion in the Cocoa community around that post.

Why can't you simply install a subversion server? If you download VisualSVN Server, which is free, you get a http server for your source code and can thus use the FogBugz scripts for integrating the two.

The reason I'm asking is because all scripts and documentation so far assumes you have the server, client-side scripts are too new for FogBugz to have templates for them so you're pretty much left to your own devices on that.

I am not sure I follow you. Do you have the repositories on the network or on your C:\ drive? According to two of your posts, you have both, or neither, or one of them or...

You can not get VisualSVN or Apache to safely serve repositories from a network share. Since you originally said you had the repositories on your C:\ drive, that's what you get advice for. If you have a different setup, you need to tell us about that.

If you have the repositories on your local harddisk, I would install VisualSVN, or integrate it into Apache. VisualSVN can run fine alongside Apache so if you go that route you only have to install it. Your existing repositories can also just be copied into the repository root directory of VisualSVN and you're up and running.

I am unsure why that big post here is labelled as incomplete, as it details the steps necessary to set up a hook script to inform FogBugz about the new revisions linked to the cases, which should be what the incomplete message says it doesn't do. Is that not working?

Create a class object and return a list(T) of the query.


I worked on a project last summer that required some pretty heavy modifications to .NET Remoting. I don't remember all the specifics, but if we had more than one network interface, we couldn't get the out-of-the-box Remoting implementation to reliably detect which one the Remoting traffic came from, which did horrible things to performance. This sounds like a similar, if not the same, issue.

Unfortunately, it seems SQL Server 2008 Client Tools requires Visual Studio 2008 SP1, and I'm loath to install a beta of this on my main development machine.

I'll wait until SP1 is RTM before I move on.

Edit: Yes, I do have Visual Studio 2008 on this machine, but I'd like to avoid beta installations of debugger applications. They tend to dig themselves too deep in for my taste.

If you have Visual Studio 2008 installed you will get a validation error and you cannot install SQL server 2008 until you install Visual Studio 2008 SP1. If you don't have Visual Studio 2008 installed it should not be a problem. So if you do have Visual Studio 2008 wait till August 11th since that is the day that Visual Studio 2008 SP1 will ship

You should never ever use the money datatype to store monetary values. If you do any calculations you will get truncated results. Run the following to see what I mean

DECLARE
@mon1 MONEY,
@mon2 MONEY,
@mon3 MONEY,
@mon4 MONEY,
@num1 DECIMAL(19,4),
@num2 DECIMAL(19,4),
@num3 DECIMAL(19,4),
@num4 DECIMAL(19,4)

SELECT
@mon1 = 100, @mon2 = 339, @mon3 = 10000,
@num1 = 100, @num2 = 339, @num3 = 10000

SET @mon4 = @mon1/@mon2*@mon3
SET @num4 = @num1/@num2*@num3

SELECT @mon4 AS moneyresult,
@num4 AS numericresult


Output:
2949.0000    2949.8525


If you want the context menu to be dependent on the selected item you're best move I think is to use Jonesinator's code to select the clicked item. Your context menu content can then be dependent on the selected item.

Selecting the item first as opposed to just using it for the context menu gives a few advantages. The first is that the user has a visual indication as to which he clicked and thus which item the menu is associated with. The second is that this way it's a hell of a lot easier to keep compatible with other methods of invoking the context menu (e.g. keyboard shortcuts).

In addition to Ishmaeel's answer, the method OpenMappedMachineConfiguration() will always return a Configuration object. So to check to see if it loaded you should check the HasFile property where true means it came from a file.


In WPF and Silverlight the binding infrastructure takes care of the switching to the UI thread.

The class documentation

has this note:


  If you create an HttpListener using
  https, you must select a Server
  Certificate for that listener.
  Otherwise, an HttpWebRequest query of
  this HttpListener will fail with an
  unexpected close of the connection.


and this:


  You can configure Server Certificates
  and other listener options by using
  HttpCfg.exe. See
  http://msdn.microsoft.com/library/default.asp?url=/library/en-us/http/http/httpcfg_exe.asp
  for more details. The executable is
  shipped with Windows Server 2003, or
  can be built from source code
  available in the Platform SDK.


Is the first note explained by the second?  As outlined in the question, I used httpcfg.exe to bind the certificate to a specific port.  If they intend something other than this, the note is ambiguous.

Protocol buffers are intended to optimize communications between machines. They are really not intended for human interaction. Also, the format is binary, so it could not replace XML in that use case. 

I would also recommend JSON as being the most compact text-based format.

The open source tool Schema Spy works well. Sample output is available here. If your working with oracle databases, Oracle has a free SQL Developer Tool that generates schema documentation very similar to the output of schema spy.

I've written hooks in Python on Windows since there are a lot of examples on the net (usually for Linux but the differences are small).  We also use Trac integrated with SVN and there is a Trac API accessible via Python which lets us automatically create/modify Trac tickets from SVN hook scripts.

Just to clarify

private static IEnumerator&lt;TextBox&gt; FindTextBoxes(Control rootControl)

Changes to

private static IEnumerable&lt;TextBox&gt; FindTextBoxes(Control rootControl)

That should be all :-)

The result of a DCT is a transformation of the original source into the frequency domain. The top left entry stores the "amplitude" the "base" frequency and frequency increases both along the horizontal and vertical axes. The outcome of the DCT is usually a collection of amplitudes at the more usual lower frequencies (the top left quadrant) and less entries at the higher frequencies. As lassevk mentioned, it is usual to just zero out these higher frequencies as they typically constitute very minor parts of the source. However, this does result in loss of information. To complete the compression it is usual to use a lossless compression over the DCT'd source. This is where the compression comes in as all those runs of zeros get packed down to almost nothing.

One possible advantage of using the DCT to find similar regions is that you can do a first pass match on low frequency values (top-left corner). This reduces the number of values you need to match against. If you find matches of low frequency values, you can increase into comparing the higher frequencies.

Hope this helps

A 304 Not Modified response can result from a GET or HEAD request with either an If-Modified-Since ("IMS") or an If-Not-Match ("INM") header.

In order to decide what to do when you receive these headers, imagine that you are handling the GET request without these conditional headers.  Determine what the values of your ETag and Last-Modified headers would be in that response and use them to make the decision.  Hopefully you have built your system such that determining this is less costly than constructing the complete response.

If there is an INM and the value of that header is the same as the value you would place in the ETag, then respond with 304.

If there is an IMS and the date value in that header is later than the one you would place in the Last-Modified, then respond with 304.

Else, proceed as though the request did not contain those headers.

For a least-effort approach to part 2 of your question, figure out which of the (Expires, ETag, and Last-Modified) headers you can easily and correctly produce in your Web application. 

For suggested reading material:

http://www.w3.org/Protocols/rfc2616/rfc2616.html

http://www.mnot.net/cache_docs/

As long as it changes whenever the resource representation changes, how you produce it is completely up to you.

You should try to produce it in a way that additionally:


doesn't require you to re-compute it on each conditional GET, and
doesn't change if the resource content hasn't changed


Using hashes of content can cause you to fail at #1 if you don't store the computed hashes along with the files.

Using inode numbers can cause you to fail at #2 if you rearrange your filesystem or you serve content from multiple servers.

One mechanism that can work is to use something entirely content dependent such as a SHA-1 hash or a version string, computed and stored once whenever your resource content changes.

When you release a new version of your CSS or JS libraries, cause the following to occur:


modify the filename to include a unique version string
modify the HTML files which reference the library to point at the versioned file


(this is usually a pretty simple matter for a release script)

Now you can set the Expires for the CSS/JS to be years in the future.  Whenever you change the content, if the referencing HTML points to a new URI, browsers will no longer use the old cached copy.

This causes the caching behavior you want without requiring anything of the user.

I'd recommend picking up a copy of Digital Video Compression - it's a really good overview of compression algorithms for images and video. 

Can you sniff the traffic to find what's actually being sent?  Is it sending any auth data at all and it's incorrect or being presented in a form the server doesn't like, or is it never being sent by firefox at all?

Joel Lucsy: That implementation of SQLite is a mixed-mode assembly which is not supported by Silverlight.  Only a pure managed implementation would work under the Silverlight CLR.

Is your scripting language bytecode generating? Does it generate debug metadata? If so, bytecode instrumentation is probably the way to go. In fact existing tools like will probably work; perhaps with minimal modification (the typical problem is the tools are written to work with Java and assume com.foo.Bar.class corresponds to com/foo/Bar.java. Unwinding that assumption can be tedious.) EMMA is a ClassLoader that does byte-code re-writing for code-coverage collection in Java. The coding style is a little funky, but I recommend reading the source for some ideas.

If your scripting language is interpreted then you will need something higher-level (source level) that hooks into the interpreter.


You'll get different results for the different methods depending on whether you compile with optimisations on. You basically have a few options:

object o;

//checking with is
o is int

//check type
o.GetType() != typeof( int )

//cast and catch exception
try{ int j = (int) o; } 
catch {}

//use the tryparse
int.TryParse( Convert.ToString( o ), out j )


You can easily set up a console app that tries each of these 10,000 times and returns durations for each (test when o is an int and when it's something else).

The try-catch method is the quickest if the object does hold an int, and by far the slowest if it doesn't (even slower than GetType).  int.TryParse is pretty quick if you have a string, but if you have an unknown object it's slower.

Interestingly, with .Net 3.5 and optimisations turned on the o is int check takes the same time as try-catch when o actually is an int. o is int is only slightly slower if o actually is something else.

Annoyingly FxCop will throw up warnings if you do something like:

if( o is int )
    int j = (int) o;


But I think that's a bug in FxCop - it doesn't know int is a value type and recommends you to use o as int instead.

If your input is always a string int.TryParse is best, otherwise the is operator is quickest.

As you have a string I'd look at whether you need to know that it's an int, rather than a double.  If int.TryParse passes then so will double.TryParse so you could half the number of checks - return either double or string and floor the doubles when you expect an int.


Another possibility is to use DTS or Integration Services (DTS for SQL Server 7 or 2000, SSIS for 2005 or higher). Both are from Microsoft, included in the Sql Server installation (in Standard edition at least) and have an FTP task and are designed for import/export jobs from Sql Server.


Check out asio. It is a cross compatable c++ library for asyncronous IO. I am not sure if this would be useful for the server ( I have never tried to link a standard c++ DLL to a c# project) but for the client it would be useful.

We use it with our application, and it solved most of our IO concurrency problems.


Remote shell doesn't solve the productivity issue. (It merely makes things possible.)

From what I've heard, everything that the future Microsoft GUI:s do will be possible to do with powershell since the GUI:s use the same API:s as those that are available from powershell.

Personally, I love cygwin but cygwin can not help you manage Microsoft applications.

You might be surprised, however, how powerfull the Windows Scripting Host is when coupled with Window Management Instrumentation. I think IIS is fully manageable with WMI or some COM objects that can be easilly used from a JScript WSH script.


Something like this?

dim cars(2),x
cars(0)="Volvo"
cars(1)="Saab"
cars(2)="BMW"

For Each x in cars
  response.write(x &amp; "&lt;br /&gt;")
Next


See www.w3schools.com.

If you want to associate keys and values use a dictionary object instead:

Dim objDictionary
Set objDictionary = CreateObject("Scripting.Dictionary")
objDictionary.Add "Name", "Scott"
objDictionary.Add "Age", "20"
if objDictionary.Exists("Name") then
    ' Do something
else
    ' Do something else 
end if



As mentioned d will be IEnumerable&lt;MyProduct&gt; while f is List&lt;MyProduct&gt;

The conversion is done by the C# compiler

var d = 
    from mp in MyProducts
    where mp.Price &lt; 50d
    select mp;


Is converted to (before compilation to IL and with generics expanded):

var d = 
    MyProducts.
    Where&lt;MyProduct&gt;( mp =&gt; mp.Price &lt; 50d ).
    Select&lt;MyProduct&gt;( mp =&gt; mp ); 
    //note that this last select is optimised out if it makes no change


Note that in this simple case it makes little difference.  Where Linq becomes really valuable is in much more complicated loops. 

For instance this statement could include group-bys, orders and a few let statements and still be readable in Linq format when the equivalent .Method().Method.Method() would get complicated.


I have used an httpmodule for url rewriting from www.urlrewriting.net with great success (albeit I believe a much earlier, simpler version)

If you have very few actual rewriting rules then url mappings built in to .NET 2.0 are probably an easier option, there are a few write ups of these on the web, the 4guysfromrolla one seems fairly exhaustive but as you can see they don't support regular expression mappings are are as such rendered fairly useless in a dynamic environment (assuming "smith" in your example is not a special case then these would be of no use)


No you cannot get the MAC address in JavaScript, mainly because the MAC address uniquely identifies the running computer so it would be a security vulnerability.

Now if all you need is a unique identifier, I suggest you create one yourself using some cryptographic algorithm and store it in a cookie.

If you really need to know the MAC address of the computer AND you are developing for internal applications, then I suggest you use an external component to do that: ActiveX for IE, XPCOM for Firefox (installed as an extension).


I would also take a look at Passenger. It's a lot easier to get going than the traditional solution of Apache/nginx + Mongrel.


I would recommend Companion JS.

This is the free version of Debug Bar but I find it easier to use and have the features I need. Great to test little JavaScript snippets in IE the same way I do with Firebug in Firefox.

EDIT 5 years later: I now uses Internet Explorer integrated developer tools.


in our continuous integration setup we use SVNRevisionLabeller and pass the variables from this to MSBuild to use when creating the compiled website dll. It's then available to .NET using GetCurrentAssembly() in the final build.


In our rails app I have a secret (unpulished url, restricted to a certain class of authenticated user) action which literally does this

render :text =&gt; `svn info #{RAILS_ROOT}`


(this is the equivalent of Process.Start( "svn info..." ) if you're only familiar with .net)

If I'm wondering if the guy who manages the servers has updated the site recently, I can hit this URL, and have a look


You could try setting up your own python installation using Virtual Python.  Check out how to setup Django using it here.  That was written a long time ago, but it shows how I got MySQLdb setup without having root access or anything like it.  Once you've got the basics going, you can install any python library you want.


I learned everything I know about the DCT from The Data Compression Book. In addition to being a great introduction to the field of data compression, it has a chapter near the end on lossy image compression which introduces JPEG and the DCT.


The dpUint testing framework has a test runner built with AIR which can be integrated with a build script.

There is also my FlexUnit automation kit which does more or less the same for FlexUnit. It has an Ant macro that makes it possible to run the tests as a part of an Ant script, for example:

&lt;target name="run-tests" depends="compile-tests"&gt;
  &lt;flexunit swf="${build.home}/tests.swf" failonerror="true"/&gt;
&lt;/target&gt;



About how to develop Flex applications the right way, I wouldn't look too much at the Cairngorm framework. It does claim to show "best practice" and so on, but I would say that the opposite is true. It's based around the use of global variables, and other things you should try to avoid. I've outlined some of the problems on my blog.

I would suggest that you look at the Mate framework instead, which has good documentation and good examples to get you going. It uses Flex to its full potential, doesn't rely on global variables as Cairngorm and PureMVC, and it makes it possible to write much more decoupled code.


It's possible to open multiple databases at once in Sqlite, but it's doubtful if can be done when working from Flex/AIR. In the command line client you run ATTACH DATABASE path/to/other.db AS otherDb and then you can refer to tables in that database as otherDb.tableName just as in MySQL or SQL Server.


  Tables in an attached database can be referred to using the syntax database-name.table-name. 
  
  ATTACH DATABASE documentation at sqlite.org



I've always used For Each...


You can also get the source from a Open Source code coverage tool and learn from it.


@ggasp Here is what I got off IBM's Information Center


  A node is a logical grouping of managed servers.
  
  A node usually corresponds to a logical or physical computer system with a distinct IP host address. Nodes cannot span multiple computers.


http://publib.boulder.ibm.com/infocenter/wasinfo/v6r1/index.jsp?topic=/com.ibm.websphere.nd.multiplatform.doc/info/ae/ae/cagt_node.html



  Is there any option besides using the hidden iframe trick?


Unfortunately, no, not now. Otherwise the microsummary code you point to would use it instead.


  And if not, what is the best way to do the iframe trick so that your code works outside the context of any currently open tabs (so that closing tabs won't screw up code, etc)?


The code you quoted uses the recent browser window, so closing tabs won't affect parsing. Closing that browser window will abort your load, but you can deal with it (detect that the load is aborted and restart it in another window for example) and it doesn't happen very often.

You need a DOM window for the iframe to work properly, so there's no clean solution at the moment (if you're keen on using the mozilla parser).


I don't have any VPN connections on my current computer but somewhere in the TCP/IP properties for the connection there's a checkbox to indicate that you use the remote host as a gateway or something like that.

This once caused me alot of issues since all my traffic would go over the VPN and then back again, even when I wanted to do something locally.


$rev and others like it are revisions for the individual files, so they won't change unless the file changes. The number on the webpage is (most likely, I'm assuming here) the svn revision number for the whole project. That is different than the file revisions, which others have been pointing to.

In this case I assume that CCNET is pulling the revision number of the project and rewriting a part of the webpage with that number. Any CI solution should be able to do this, set this up myself with CCNET and Teamcity (although not webpages, but automatic versioning of deployment/assembly versions).

In order for you to do this, use a CI solution that supports it, or use your build process (MSbuild/Nant) to store that version and write it to the files before "deploying" it.



  this must be a common need.


Some small percentage of software developers develop for .NET 
Some very small percentage of that group develop for mono 
Some small percentage of that group wants to provide .debs instead of just a zip 
Some very small percentage of that group wants to build their linux apps on windows instead of natively on linux 

It's just you :-)


The problem is that FogBugz will link to a web page, and file:///etc is not a web page. To get integration two ways, you need a web server for your subversion repository. Either set up Apache or something else that can host those things the proper way.


I have used NuSOAP in the past.  I liked it because it is just a set of PHP files that you can include.  There is nothing to install on the web server and no config options to change.  It has WSDL support as well which is a bonus.


Sql Server 2005 gives you the ability to specify a covering index.  This is an index that includes data from other columns at the leaf level, so you don't have to go back to the table to get columns that aren't included in the index keys.

create nonclustered index myidx on mytable (mycol1 asc, mycol2 asc) include (my_col3);

This is invaluable for a query that has mycol3 in the select list, and mycol1 and my_col2 in the where clause.


Here's what I do for paging:  All of my big queries that need to be paged are coded as inserts into a temp table.  The temp table has an identity field that will act in a similar manner to the row_number() mentioned above.  I store the number of rows in the temp table in an output parameter so the calling code knows how many total records there are.  The calling code also specifies which page it wants, and how many rows per page, which are selected out from the temp table.

The cool thing about doing it this way is that I also have an "Export" link that allows you to get all rows from the report returned as CSV above every grid in my application.  This link uses the same stored procedure: you just return the contents of the temp table instead of doing the paging logic.  This placates users who hate paging, and want to see everything, and want to sort it in a million different ways.


This was the fastest I could come up with after some fiddling:

private function castMethod4(dateString:String):Date {    
    if ( dateString == null ) 
        return null;    
    if ( dateString.length != 10 &amp;&amp; dateString.length != 19) 
        return null;

    dateString = dateString.replace("-", "/");
    dateString = dateString.replace("-", "/");

    return new Date(Date.parse( dateString ));
}


I get 50k iterations in about 470ms for castMethod2() on my computer and 300 ms for my version (that's the same amount of work done in 63% of the time). I'd definitely say both are "Good enough" unless you're parsing silly amounts of dates.  


I just looked at the AIR SQL API, and there's an attach method on SQLConnection it looks exactly what you need.

I haven't tested this, but according to the documentation it should work:

var connection : SQLConnection = new SQLConnection();

connection.open(firstDbFile);
connection.attach(secondDbFile, "otherDb");

var statement : SQLStatement = new SQLStatement();

statement.connection = connection;
statement.text = "INSERT INTO main.myTable SELECT * FROM otherDb.myTable";
statement.execute();


There may be errors in that code snipplet, I haven't worked much with the AIR SQL API lately. Notice that the tables of the database opened with open are available using main.tableName, any attached database can be given any name at all (otherDb in the example above).


I've been using the following snipplet to parse UTC date strings:

private function parseUTCDate( str : String ) : Date {
    var matches : Array = str.match(/(\d\d\d\d)-(\d\d)-(\d\d) (\d\d):(\d\d):(\d\d)Z/);

    var d : Date = new Date();

    d.setUTCFullYear(int(matches[1]), int(matches[2]) - 1, int(matches[3]));
    d.setUTCHours(int(matches[4]), int(matches[5]), int(matches[6]), 0);

    return d;
}


Just remove the time part and it should work fine for your needs:

private function parseDate( str : String ) : Date {
    var matches : Array = str.match(/(\d\d\d\d)-(\d\d)-(\d\d)/);

    var d : Date = new Date();

    d.setUTCFullYear(int(matches[1]), int(matches[2]) - 1, int(matches[3]));

    return d;
}


No idea about the speed, I haven't been worried about that in my applications. 50K iterations in significantly less than a second on my machine.


I think it's great that Joel et al. let people use FogBugs hosted for free on their own.  It's a great business strategy, because the users become fans (it is great software after all), and then they recommend it to their businesses or customers.


Watch out for the Gnu Scientific Library.  It's licensed under the GPL rather than LGPL.

As other folks mentioned, the Boost random classes are a good start.  Their implementation conforms to the PRNG code slated for TR1:

http://www.boost.org/doc/libs/1_35_0/libs/random/index.html
http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2003/n1452.html

If you have a recent version of the G++ compiler, you may find the TR1 libraries already included



Choose Project -> Properties
Select Configuration -> General
In the box for how you should link MFC, choose to statically link it.
Choose Linker -> Input.  Under Additional Dependencies, add any libraries you need your app to statically link in.


For more info, see this article: http://www.geekadmin.com/?p=34


On my project we're using Maven to build both our Flex RIA and the Java-based back end.  In order to build and test the Flex app we use the flex-mojos maven plugins.  They do a great job for us and I would highly recommend using Maven over Ant.

That being said, if you're already using Ant it can be a little tricky to transition over to Maven.  So if you're in that position I would recommend using the flexunit tasks available here: Ant Task

Both of these libraries do basically the same thing, they launch a generated flexunit test runner mxml application in a window and open a socket connection back to the build process using a JUnit test runner.  Amazingly enough it works pretty well.  The only problem is that you can't run it headless so if you want to run the build from a CI server you have to make sure that process has the ability to launch new windows otherwise it won't work.


We follow the practice of using a vendor directory which contains all vendor specific headers and binaries.  The goal is that anybody should be able to build the product just by checking it out and running some top level build script.  


I don't have the power to edit @Michael Stum's answer, but it's not quite correct.  He reduces

(i + 4) - (a + b)


to

(i + 4 - a + b)


They are not equivalent.  The best reduction I can get for the whole expression is

((i + 4) - (a + b)) * MAGIC_NUMBER - ANOTHER_MAGIC_NUMBER;


or

(i + 4 - a - b) * MAGIC_NUMBER - ANOTHER_MAGIC_NUMBER;



I concur with all the previous answers that it would be a privacy/security vulnerability if you would be able to do this directly from Javascript. There are two things I can think of:


Using Java (with a signed applet)
Using signed Javascript, which in FF (and Mozilla in general) gets higher privileges than normal JS (but it is fairly complicated to set up)



I've previously used a component from here: www.weonlydo.com.  I didn't find it the easiest piece of kit to develop against but it got the job done in a hurry.


Currently adobe is not supporting opening files in there default applications. Passing it off to the browser seems to be the only way to make it work.

You could however use a FileStream and write a small html file with some javascript that sets the location of an iframe to the file, then after 100ms or so calls window.close(). Then open that file in the browser.


If you're trying to get it in one statement (the total plus the paging).  You might need to explore SQL Server support for the partition by clause (windowing functions in ANSI SQL terms).  In Oracle the syntax is just like the example above using row_number(), but I have also added a partition by clause to get the total number of rows included with each row returned in the paging (total rows is 1,262):

SELECT rn, total_rows, x.OWNER, x.object_name, x.object_type
  FROM (SELECT COUNT (*) OVER (PARTITION BY owner) AS TOTAL_ROWS,
               ROW_NUMBER () OVER (ORDER BY 1) AS rn, uo.*
          FROM all_objects uo
         WHERE owner = 'CSEIS') x
 WHERE rn BETWEEN 6 AND 10


Note that I have where owner = 'CSEIS' and my partition by is on owner.  So the results are:

RN  TOTAL_ROWS  OWNER   OBJECT_NAME OBJECT_TYPE
6   1262    CSEIS   CG$BDS_MODIFICATION_TYPES   TRIGGER
7   1262    CSEIS   CG$AUS_MODIFICATION_TYPES   TRIGGER
8   1262    CSEIS   CG$BDR_MODIFICATION_TYPES   TRIGGER
9   1262    CSEIS   CG$ADS_MODIFICATION_TYPES   TRIGGER
10  1262    CSEIS   CG$BIS_LANGUAGES    TRIGGER



I would re-factor the code inside the loop and then do

if( ref $results eq 'ARRAY' ){
    my_sub($result) for my $result (@$results);
}else{
    my_sub($results);
}


Of course I would only do that if the code in the loop was non-trivial.


This is something that annoys me about MSSQL (rant on my blog). I wish MSSQL supported upsert. 

@Dillie-O's code is a good way in older SQL versions (+1 vote), but it still is basically two IO operations (the exists and then the update or insert)

There's a slightly better way on this post, basically:

--try an update
update tablename 
set field1 = 'new value',
    field2 = 'different value',
    ...
where idfield = 7

--insert if failed
if @@rowcount = 0 and @@error = 0
    insert into tablename 
           ( idfield, field1, field2, ... )
    values ( 7, 'value one', 'another value', ... )


This reduces it to one IO operations if it's an update, or two if an insert. 

MS Sql2008 introduces merge from the SQL:2003 standard:

merge tablename as target
using (values ('new value', 'different value'))
    as source (field1, field2)
    on target.idfield = 7
when matched then
    update
    set field1 = source.field1,
        field2 = source.field2,
        ...
when not matched then
    insert ( idfield, field1, field2, ... )
    values ( 7,  source.field1, source.field2, ... )


Now it's really just one IO operation, but awful code :-(


Instead of joining an open source project, find an itch you want to scratch.

I find my first year with a language is almost always throw away code (or at least, it should be).

Find a problem you (personally) want to solve. Use ruby to do it. You'll learn a lot.


As mentioned in the question, IEnumerable has a CopyToDataTable method:

IEnumerable&lt;DataRow&gt; query =
    from order in orders.AsEnumerable()
    where order.Field&lt;DateTime&gt;("OrderDate") &gt; new DateTime(2001, 8, 1)
    select order;

// Create a table from the query.
DataTable boundTable = query.CopyToDataTable&lt;DataRow&gt;();


Why won't that work for you?


I'd recommend checking out the iMacros addon for Firefox. I use it to login to a local web server and after logging in, navigate directly to a certain page. The code I have looks like this, but it allows you to record your own macros:

VERSION BUILD=6000814 RECORDER=FX
TAB T=1
URL GOTO=&lt;http://10.20.2.4/login&gt;
TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:introduce ATTR=NAME:initials CONTENT=username-goes-here
SET !ENCRYPTION NO
TAG POS=1 TYPE=INPUT:PASSWORD FORM=NAME:introduce ATTR=NAME:password CONTENT=password-goes-here
TAG POS=1 TYPE=INPUT:SUBMIT FORM=NAME:introduce ATTR=NAME:Submit&amp;&amp;VALUE:Go
URL GOTO=&lt;http://10.20.2.4/timecard&gt;


I middle click on it and it opens a new tab and runs the macro taking me directly to the page I want, logged in with the account I specified.


Make a set of Data Transfer Objects, a couple of mappers, and return that via the .asmx.
You should never expose the database objects directly, as a change in the procedure schema will propagate to the web service consumer without you noticing it.


I've had great success with wsdl2php.  It will automatically create wrapper classes for all objects and methods used in your web service.


You need to use a layer 4 load balancer in front of the two endpoints. Prob best to stick with a dedicated piece of hardware.


For reference&mdash;future Python possibilities:
Starting with Python 2.6 you can express binary literals using the prefix 0b or 0B:

&gt;&gt;&gt; 0b101111
47


You can also use the new bin function to get the binary representation of a number:

&gt;&gt;&gt; bin(173)
'0b10101101'


Development version of the documentation: What's New in Python 2.6


I think you can do this with the Row Test Attribute (available in MbUnit and later versions of NUnit) where you could specify several sets to populate one unit test.


Even if you capture the keydown/keyup event, those are the only events that the tab key fires, you still need some way to prevent the default action, moving to the next item in the tab order, from occurring.

In Firefox you can call the preventDefault() method on the event object passed to your event handler. In IE, you have to return false from the event handle. The JQuery library provides a preventDefault method on its event object that works in IE and FF.

&lt;body&gt;
&lt;input type="text" id="myInput"&gt;
&lt;script type="text/javascript"&gt;
    var myInput = document.getElementById("myInput");
    if(myInput.addEventListener ) {
        myInput.addEventListener('keydown',this.keyHandler,false);
    } else if(myInput.attachEvent ) {
        myInput.attachEvent('onkeydown',this.keyHandler); /* damn IE hack */
    }

    function keyHandler(e) {
        var TABKEY = 9;
        if(e.keyCode == TABKEY) {
            this.value += "    ";
            if(e.preventDefault) {
                e.preventDefault();
            }
            return false;
        }
    }
&lt;/script&gt;
&lt;/body&gt;



The new framework is .Net 3.5, you'll have a new assembly System.Core, + a few more if you use features like Linq

.Net 3.5 comes with the new C#3.0 compiler

ASP.Net is still version 2.0

Lovely and confusing isn't it ;-)

You should upgrade the .Net framework on the server to .Net 3.5 SP1, but you're still going to be running ASP.Net 2.0


GateKiller,

.NET 3.0 and .NET 3.5 did not change the version of the CLR, so "using ASP.NET 3.5" is a more complicated thing that it sounds like it should be at first. In essence, you're still running on the 2.0 CLR, but you're using the C# 3.0 compiler and linking against the 3.5 libraries. It means adding a bunch of stuff to your Web.config file to become an ASP.NET 3.5 project.

Scott Hanselman has an awesome blog post covering the details:

http://www.hanselman.com/blog/HowToSetAnIISApplicationOrAppPoolToUseASPNET35RatherThan20.aspx


The upcoming release of the Framework Design Guidelines, 2nd Edition will have some guidance for implementing extension methods, but in general:

You should only define extension methods "where they make semantic sense" and are providing helper functionality relevant to every implementation.

You also should avoid extending System.Object as not all .NET languages will be able to call the extension method as an extension. (VB.NET for instance would need to call it as a regular static method on the static extension class.)

Don't define an extension method in the same namespace as the extended type unless you're extending an interface.

Don't define an extension method with the same signature as a "real" method since it will never be called.


We do this with xUnit.net for our automated builds. We use CruiseControl.net (and are trying out TeamCity). The MSBuild task that we run for continuous integration automatically changes the build number for us, so the resulting build ZIP file contains a properly versioned set of DLLs and EXEs.

Our MSBuild file contains a UsingTask reference for a DLL which does regular expression replacements: (you're welcome to use this DLL, as it's covered by the MS-PL license as well)


  &lt;UsingTask
     AssemblyFile="3rdParty\CodePlex.MSBuildTasks.dll"
     TaskName="CodePlex.MSBuildTasks.RegexReplace"/&gt;


Next, we extract the build number, which is provided automatically by the CI system. You could also get your source control provider to provide the source revision number if you want, but we found the build # in the CI system was more useful, because not only can see the integration results by the CI build number, that also provides a link back to the changeset(s) which were included in the build.


 &lt;!-- Cascading attempts to find a build number -->

 &lt;PropertyGroup Condition="'$(BuildNumber)' == ''">
   &lt;BuildNumber>$(BUILD_NUMBER)&lt;/BuildNumber>
 &lt;/PropertyGroup>
 &lt;PropertyGroup Condition="'$(BuildNumber)' == ''">
   &lt;BuildNumber>$(ccnetlabel)&lt;/BuildNumber>
 &lt;/PropertyGroup>
 &lt;PropertyGroup Condition="'$(BuildNumber)' == ''">
   &lt;BuildNumber>0&lt;/BuildNumber>
 &lt;/PropertyGroup>


(We try BUILD_NUMBER, which is from TeamCity, then ccnetlabel, which is from CC.net, and if neither is present, we default to 0, so that we can test the automated build script manually.)

Next, we have a task which sets the build number into a GlobalAssemblyInfo.cs file that we link into all of our projects:


 &lt;Target Name="SetVersionNumber">
   &lt;RegexReplace
       Pattern='AssemblyVersion\("(\d+\.\d+\.\d+)\.\d+"\)'
       Replacement='AssemblyVersion("$1.$(BuildNumber)")'
       Files='GlobalAssemblyInfo.cs'/>
   &lt;Exec Command="attrib -r xunit.installer\App.manifest"/>
 &lt;/Target>


This find the AssemblyVersion attribute, and replaces the a.b.c.d version number with a.b.c.BuildNumber. We will usually leave the source checked into the tree with the first three parts of the builder number fixed, and the fourth at zero (f.e., today it's 1.0.2.0).

In your build process, make sure the SetVersionNumber task precedes your build task. At the end, we use our Zip task to zip up the build results so that we have a history of the binaries for every automated build.


The syntax you are using for d will get transformed by the compiler into the same IL as the extension methods. The "SQL-like" syntax is supposed to be a more natural way to represent a LINQ expression (although I personally prefer the extension methods). As has already been pointed out, the first example will return an IEnumerable result while the second example will return a List result due to the call to ToList(). If you remove the ToList() call in the second example, they will both return the same result as Where returns an IEnumerable result.


Using SQL Server 2005 (or presumably 2008) I find for XML PATH to allow for much easier to maintain SQL than for XML Explicit (particularly once the SQL is longer).

In this case:

SELECT AccountNumber as "clientID"
FROM Location.LocationMDAccount
WHERE locationid = 'long-guid-here'
FOR XML PATH (''), Root ('root');



@travis Looks very nice! I will sure take a look into it. I can think of several places I can use that

I never got round to sniff the traffic but found out that a php site on my own server with http-auth worked fine, so i figured it was something with delicious. I then created a php page that does a wget of the delicious api and everything works fine :)


Using asynchronous communication is totally possible in single thread! 

There is a common design pattern in network software development called the reactor pattern (look at this book). Some well known network library provides an implementation of this pattern (look at ACE).

Briefly, the reactor is an object, you register all your sockets inside, and you wait for something. If something happened (new data arrived, connection close...) the reactor will notify you. And of course, you can use only one socket to send and received data asynchronously.


Check CaptainHook, "a simple plugin framework for writing Subversion hooks using .NET".



  Separating your source code into
  multiple projects makes only sense if
  you... 
  ... More developers involved
  and you want to treat their work as
  consumable black box. (not very
  recommended)  ...


Why isn't this recommended?  I've found it a very useful way to manage an application with several devs working on different portions.  Makes checkins much easier, mainly by virtually eliminating merges.  Very rarely will two devs have to work on the same project at the same time.


Separating features into projects is often a YAGNI architecture optimization. How often have you reused those separate projects, really? If it's not a frequent occurrence, you're complicating your development, build, deployment, and maintenance for theoretical reuse.

I much prefer separating into folders (using appropriate namespaces) and refactoring to separate projects when you've got a real-life reuse use case.


The "correct" way to do this is to respond to the WM_SYSCOMMAND message. In C# this looks something like this:

protected override void WndProc(ref Message m)
{
    // Abort screensaver and monitor power-down
    const int WM_SYSCOMMAND = 0x0112;
    const int SC_MONITOR_POWER = 0xF170;
    const int SC_SCREENSAVE = 0xF140;
    int WParam = (m.WParam.ToInt32() &amp; 0xFFF0);

    if (m.Msg == WM_SYSCOMMAND &amp;&amp;
        (WParam == SC_MONITOR_POWER || WParam == SC_SCREENSAVE)) return;

    base.WndProc(ref m);
}


According to MSDN, if the screensaver password is enabled by policy on Vista or above, this won't work. Presumably programmatically moving the mouse is also ignored, though I have not tested this.


Unit testing, Defense Programming and lots of logs

Unit testing

Make sure you unit test as early as possible (e.g. the password should be encrypted before sending, the SSL tunnel is working, etc). This would prevent your programmers from accidentally making the program insecure.

Defense Programming

I personally call this the Paranoid Programming but Wikipedia is never wrong (sarcasm). Basically, you add tests to your functions that checks all the inputs:


is the user's cookies valid?
is he still currently logged in?
are the function's parameters protected against SQL injection? (even though you know that the input are generated by your own functions, you will test anyway)


Logging

Log everything like crazy. Its easier to remove logs then to add them. A user have logged in? Log it. A user found a 404? Log it. The admin edited/deleted a post? Log it. Someone was able to access a restricted page? Log it.

Don't be surprised if your log file reaches 15+ Mb during your development phase. During beta, you can decide which logs to remove. If you want, you can add a flag to decide when a certain event is logged.


Statistics is hard :-). After a year of reading and re-reading books and papers and can only say with confidence that I understand the very basics of it.

You might wish to investigate ready-made libraries for whichever programming language you are using, because they are many gotcha's in math in general and statistics in particular (rounding errors being an obvious example).

As an example you could take a look at the R project, which is both an interactive environment and a library which you can use from your C++ code, distributed under the GPL (ie if you are using it only internally and publishing only the results, you don't need to open your code).


some day i work in a company that use VSS (and in other companies that use other less unknow SCM) but i prefer use SVN (someday i'll try GIT) for active development, for me and my group.

First of all, this situation it's only good idea, if commit to VSS are few over month, because working with other SCM (than VSS) give you more flexiblity, but commint to VSS from SVN is expensive in time.

My solution was:

VSS -> SVN: I have linux script (or ant script, or XXX script) that copy from currrent update directory work of VSS to current SVN, then refresh SVN client and update/merge/commit to SVN. With this, you are update from changes of the rest of company that use VSS.

SVN -> VSS: In this way, you need a checkout of all your modify files to VSS, then you can simply use the reverse script to copy from current update SVN directory (ignore .svn directories) and copy to current update VSS directory, update and commit.

But remember, in a few case does worth your time to do this.


a good start point, could be this maven pluggin, not for use it, or maybe yes, but this maven is build over ant task. If you see WAS5+Plugin+Mojo.zip\src\main\scripts\was5.build.xml

Or as said "McDowell", you can use "WebSphere Application Server (WAS) Ant tasks" but directly as ANT task.

&lt;path id="classpath"&gt;
    &lt;fileset file="com.ibm.websphere.v61_6.1.100.ws_runtime.jar"/&gt;
&lt;/path&gt;

&lt;taskdef name="wsStartApp" classname="com.ibm.websphere.ant.tasks.StartApplication" classpathref="classpath" /&gt;
&lt;taskdef name="wsStopApp" classname="com.ibm.websphere.ant.tasks.StopApplication" classpathref="classpath" /&gt;
&lt;taskdef name="wsInstallApp" classname="com.ibm.websphere.ant.tasks.InstallApplication" classpathref="classpath" /&gt;
&lt;taskdef name="wsUninstallApp" classname="com.ibm.websphere.ant.tasks.UninstallApplication" classpathref="classpath" /&gt;

&lt;target name="startWebApp1" depends="installEar"&gt;
 &lt;wsStartApp wasHome="${wasHome.dir}" 
    application="${remoteAppName}" 
    server="${clusterServerName}" 
    conntype="${remoteProdConnType}" 
    host="${remoteProdHostName}" 
    port="${remoteProdPort}" 
    user="${remoteProdUserId}" 
    password="${remoteProdPassword}" /&gt;
&lt;/target&gt;

&lt;target name="stopWebApp1" depends="prepare"&gt;
 &lt;wsStopApp wasHome="${wasHome.dir}"
    application="${remoteAppName}"
    server="${clusterServerName}"
    conntype="${remoteConnType}"
    host="${remoteHostName}"
    port="${remotePort}"
    user="${remoteUserId}"
    password="${remotePassword}"/&gt;
&lt;/target&gt;

&lt;target name="uninstallEar" depends="stopWebApp1"&gt;
 &lt;wsUninstallApp wasHome="${wasHome.dir}"
     application="${remoteAppName}"
     options="-cell uatNetwork -cluster DOL"
     conntype="${remoteConnType}"
     host="${remoteHostName}"
     port="${remoteDmgrPort}"
     user="${remoteUserId}"
     password="${remotePassword}"/&gt;
&lt;/target&gt;

&lt;target name="installEar" depends="prepare"&gt;
 &lt;wsInstallApp ear="${existingEar.dir}/${existingEar}" 
      wasHome="${wasHome.dir}" 
      options="${install_app_options}"
      conntype="${remoteConnType}" 
      host="${remoteHostName}" 
      port="${remoteDmgrPort}" 
      user="${remoteUserId}" 
      password="${remotePassword}" /&gt;
&lt;/target&gt;


Another useful link could be this.


The key difference is in the ViewState management IIRC. The DataGrid requires ViewState turned on in order to have edit and sort capabilities.


If you're working in Visual Studio 2008 / .NET 3.5, you probably shouldn't use either. Use the ListView - it gives you the features of the GridView combined with the styling flexibility of a repeater.


I upmodded Mark's post about Toad Data Modeler and wanted to point out that they have a beta version that is fully functional and free. The only downsides are the occasional bug and built in expiration (typically around the time a new beta is available), but for this poor bloke it does wonders until I can get my boss to chip in for a license.


I would recommend that you check out the following blog article:

http://blogs.msdn.com/vbteam/archive/2007/03/10/extension-methods-best-practices-extension-methods-part-6.aspx

It has some guidelines for how to use extension methods.

I wrote it back when I used to work for Microsoft, where I was the guy on the VB compiler team that implemented extension methods.


No. For some things you will need the .net Framework (like reporting services), and you can't install it (in a supported way) in a server core.


I previously organised my DDL code organised by one file per entity and made a tool that combined this into a single DDL script.

My former employer used a scheme where all table DDL was in one file (stored in oracle syntax), indicies in another, constraints in a third and static data in a fourth. A change script was kept in paralell with this (again in Oracle). The conversion to SQL was manual. It was a mess. I actually wrote a handy tool that will convert Oracle DDL to SQL Server (it worked 99.9% of the time).

I have recently switched to using Visual Studio Team System for Database professionals. So far it works fine, but there are some glitches if you use CLR functions within the database.


Perhaps try ctypes instead of SWIG. If it has been included as a part of Python 2.5, then it must be good :-)


GROUP BY is similar to DISTINCT in that it groups multiple records into one.

This example, borrowed from http://www.devguru.com/technologies/t-sql/7080.asp, lists distinct products in the Products table. 

SELECT Product FROM Products GROUP BY Product

Product
-------------
Desktop
Laptop
Mouse
Network Card
Hard Drive
Software
Book
Accessory


The advantage of GROUP BY over DISTINCT, is that it can give you granular control when used with a HAVING clause.

SELECT Product, count(Product) as ProdCnt
FROM Products
GROUP BY Product
HAVING count(Product) &gt; 2

Product      ProdCnt
--------------------
Desktop          10
Laptop            5
Mouse             3
Network Card      9
Software          6



Another solution would be to wrap the call to the server and have it always return an array to simplify the rest of your life:

sub call_to_service
{
    my $returnValue = service::call();

    if (ref($returnValue) eq "ARRAY")
    {
        return($returnValue);
    }
    else
    {
       return( [$returnValue] );
    }
}


Then you can always know that you will get back a reference to an array, even if it was only one item.

foreach my $item (@{call_to_service()})
{
  ...
}



We've got complex requirements like:


Only certain users can create folders in parts of the SVN tree, but everyone can edit files there
Certain file extensions cannot contain certain text in the file
Certain file extensions can only be stored in a subset of directories
As well as several simpler ones like, Must have a commit comment
Regression testable by running new hook against all previous SVN commits


#5 is huge for us, there's no better way to know you're not gonna break commits moving forward than to be able to push all previous commits through your new hook.  Making the hook understand that 1234 was a revision and 1234-1 was a transaction and making the appropriate argument changes when calling svnlook, etc. was the best decision we made during the process.

For us the nut got big enough that a fully unit testable, regression testable, C# console exe made the most sense.  We have config files that feed the directory restrictions, parse the existing httpd_authz file to get "privileged" users, etc.  Had we not been running on Windows with a .NET development work force, I would have probably written it all in Python, but since others might need to support it in the future I went .NET over .BAT, .VBS, Powershell silliness.  

Personally I think Powershell is different enough from .NET to be mostly useless as a "scripting" language.  It's good if the only cmd line support for a product comes via PS (Exchange, Windows 2k8), etc. but if all you want to do is parse some text or access regular .NET objects PS just adds a crazy syntax and stupid Security Iron Curtain to what could be a quick and easy little .NET app.


Thanks for the help @Stephen and @Greg Castle, using it I've managed to resolve my problem.

To provide a basic guide for others (from scratch):

Using Xwindows on a Windows PC to connect to a UNIX server over a VPN

What you need to start with:


The Putty Telnet/SSH client, download putty.exe (for free) from:

http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html
The Xming X server, download Xming (for free) from: 

http://sourceforge.net/project/showfiles.php?group_id=156984


What to do:


Install both of the above on your Windows PC
From the Windows start menu select: Programs -> Xming -> Xming
Run the Putty.exe program in the location you downloaded it to
In the PuTTY configuration screen do the following:


Set the IP address to be the IP address of your UNIX server
Select the SSH Protocol radio-button
Click the SSH : Tunnels category in the left hand pane of the configuration screen
Click the Enable X11 forwarding check-box
Click the Open button
Logon as usual to your UNIX server
Check the directory containing the X windows utilities are in your path, e.g. /usr/X/bin on Solaris
Run your X Windows commands in your putty window and they will spawn new windows on your desktop




I'm probably going a million miles in the wrong direct (but i'm only young :P ). but couldn't you add the graphic to a panel and then a mouselistener to the graphic object so that when the user on the graphic your action is preformed.


If you are querying a SQL Server database (Version 7 and up) you should replace the OleDb classes with corresponding classes in the System.Data.SqlClient namespace (SqlConnection, SqlCommand and SqlDataReader) as those classes have been optimized to work with SQL Server.

Another thing to note is that you should 'never' select all as this might lead to unexpected results later on if you add or remove columns to this table.


I'd agree with the previous two answers that in this instance it may be better from a usability perspective to split the two functions into separate screens. You really want your users to be focussed on entering complete and accurate credit card information, and having a map on the same screen may be distracting.

For the record though, Virtual Earth certainly does fully support SSL. To enable it you simple need to change the script reference from http:// to https:// and append &amp;s=1 to the URL, e.g.

&lt;script src="http://dev.virtualearth.net/mapcontrol/mapcontrol.ashx?v=6.1" type="text/javascript"&gt;&lt;/script&gt;


becomes

&lt;script src="https://dev.virtualearth.net/mapcontrol/mapcontrol.ashx?v=6.1&amp;s=1" type="text/javascript"&gt;&lt;/script&gt;



The problem is the strings in your struct. I found that marshaling types like byte/short/int is not a problem; but when you need to marshal into a complex type such as a string, you need your struct to explicitly mimic an unmanaged type. You can do this with the MarshalAs attrib.

For your example, the following should work:

[StructLayout(LayoutKind.Explicit)]
struct StructType
{
    [FieldOffset(0)]
    [MarshalAs(UnmanagedType.ByValTStr, SizeConst = 8)]
    public string FileDate;

    [FieldOffset(8)]
    [MarshalAs(UnmanagedType.ByValTStr, SizeConst = 8)]
    public string FileTime;

    [FieldOffset(16)]
    public int Id1;

    [FieldOffset(20)]
    [MarshalAs(UnmanagedType.ByValTStr, SizeConst = 66)] //Or however long Id2 is.
    public string Id2;
}



I have found that I also need to set accessibility.tabfocus to 7 in Firefox's about:config.


I agree with Joseph,  my experience with  ClickOnce is its great for the vast majority of projects especially in a corporate environment where it makes build, publish and deployment easy. Implementing the "forced upgrade" to ensure users have the latest version when running is so much easier in ClickOnce, and a main reason for my usage of it.

Issues with ClickOnce: In a corporate environment it has issues with proxy servers and the workarounds are less than ideal. I've had to deploy a few apps in those cases from UNC paths...but you can't do that all the time. Its "sandbox" is great, until you want to find the executable or create a desktop shortcut.  

Have not deployed out of 2008 yet so not sure if those issues still exist.


Does the Fox app use .CDX indexes? If so, you might be able to improve performance by adding indexes without needing to change any program code. If it uses .IDX indexes, though, the change would have to be done in the actual app.


To get rid of the _AFXDLL error, have you tried changing to the settings to use MFC as a static lib instead of a DLL?  This is similar to what you're already doing in changing the runtime libs to static instead of DLL.


In PHP 5 you can use SoapClient on the WSDL to call the web service functions. For example:

$client = new SoapClient("some.wsdl");


and $client is now an object which has class methods as defined in some.wsdl. So if there was a method called getTime in the WSDL then you would just call:

$result = $client-&gt;getTime();


And the result of that would (obviously) be in the $result variable. You can use the __getFunctions method to return a list of all the available methods.


One consideration would be whether video playback is via progressive download or streaming. If it's progressive download, then I would say use Flash because you get a wider audience reach.

For streaming wmv, it is out of the box functionality provided by Windows Media Services

For streaming flash, you will have to install a streaming server on your Windows box. Some options are:


Adobe Flash Media Server (Commercial)
Wowza Media Server (Free/Commercial) 
Red5 Flash Server (Open Source)



Have you tried using SetWindowPos. This is the canonical function for moving, resizing and setting z-order in Windows. There is a SWP_NOACTIVATE flag you can use. Look at http://msdn.microsoft.com/en-us/library/ms633545(VS.85).aspx. I have not tried this on a window belonging to another process, but it is probably worth a try.


We've been using a combination of 


TWiki
OpenGrok for the codebase
usenet
LotusNotes based system


As long as there is a google search appliance pointed at these things I think it's ok to have any or many versions as long as people use them


There is another way which avoids tempdata. The pattern I like involves creating 1 action for both the original render and re-render of the invalid form. It goes something like this:

var form = new FooForm();

if (request.UrlReferrer == request.Url)
{
     // Fill form with previous request's data
}

if (Request.IsPost())
{
     if (!form.IsValid)
     {
         ViewData["ValidationErrors"] = ...
     } else {
         // update model
         model.something = foo.something;
         // handoff to post update action
         return RedirectToAction("ModelUpdated", ... etc);
     }
}

// By default render 1 view until form is a valid post
ViewData["Form"] = form;
return View();


That's the pattern more or less. A little pseudoy. With this you can create 1 view to handle rendering the form, re-displaying the values (since the form will be filled with previous values), and showing error messages.

When the posting to this action, if its valid it transfers control over to another action.

I'm trying to make this pattern easy in the .net validation framework as we build out support for MVC.


Yes and No.

No, you cannot limit the results within the LinqDataSource control. Because Linq uses deferred execution, the expectation is that the presentation control will do the recordset limits.

Yes, you can do this with a ListView control. The trick is to use the DataPager control within the LayoutTemplate, like so:

&lt;LayoutTemplate&gt;
  &lt;div id="itemPlaceholder" runat="server" /&gt;
  &lt;asp:DataPager ID="DataPager1" runat="server" PageSize="3"&gt;
  &lt;/asp:DataPager&gt;            
&lt;/LayoutTemplate&gt;


Normally, you would include controls inside the DataPager like first, last, next, and previous. But if you just make it empty, then you will only see the three results that you desire.

Hope this helps.


There's a couple of small cleanups you can make...

package
{
    import flash.filesystem.File;

    public class UserUtil
    {
        public static function get currentOSUser():String
        {
            var userDir:String = File.userDirectory.nativePath;
            var userName:String = userDir.substr(userDir.lastIndexOf(File.separator) + 1);
            return userName;
        }
    }
}


As Kevin suggested, use File.separator to make the directory splitting cross-platform (just tested on Windows and Mac OS X).

You don't need to use resolvePath("") unless you're looking for a child.

Also, making the function a proper getter allows binding without any further work.

In the above example I put it into a UserUtil class, now I can bind to UserUtil.currentOSUser, e.g:

&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;mx:WindowedApplication xmlns:mx="http://www.adobe.com/2006/mxml" layout="absolute"&gt;
    &lt;mx:Label text="{UserUtil.currentOSUser}"/&gt; 
&lt;/mx:WindowedApplication&gt;



I used the update url and I installed the JavaHL adapter, the Subclipse project itself and the SVNKit adapter BETA.

After this it worked fine for me, this is for linux platform hope it works for you.


I would strongly advocate you look at NetworkX. It's a battle-tested war horse and the first tool most 'research' types reach for when they need to do analysis of network based data. I have manipulated graphs with 100s of thousands of edges without problem on a notebook. Its feature rich and very easy to use. You will find yourself focusing more on the problem at hand rather than the details in the underlying implementation.

Example of Erdős-Rényi random graph generation and analysis


"""
Create an G{n,m} random graph with n nodes and m edges
and report some properties.

This graph is sometimes called the Erd##[m~Qs-Rényi graph
but is different from G{n,p} or binomial_graph which is also
sometimes called the Erd##[m~Qs-Rényi graph.
"""
__author__ = """Aric Hagberg (hagberg@lanl.gov)"""
__credits__ = """"""
#    Copyright (C) 2004-2006 by 
#    Aric Hagberg 
#    Dan Schult 
#    Pieter Swart 
#    Distributed under the terms of the GNU Lesser General Public License
#    http://www.gnu.org/copyleft/lesser.html

from networkx import *
import sys

n=10 # 10 nodes
m=20 # 20 edges

G=gnm_random_graph(n,m)

# some properties
print "node degree clustering"
for v in nodes(G):
    print v,degree(G,v),clustering(G,v)

# print the adjacency list to terminal 
write_adjlist(G,sys.stdout)


Visualizations are also straightforward:



More visualization: http://jonschull.blogspot.com/2008/08/graph-visualization.html


I've actually done a small test like this in python on a website I maintain and found that they are almost equivalent in speed, with the procedural approach winning by something like ten-thousandths of a second, but that the OO code was so significantly cleaner I didn't continue the exercise any longer than one iteration.

So really, it doesn't matter (in my experience anyway).


I think John is correct. 

"My main concern is that instantiating and disposing one huge DataContext class all the time for individual operations that relate to specific areas of the Database would be impose an unnecessary imposition on application resources"

How do you support that statement? What is your experiment that shows that a large DataContext is a performance bottleneck? Having multiple datacontexts is a lot like having multiple databases and makes sense in similar scenarios, that is, hardly ever. If you are working with multiple datacontexts you need to keep track of which objects belong to which datacontext and you can't relate objects that are not in the same data context. That is a costly design smell for no real benefit. 

@Evan "The DataContext (or Linq to Entities ObjectContext) is more of a "unit of work" than a connection"
That is precisely why you should not have more than one datacontext. Why would you want more that one "unit of work" at a time?


CGAL is a C++ library that has data structures and algorithms used in Computational Geometry.


As already mentioned, NetworkX is very good, with another option being igraph. Both modules will have most (if not all) the analysis tools you're likely to need, and both libraries are routinely used with large networks.


Anthony Cramp's answer looked good to me. As he mentions the DCT transforms the data into the frequency domain. The DCT is heavily used in video compression as the human visual system is must less sensitive to high frequency changes, therefore zeroing out the higher frequency values results in a smaller file, with little effect on a human's perception of the video quality.

In terms of using the DCT to compare images, I guess the only real benefit is if you cut away the higher frequency data and therefore have a smaller set of data to search/match. Something like Harr wavelets may give better image matching results.


Server Core won't be very useful (to me at least, and I think many others as well) until they get a version of .Net framework on it.  Maybe a specialized subset like they have in the Compact Framework on smart phones.


Here's what I came up with

How can I merge in the test report?

First you'll need to get mbunit to generate both an XML and HTML report. The Command line arguments look like this

/rt:Xml /rt:Html /rnf:mbunit /rf:..\reports


this will generate the reports into a dir called reports and the file will be called mbunit.xml and mbunit.html

next we want to add these files as artifacts on the build

build\reports\* =&gt; Reports


the last step is to tell teamcity to add it as a tab for the build

find the .BuildServer\config\main-config.xml and add this line
(on windows this is in c:\Documents and Settings\, on linux it was in the /root dir)

&lt;report-tab title="Tests" basePath="Reports" startPage="mbunit.html" /&gt;


How can I display "Tests failed: 1 (1 new), passed: 3049" for the build?

TeamCity looks for a file called teamcity-info.xml where you can stick messages in to be displayed. The Actual test count is actually just plain text. I think you can just add the file as an artifact but I've also got it in the root dir of the build.

in NAnt you'll want to use this command to do an XSLT on the MBUnit XML Report

&lt;style style="includes\teamcity-info.xsl" in="reports\mbunit.xml" out="..\teamcity-info.xml" /&gt;


the actual xsl looks like this.
(Note: that the { and } are reserved in xsl so we have to use params)

&lt;?xml version="1.0" encoding="ISO-8859-1"?&gt;

&lt;xsl:stylesheet version="1.0"
xmlns:xsl="http://www.w3.org/1999/XSL/Transform"&gt;
&lt;xsl:param name="cbl" select="'{'"/&gt;
&lt;xsl:param name="cbr" select="'}'"/&gt;
&lt;xsl:template match="/"&gt;
&lt;xsl:for-each select="report-result/counter"&gt;

&lt;build number="1.0.{concat($cbl,'build.number',$cbr)}"&gt;
    &lt;xsl:if test="@failure-count &amp;gt; 0"&gt;
     &lt;statusInfo status="FAILURE"&gt;    
      &lt;text action="append"&gt; Tests failed: &lt;xsl:value-of select="@failure-count"/&gt;, passed: &lt;xsl:value-of select="@success-count"/&gt;&lt;/text&gt;
     &lt;/statusInfo&gt;
    &lt;/xsl:if&gt;
    &lt;xsl:if test="@failure-count = 0"&gt;
     &lt;statusInfo status="SUCCESS"&gt;
      &lt;text action="append"&gt; Tests passed: &lt;xsl:value-of select="@success-count"/&gt;&lt;/text&gt;
     &lt;/statusInfo&gt;
    &lt;/xsl:if&gt;

&lt;/build&gt;
&lt;/xsl:for-each&gt;

&lt;/xsl:template&gt;
&lt;/xsl:stylesheet&gt;


This will give you a file that looks like this

&lt;build number="1.0.{build.number}"&gt;
   &lt;statusInfo status="FAILURE"&gt;
      &lt;text action="append"&gt;Tests failed: 16, passed: 88&lt;/text&gt;
   &lt;/statusInfo&gt;
&lt;/build&gt;



Do you really care about the individual data points? Or will using the statistical aggregate functions on the day number instead suffice to tell you what you wish to know?


AVG
STDDEV_POP
VARIANCE
TO_DAYS



select
    timefield
from
    entries
where
    rand() = .01 --will return 1% of rows adjust as needed.


--not a mysql expert so I'm not sure how rand() operates in this environment.


You can use a static class member to hold the default:

class Shipment
{
    public static $DefaultWeight = '0';
    public function createShipment($startZip,$endZip,$weight=Shipment::DefaultWeight) {
        // your function
    }
}



NUnit is the way to go for you.
You could find some links here :


http://nunitasp.sourceforge.net/tutorial/
http://www.hanselman.com/blog/NUnitUnitTestingOfASPNETPagesBaseClassesControlsAndOtherWidgetryUsingCassiniASPNETWebMatrixVisualStudioWebDeveloper.aspx



Take a look at http://selenium.openqa.org/ it offers a good automated way to build unit tests hooking into the browser.  there is a nice firefox plugin for recording tests and can utilize almost any unit testing framework. We had a presentation/demo at our local user group meeting last month and it looked awesome.


Neat trick with boolean OR operator:

public function createShipment($startZip, $endZip, $weight = 0){
    $weight or $weight = $this-&gt;getDefaultWeight();
    ...
}



\0 will also match the entire matched expression without doing an explicit capture using parenthesis.

preg_replace("/[A-Z]/", "&lt;span class=\"initial\"&gt;\\0&lt;/span&gt;", $str)


As always, you can go to php.net/preg_replace or php.net/&lt;whatever search term&gt; to search the documentation quickly. Quoth the documentation:


  \0 or $0 refers to the text matched by the whole pattern. 



Even if you didn't want to actually edit in VS, you could create the project there and edit the files in another editor.


This isn't a full answer for you, but on the left join piece you can use the DefaultIfEmpty operator like so:

var collection = 
from u in db.Universe
join history in db.History on u.id = history.id into temp
from h in temp.DefaultIfEmpty()
where h.dateCol &lt; DateTime.Now.Date.AddDays(-1)
select u.id, u.name, h.dateCol ?? '1900-01-01'


I haven't had the need to do any groupby commands yet, so I left that out as to not send you down the wrong path.  Two other quick things to note.  I have been unable to actually join on two parameters although as above there are ways to get around it.  Also, the ?? operator works really well in place of the isnull in SQL.  


if you're not going to be using mylyn just uncheck that dependency. I'm not really familiar with Aptana, but in eclipse you can expand whats being installed and uncheck anything you don't need.


Why use a BST at all? From your description a dictionary will work just as well, if not better. 

The only reason for using a BST would be if you wanted to list out the contents of the container in key order. It certainly doesn't sound like you want to do that, in which case go for the hash table. O(1) insertion and search, no worries about deletion, what could be better?


CC.NET is simply the build server technology, not the build script technology. We use CC.NET at work to very successfully call MSBuild build scripts with no problems.

NAnt is an older and more mature build scripting language, but they are both similar in how they work. There are very few things I could do in NAnt that I can't also do in MSBuild, so it really comes down to which one you are more comfortable with. As far as how active NAnt is, don't go by when the last release was...instead go by when the last nightly build was. NAnt tends to go a long time between releases, but the nightly builds are usually pretty stable.


Doesn't using multiple open-id providers sort of undermine the point of open id?



  Doesn't using multiple open-id providers sort of undermine the point of open id?


No. Say you are using a Yahoo OpenID, but you decide to move to Google instead. Multiple OpenIDs per account allows you to associate your account with the Google OpenID, then deauthorize the Yahoo OpenID.


I used to get these all the time on Apache1/fastcgi. I think it's caused by fastcgi hanging up before Ruby is done. 

Switching to mongrel is a good first step, but there's more to do. It's a bad idea to cull from web services on live pages, particularly from Rails. Rails is not thread-safe. The number of concurrent connections you can support equals the number of mongrels (or Passenger processes) in your cluster. 

If you have one mongrel and someone accesses a page that calls a web service that takes 10 seconds to time out, every request to your website will timeout during that time. Most of the load balancers just cycle through your mongrels blindly, so if you have two mongrels, every other request will timeout.

Anything that can be unpredictably slow needs to happen in a job queue. The first hit to /slow/action adds the job to the queue, and /slow/action keeps on refreshing via page refreshes or queries via ajax until the job is finished, and then you get your results from the job queue.  There are a few job queues for Rails nowadays, but the oldest and probably most widely used one is BackgroundRB.

Another alternative, depending on the nature of your app, is to cull the service every N minutes via cron, cache the data locally, and have your live page read from the cache. 


Yea FogBugz is great for process-light, quick and easy task management.  It seems especially well suited for soloing, where you don't need or want a lot of complexity in that area.  

By the way, if you want to keep track of what you're doing at the computer all day, check out TimeSprite, which integrates with FogBugz.  It's a Windows app that logs your active window and then categorizes your activity based on the window title / activity type mappings you define as you go.  (You can also just tell it what you're working on.)  And if you're a FogBugz user, you can associate your work with a FogBugz case, and it will upload your time intervals for that case.  This makes accurate recording of elapsed time pretty painless and about as accurate as you can get, which in turn improves FogBugz predictive powers in its evidence-based scheduling.  Also, when soloing, I find that such specific logging of my time keeps me on task, in the way a meandering manager otherwise might. (I'm not affiliated with TimeSprite in any way.)


Your first step is to find and understand the parallelism in your problem. It is really easy to write multi-threaded code that performs no better than the single-threaded code it replaces. "Patterns for Parallel Programming" (Amazon) is a great introduction to the key concepts.

Once you have a workable design, start reading the articles in the "Concurrency" topic in the MSDN Magazine archives (link), particularly anything written by Jeff Richter. Those will give you the nuts and bolts stuff on the threading constructs specific to Windows and .NET. (The multi-threading section in Richter's "CLR via C# (Amazon)is short, but very insightful - highly recommended.)


There are many options and the best solution will depend on the nature of the problem you are trying to solve.  If you are trying to solve an embarassingly parallel problem then dividing and parallelising the tasks will be trivial.  In that case the challenge will come in distributing and managing the data used.  

Some suggestions would be:


ICE Grid which has bindings for .Net and other common languages
Velocity which is Microsoft's version of Oracle (Tangersol) Coherence
The forthcoming HPC offering from Microsoft Compute Cluster Server
Data Synapse Grid Server



It depends. If your business logic is in your click events and page loads, it is NOT acceptable.

It appears that your business logic is somewhere within the DAL (e.g., stored procedures and such), just as long as you are consistent, it's fine. As long as you are very, very sure that your clients will always be using SQL Server then this approach is not a problem.

I know a colleague who has all his business logic in stored procedures that his views are mostly thin clients to database backends: he has been immensely successful with the product that he sells. But that's only because he's very consistent with it.


Watch out for packing issues. In the example you gave all fields are at the obvious offsets because everything is on 4 byte boundaries but this will not always be the case. Visual C++ packs on 8 byte boundaries by default.


There's some stuff in the Apache Portable Runtime (APR) that I'd expect to be very solid.


Check out Chapter 6 of Programming Collective Intelligence


I have found these two links useful:

Using CLR and single audit table.
Creating a generic audit trigger with SQL 2005 CLR

Using triggers and separate audit table for each table being audited.
How do I audit changes to SQL Server data?


From the Schema Spy documentation it supports multiple databases (see below). It also supports data types, constraints and maps relationships up to two degrees of separation.

The Samples are well worth looking at.

Schema Spy Supported Databases

IBM DB2 with the 'App' Driver,IBM DB2 with the 'Net' Driver, Firebird, HSQLDB Server,   Microsoft SQL Server, MySQL, Oracle with OCI8 Driver, Oracle with Thin Driver, PostgreSQL, Sybase Server with JDBC3 Driver, Sybase Server with JDBC2 Driver, DB2 UDB Type 4 Driver


You might try a log viewer; this basically just lets you look at the transactions in the transaction log, so you should be able to find the statement that updated the row in question.  I wouldn't recommend this as a production-level auditing strategy, but I've found it to be useful in a pinch.

Here's one I've used; it's free and (only) works w/ SQL Server 2000.

http://www.red-gate.com/products/SQL_Log_Rescue/index.htm


There's a specific problem with mixing doPostback, UpdatePanel and SharePoint -- and the symptom is exactly what you're seeing: a full-page postback instead of an asynchronous postback. See this KB article for a workaround: http://support.microsoft.com/kb/941955


I definitely recommend Weka which is an Open Source Data Mining Software written in Java:


  Weka is a collection of machine learning algorithms for data mining tasks. The algorithms can either be applied directly to a dataset or called from your own Java code. Weka contains tools for data pre-processing, classification, regression, clustering, association rules, and visualization. It is also well-suited for developing new machine learning schemes.


As mentioned above, it ships with a bunch of different classifiers like SVM, Winnow, C4.5, Naive Bayes (of course) and many more (see the API doc).
Note that a lot of classifiers are known to have much better perfomance than Naive Bayes in the field of spam detection or text classification.

Furthermore Weka brings you a very powerful GUI…


To make it more readable and maintainable, you can also split it up into multiple LINQ statements.


First, select your data into a new list, let's call it x1, do a projection if desired
Next, create a distinct list, from x1 into x2, using whatever distinction you require
Finally, create an ordered list, from x2 into x3, sorting by whatever you desire 



IE prior to 8 has a temporal aspect to its box model that most notably creates a problem with percentage based widths. In your case here an absolutely positioned div by default has no width. Its width will be worked out based on the pixel width of its content and will be calculated after the contents are rendered. So at the point IE encounters and renders your relatively positioned div its parent has a width of 0 hence why it itself collapses to 0.

If you would like a more in depth discussion of this along with lots of working examples, have a gander here.


Depending on what version of IIS you're considering, I would second lbrandy's recommendation to check out PowerShell. Microsoft is working on a PowerShell provider for IIS (specifically version 7). There is a decent post about this at http://blogs.iis.net/thomad/archive/2008/04/14/iis-7-0-powershell-provider-tech-preview-1.aspx. The upcoming version of PowerShell will also add remoting capabilities so that you can remotely manage machines. PowerShell is quite different from *NIX shells, though, so that is something to consider.

Hope this helps.


Like what so many people have already indicated, the answer here is "it depends". There are some things like repeating operations that are much simpler and cleaner in NAnt. See the MSDN forums for a discussion about this.


I am not aware of any API to do get the OS's scheduler to do what you want (even if your thread is idle-priority, if there are no higher-priority ready threads, yours will run).  However, I think you can improvise a fairly elegant throttling function based on what you are already doing. Essentially (I don't have a Windows dev machine handy):

Pick a default amount of time the thread will sleep each iteration.  Then, on each iteration (or on every nth iteration, such that the throttling function doesn't itself become a significant CPU load),


Compute the amount of CPU time your thread used since the last time your throttling function was called (I'll call this dCPU).  You can use the GetThreadTimes() API to get the amount of time your thread has been executing.
Compute the amount of real time elapsed since the last time your throttling function was called (I'll call this dClock).
dCPU / dClock is the percent CPU usage (of one CPU).  If it is higher than you want, increase your sleep time, if lower, decrease the sleep time.
Have your thread sleep for the computed time.


Depending on how your watchdog computes CPU usage, you might want to use GetProcessAffinityMask() to find out how many CPUs the system has.  dCPU / (dClock * CPUs) is the percentage of total CPU time available.

You will still have to pick some magic numbers for the initial sleep time and the increment/decrement amount, but I think this algorithm could be tuned to keep a thread running at fairly close to a determined percent of CPU.


The problem is it's not normal to want to leave the CPU idle while you have work to do. Normally you set a background task to IDLE priority, and let the OS handle scheduling it all the CPU time that isn't used by interactive tasks. 

It sound to me like the problem is the watchdog process. 

If your background task is CPU-bound then you want it to take all the unused CPU time for its task.

Maybe you should look at fixing the watchdog program?


Be aware that Microsoft do not recommend that you static link the runtime into your project, as this prevents it from being serviced by windows update to fix critical security bugs. There are also potential problems if you are passing memory between your main .exe and .dll files as if each of these static links the runtime you can end up with malloc/free mismatch problems.

You can include the DLLs with the executable, without compiling them into the .exe and without running the redist tool - this is what I do and it seems to work fine. 

The only fly in the ointment is that you need to include the files twice if you're distributing for a wide range of Windows versions - newer OSs need the files in manifest-defined directories, and older ones want all the files in the program directory.


Do you really want to do run the regexp twice? Without having checked (bad me) I would have thought that this would work well:

public static string StripStringFormating(string formattedString)
{    
    return rTest.Replace(formattedString, string.Empty);
}


If it does, you should see it run ~twice as fast...


What's your application domain? It depends. 

Since you used the word "Agile", I'm guessing it's a web app. I have a nice easy answer for you. 

Go buy a copy of Burp Suite (it's the #1 Google result for "burp" --- a sure endorsement!); it'll cost you 99EU, or ~$180USD, or $98 Obama Dollars if you wait until November. 

Burp works as a web proxy. You browse through your web app using Firefox or IE or whatever, and it collects all the hits you generate. These hits get fed to a feature called "Intruder", which is a web fuzzer. Intruder will figure out all the parameters you provide to each one of your query handlers. It will then try crazy values for each parameter, including SQL, filesystem, and HTML metacharacters. On a typical complex form post, this is going to generate about 1500 hits, which you'll look through to identify scary --- or, more importantly in an Agile context, new --- error responses.

Fuzzing every query handler in your web app at each release iteration is the #1 thing you can do to improve application security without instituting a formal "SDLC" and adding headcount. Beyond that, review your code for the major web app security hot spots:


Use only parameterized prepared SQL statements; don't ever simply concatenate strings and feed them to your database handle.
Filter all inputs to a white list of known good characters (alnum, basic punctuation), and, more importantly, output filter data from your query results to "neutralize" HTML metacharacters to HTML entities (quot, lt, gt, etc). 
Use long random hard-to-guess identifiers anywhere you're currently using simple integer row IDs in query parameters, and make sure user X can't see user Y's data just by guessing those identifiers.
Test every query handler in your application to ensure that they function only when a valid, logged-on session cookie is presented.
Turn on the XSRF protection in your web stack, which will generate hidden form token parameters on all your rendered forms, to prevent attackers from creating malicious links that will submit forms for unsuspecting users.
Use bcrypt --- and nothing else --- to store hashed passwords.



If you do go for creating several projects, make sure everyone who adds code to the solution is fully aware of the intention of them and do everything you can to get them to understand the dependencies between the projects. If you have ever tried to sort out the mess when someone has gone and added references that shouldn't have been there and got away with it for weeks you will understand this point


@lassevk: I agree with these rules, and have one more to add.

When I have nested classes, I still split them out, one per file.  Like this:

// ----- Foo.cs
partial class Foo
{
    // Foo implementation here
}


and

// ----- Foo.Bar.cs
partial class Foo
{
    class Bar
    {
        // Foo.Bar implementation here
    }
}



Todd Bleeker at Mindsharp showed me a piece of code he wrote that can use Ajax on Sharepoint 2.0.  It was pretty cool.  I believe the company used it on their sharepoint site managment software if you want to take a look.  (you used to be able to request a 30 day trial).  I bet how to do it is on their yahoo group (I can't remember the name, but I am sure that if you search for mindsharp you'll find it.)

As a note, Ajax has been around for a long time.  Microsoft easily supported it since 2002 maybe earlier with the release of IE 5.5 (I don't know about other browsers, I was doing internal development and we only supported ie at the time).  It just wasn't called that.  The term Ajax is nothing more than a marketing term that someone coined later on.


Tree Surgeon is a great tool which creates an empty .NET development tree.  It has been tweaked over years of use and implements lots of best practices.


I think the basic idea is good.  

The nice thing about building all the tables first and then building all the constraints, is that the tables can be created in any order.  When I've done this I had one file per table, which I put in a directory called "Tables" and then a script which executed all the files in that directory.  Likewise I had a folder for constraint scripts (which did foreign key and indexes too), which were executed when after the tables were built.

I would separate the build of the triggers and stored procedures, and run these last.  The point about these is they can be run and re-run on the database without affecting the data.  This means you can treat them just like ordinary code.  You should include "if exists...drop" statements at the beginning of each trigger and procedure script, to make them re-runnable.

So the order would be


table creation
add indexes
add constraints


Then


add triggers
add stored procedures


On my current project we are using MSBuild to run the scripts.  There are some extension targets that you can get for it which allow you to call sql scripts.  In the past I have used perl which was fine too (and batch files...which I would not recommend - the're too limited).


Getting the latest service pack for SharePoint 2007 will resolve your problem (and add full support for AJAX).  Without the service pack you will need to follow an example like that outlined in this article:

AjaxBasePart: Easy ASP.NET 2.0 AJAX Extensions 1.0 and Office SharePoint Server 2007

Posting this hear so that people know there is an answer even without the latest service pack (secretGeek's response seems to say there is no chance).


I was also wondering how to do this, when I found grom's answer.  Thanks for the code.

I struggled with understanding how the code was supposed to be used.  (I don't use a version control system.)  In summary, you include the timestamp (ts) when you call the stylesheet.  You're not planning on changing the stylesheet often:

&lt;?php 
    include ('grom_file.php');
    // timestamp on the filename has to be updated manually
    include_css('_stylesheets/style.css?ts=20080912162813', 'all');
?&gt;



There are some optimizations you can use when filling a DataTable, such as calling BeginLoadData(), inserting the data, then calling EndLoadData().  This turns off some internal behavior within the DataTable, such as index maintenance, etc.  See this article for further details.


On major difference is that DataSets can hold multiple tables and you can define relationships between those tables. 

If you are only retuning a single result set though I would think a DataTable would be more optimized. I would think there has to be some overhead (granted small) to offer the functionality a DataSet does and keep track of multiple DataTables. 


We use both Buildbot and Hudson for Jython development.  Both are useful, but have different strengths and weaknesses.

Buildbot's configuration is pure Python and quite simple once you get the hang of it (look at the epydoc-generated API docs for the most current info).  Buildbot makes it easier to define non-testing tasks and distribute the testers.  However, it really has no concept of individual tests, just textual, HTML, and summary output, so if you want to have multi-level browsable test output and so forth you'll have to build it yourself, or just use Hudson.

Hudson has terrific support for drilling down from overall results into test suites and individual tests; it also is great for comparing test output between builds, but the distributed (master/slave) stuff is comparatively more complicated because you need a Java environment on the slaves too; also, Hudson is less tolerant of flaky network links between the master and slaves.

So, to get the benefits of both tools, we run a single instance of Hudson, which catches the common test failures, then we do multi-platform regression with Buildbot.

Here are our instances:


Jython Hudson
Jython buildbot



I had this same issue. The way I got round this was to use the Selecting event on the LinqDataSource and return the result manually.

e.g.

protected void lnqRecentOrder_Selecting(object sender, LinqDataSourceSelectEventArgs e)
{
    DataClassesDataContext dx = new DataClassesDataContext();
    e.Result = (from o in dx.Orders
                where o.CustomerID == Int32.Parse(Request.QueryString["CustomerID"])
                select o).Take(5);
}



You could base your Linq query on a stored proc that only returns x number of rows using a TOP statement. Remember just because you can do all your DB code in Linq doesn't mean you should. Plus, you can tell Linq to use the same return type for the stored proc as the normal table, so all your binding will still work, and the return results will be the same type


Subclipse does not require Mylyn, but the update site includes a plugin that integrates Mylyn and Subclipse.  This is intended for people that use Mylyn.  In your case, you would want to just de-select Mylyn in the update dialog.

Subclipse also requires Subversion 1.5 and the corresponding version of the JavaHL native libraries.  I have written the start of an FAQ to help people understand JavaHL and how to get it.  See: http://desktop-eclipse.open.collab.net/wiki/JavaHL


Kev posted an example over here that implements his suggestion to "do this in a CLR integration assembly and use the FTP classes in the System.Net namespace to build a simple FTP client."


SQL Server 2005 "Standard", "Developer" and "Enterprise" editions have SSIS, which replaced DTS from Sql server 2000. SSIS has a built in connection to it's own DB, and you can find a connection that someone else has written for MySQL. Here is one example. Once you have your connections, you should be able to create an SSIS package that moves data between the two. 

I have not had to move data from SQlServer to MySQL, but I imagine that once the MySQl connection is installed, it works the same as moving data between two SQLServer DBs, which is pretty straight forward.


Using MSSQL Management Studio i've transitioned tables with the MySQL OLE DB.  Right click on your database and go to "Tasks->Export Data" from there you can specify a MsSQL OLE DB source, the MySQL OLE DB source and create the column mappings between the two data sources.  

You'll most likely want to setup the database and tables in advance on the MySQL destination (the export will want to create the tables automatically, but this often results in failure). You can quickly create the tables in MySQL using the "Tasks->Generate Scripts" by right clicking on the database.  Once your creation scripts are generated you'll need to step through and search/replace for keywords and types that exist in MSSQL to MYSQL. 

Of course you could also backup the database like normal and find a utility which will restore the MSSQL backup on MYSQL. I'm not sure if one exists however.


I'm actually dealing with exactly this kind of stuff for my company's video game.  The most useful info I've found are at these two links:

Paul Bourke's page at UWA, with his 1989 paper on Delaunay and a series of implementation links.

A great explanation of the psudocode and a visual of doing Delaunay at codeGuru.com.

In terms of rendering these - most of the implementations I've found will need massaging to get what you'd want, but since using this for a game map would lead to a number of points plus lines between them, it could be a very simple matter to do draw this out to screen.


You might also want to use something like BCEL to analyse which lines of source actually exist in the byte-code.  You don't want to report that things like blank lines and comments haven't been covered.


SAP is launching the Enterprise Social Messaging Experiment right now.  This appears to be a twitter-inspired social network for SAP practitioners.

I really hope it takes off because there just aren't that many people in my organization doing the work that I do.  I'd love to interact with people outside of my company who are working in this domain.

Site: http://www.esme.us/esme/static/about
Blog: http://blog.esme.us/


@Balloon
If you are using TortoiseSVN, you can use the packaged SubWCRev program. It queries a working copy and tells you just the highest revision number. Admittedly, this seems to be a client-side approach to a server-side problem, but since it's a nice command line program, you should be able to capture its output for use fairly easily.


If you are intending on reading a large number of columns or records it's also worth caching the ordinals and accessing the strongly-typed methods, e.g.

using (DbDataReader dr = cmd.ExecuteReader()) {
  if (dr.Read()) {
    int idxColumnName = dr.GetOrdinal("columnName");
    int idxSomethingElse = dr.GetOrdinal("somethingElse");

    do {
      Console.WriteLine(dr.GetString(idxColumnName));
      Console.WriteLine(dr.GetInt32(idxSomethingElse));
    } while (dr.Read());
  }
}



If you have installed SharePoint 2007 (without Service Pack 1) then you can follow an example like the following article:

AjaxBasePart: Easy ASP.NET 2.0 AJAX Extensions 1.0 and Office SharePoint Server 2007

The reason for this is that there exists a specific problem with mixing doPostback, UpdatePanel and SharePoint -- and the symptom is exactly what you're seeing: a full-page postback instead of an asynchronous postback. See this KB article for a workaround: A Web Part that contains an ASP.NET AJAX 1.0 UpdatePanel control that uses the _doPostBack() ...

Otherwise you can just install Service Pack 1 to fix your problem:

Windows SharePoint Services 3.0 Service Pack 1 (SP1)


If you're using Apache, you can use mod_rewrite to statically cache your web pages. Lets say you're using PHP, and you have a request for "/somepage.php". In your .htaccess file you put the following:

RewriteEngine on
RewriteCond %{QUERY_STRING} ^$ # let's not cache urls with queries
RewriteCond %{REQUEST_METHOD} ^GET$ # or POST/PUT/DELETE requests
RewriteCond static_cache/%{REQUEST_URI} -s # Check that this file exists and is &gt; 0 bytes
RewriteRule (^.*$) static_cache$1 [L] # If all the conditions are met, we rewrite this request to hit the static cache instead


If your cache turns up empty, the request is handled by your php script as usual, so now it's simply a matter of making your php script store the resulting html in the cache. The simplest way to do this is using another htaccess rule to prepend end append a couple of php files to all your php requests (this might or might not be a good idea, depending on your application):

php_value auto_prepend_file "pre_cache.php"
php_value auto_append_file "post_cache.php"


Then you'd do something like this:

pre_cache.php:

ob_start();


post_cache.php:

$result = ob_get_flush();
if(!$_SERVER['QUERY_STRING']) { # Again, we're not caching query string requests
  file_put_contents("static_cache/" + __FILE__, $result);
}


With some additional regular expressions in the .htaccess file we could probably start caching query string requests as well, but I'll leave that as an exercise for the reader :)


I really think it is better to split the project as well, but it all depends on the size of the project and the number of people working on it. 

For larger projects, I have a projects for  


data access (models)  
services  
front end  
tests  


I got the model from Rob Connery and his storefront application... seems to work really well. 

mvc-storefront


I found this resource to be very helpful

Its a wrapper round the HttpContext.Current.GetGlobalResourceString and HttpContext.Current.GetLocalResourceString that allows you to call the resources like this...

// default global resource
Html.Resource("GlobalResource, ResourceName")

// global resource with optional arguments for formatting
Html.Resource("GlobalResource, ResourceName", "foo", "bar")

// default local resource
Html.Resource("ResourceName")

// local resource with optional arguments for formatting
Html.Resource("ResourceName", "foo", "bar")


The only problem I found is that controllers don't have access to local resouce strings.


Really stupid question: Are you sure the string is being truncated, and not just broken at the linebreak you specify (and possibly not showing in your interface)? Ie, do you expect the field to show as


  This will be inserted \n This will not
  be


or 


  This will be inserted 
  
  This will not be


Also, what interface are you using? Is it possible that something along the way is eating your backslashes?


Its worth mentioning that the ActiveMQ open source project defines a C# API for messaging called NMS which allows you to develop against a single C# / .Net API that can then use various messaging back ends such as


ActiveMQ 
MSMQ
TibCo's EMS
any STOMP provider
any JMS provider via StompConnect



@Eridius' answer is the preferred way to do it. Requiring a process to register a name may have unintended side-effects such as increasing the visibility of the process not to mention the hassle of coming up with unique names when you have lots of processes.


When I need to do paging, I typically use a temporary table as well.  You can use an output parameter to return the total number of records.  The case statements in the select allow you to sort the data on specific columns without needing to resort to dynamic SQL.

--Declaration--

--Variables
@StartIndex INT,
@PageSize INT,
@SortColumn VARCHAR(50),
@SortDirection CHAR(3),
@Results INT OUTPUT

--Statements--
SELECT @Results = COUNT(ID) FROM Customers
WHERE FirstName LIKE '%a%'

SET @StartIndex = @StartIndex - 1 --Either do this here or in code, but be consistent
CREATE TABLE #Page(ROW INT IDENTITY(1,1) NOT NULL, id INT, sorting_1 SQL_VARIANT, sorting_2 SQL_VARIANT)
INSERT INTO #Page(ID, sorting_1, sorting_2)
SELECT TOP (@StartIndex + @PageSize)
    ID,
    CASE
     WHEN @SortColumn='FirstName' AND @SortDirection='ASC' THEN CAST(FirstName AS SQL_VARIANT)
     WHEN @SortColumn='LastName' AND @SortDirection='ASC' THEN CAST(LastName AS SQL_VARIANT)
     ELSE NULL
    END AS sort_1,
    CASE
     WHEN @SortColumn='FirstName' AND @SortDirection='DES' THEN CAST(FirstName AS SQL_VARIANT)
     WHEN @SortColumn='LastName' AND @SortDirection='DES' THEN CAST(LastName AS SQL_VARIANT)
     ELSE NULL
    END AS sort_2
FROM (
    SELECT
     CustomerId AS ID,
     FirstName,
     LastName
    FROM Customers
    WHERE
     FirstName LIKE '%a%'
) C
ORDER BY sort_1 ASC, sort_2 DESC, ID ASC;

SELECT
    ID,
    Customers.FirstName,
    Customers.LastName
FROM #Page
INNER JOIN Customers ON
    ID = Customers.CustomerId
WHERE ROW &gt; @StartIndex AND ROW &lt;= (@StartIndex + @PageSize)
ORDER BY ROW ASC

DROP TABLE #Page



Debian's .deb packages are just "ar" archives containing tarballs.  You can manipulate both types of files using cygwin or msys quite easily:

$ ar xv asciidoc_8.2.1-2_all.deb 
x - debian-binary
x - control.tar.gz
x - data.tar.gz

$ tar -tzf control.tar.gz 
./
./conffiles
./md5sums
./control


Or you can install all the "standard" Debian stuff using cygwin, I suppose, but most of that stuff won't benefit you much if you're building a .Net app anyway.


We are using Bitten wich is integrated with trac. And it's python based.


The way I understand things, Samba created tdb to allow "multiple concurrent writers" for any particular database file.  So if your workload has multiple writers your performance may be bad.  On the other hand, if your workload has lots of readers, then the question is how well your operating system handles multiple readers.


The closest you are going to get is using a Dictionary (as mentioned by Pacifika)

Dim objDictionary
Set objDictionary = CreateObject("Scripting.Dictionary")
objDictionary.CompareMode = vbTextCompare 'makes the keys case insensitive'
objDictionary.Add "Name", "Scott"
objDictionary.Add "Age", "20"


But I loop through my dictionaries like a collection

For Each Entry In objDictionary
  Response.write objDictionary(Entry) &amp; "&lt;br /&gt;"
Next


You can loop through the entire dictionary this way writing out the values which would look like this:

Scott
20


You can also do this

For Each Entry In objDictionary
  Response.write Entry &amp; ": " &amp; objDictionary(Entry) &amp; "&lt;br /&gt;"
Next


Which would produce

 Name: Scott
 Age: 20



If you are looking for an automation tool, I have often worked with EMS SQLManager, which allows you to generate automatically a ddl script from a database.

Data inserts in reference tables might be mandatory before putting your database on line. This can even be considered as part of the ddl script. EMS can also generate scripts for data inserts from existing databases.

Need for indexes might not be properly estimated at the ddl stage. You will just need to declare them for primary/foreign keys. Other indexes should be created later, once views and queries have been defined 


In short: don't do this by hand, link/use existing software.  And sain_grocen's answer is incorrect.  :(

These are all tests for significance of parameter estimates that are typically used in Multivariate response Multiple Regressions.  These would not be simple things to do outside of a statistical programming environment.  I would suggest either getting the output from a pre-existing statistical program, or using one that you can link to and use that code.

I'm afraid that the first answer (sain_grocen's) will lead you down the wrong path.  His explanation is likely of a special case of what you are actually dealing with.  The anova explained in his links is for a single variate response, in a balanced design.  These aren't the F statistics you are seeing.  The names in your output (Pillai's Trace, Hotelling's Trace,...) are some of the available multivariate versions.  They have F distributions under certain assumptions.  I can't explain a text books worth of material here, I would advise you to start by looking at 
"Applied Multivariate Statistical Analysis" by Johnson and Wichern


Keep in mind that usually &lt;> always.

Since WAS 6.0 and up you usually want to setup more than one node in each physical computer, given the usual power of the server you use the node to separate logical business entities.

Like for example have 6 nodes, 3 in each of 2 machines and have 1 pair of nodes you could define 3 different clusters one for each stage (dev, qa, staging) and making each cluster be invisible to the other.


While it's possible to create lots of test data and see what happens, it's more efficient to try to minimize the data being used.

From a typical QA perspective, you would want to identify different classifications of inputs. Produce a set of input values for each classification and determine the appropriate outputs. 

Here's a sample of classes of input values


valid triangles with small numbers such as (1 billion, 2, billion, 2 billion)
valid triangles with large numbers such as (0.000001, 0.00002, 0.00003)
valid obtuse triangles that are 'almost'flat such as (10, 10, 19.9999)
valid acute triangles that are 'almost' flat such as (10, 10, 0000001)
invalid triangles with at least one negative value
invalid triangles where the sum of two sides equals the third
invalid triangles where the sum of two sides is greater than the third
input values that are non-numeric


...

Once you are satisfied with the list of input classifications for this function, then you can create the actual test data.  Likely, it would be helpful to test all permutations of each item.  (e.g. (2,3,4), (2,4,3), (3,2,4), (3,4,2), (4,2,3), (4,3,2)) Typically, you'll find there are some classifications you missed (such as the concept of inf as an input parameter).

Random data for some period of time may be helpful as well, that can find strange bugs in the code, but is generally not productive.

More likely, this function is being used in some specific context where additional rules are applied.(e.g. only integer values or values must be in 0.01 increments, etc.) These add to the list of classifications of input parameters.


You're going to want to use the join into construct to create a group query.

TestContext db = new TestContext(CreateSparqlTripleStore());
var q = from a in db.Album
        join t in db.Track on a.Name equals t.AlbumName into tracks
        select new Album{Name = a.Name, Tracks = tracks};
foreach(var album in q){
    Console.WriteLine(album.Name);
    foreach (Track track in album.Tracks)
    {
        Console.WriteLine(track.Title);
    }
}



Can you explain more why SPSS itself isn't a fine solution to the problem?  Is it that it generates pivot tables as output that are hard to manipulate?  Is it the cost of the program?  

F-statistics can arise from any number of particular tests.  The F is just a distribution (loosely:  a description of the "frequencies" of groups of values), like a Normal (Gaussian), or Uniform.  In general they arise from ratios of variances.  Opinion: many statisticians (myself included), find F-based tests to be unstable (jargon:  non-robust).

The particular output statistics (Pillai's trace, etc.) suggest that the original analysis is a MANOVA example, which as other posters describe is a complicated, and hard to get right procedure.

I'm guess also that, based on the MANOVA, and the use of SPSS, this is a psychology or sociology project... if not please enlighten.  It might be that other, simpler models might actually be easier to understand and more repeatable.  Consult your local university statistical consulting group, if you have one.  

Good luck!  


I'm not sure if this will help with what you are working on, but long ago I wrote a regular expression to parse ANSI graphic files.

(?s)(?:\e\[(?:(\d+);?)*([A-Za-z])(.*?))(?=\e\[|\z)


It will return each code and the text associated with it.

Input string:

&lt;ESC&gt;[1;32mThis is bright green.&lt;ESC&gt;[0m This is the default color.


Results:

[ [1, 32], m, This is bright green.]
[0, m, This is the default color.]



There are two varients of UNION. 

'UNION' and 'UNION ALL'


In most cases what you really want to say is UNION ALL as it does not do duplicate elimination (Think SELECT DISTINCT) between sets which can result in quite a bit of savings in terms of execution time.

Others have suggested multiple result sets which is a workable solution however I would caution against this in time sensitive applications or applications connected over WANs as doing so can result in significantly more round trips on the wire between server and client.


Haven't done it yet with WCF but plan to have a local DNS entry pointing to our Network Load Balancing (NLB) virtual iP address which will direct all traffic to one of our servers hosting services within IIS. I have used NLB for this exact scenario in the past for web sites and see no reason why it will not work well with WCF. 

The beauty of it is that you can take servers in and out of the virtual cluster at will and NLB takes care of all the ugly re-directing to an available node. It also comes with a great price tag: $FREE with your Windows Server license.


It should be pointed out that you don't want to encrypt the password, you want to hash it.

Encrypted passwords can be decrypted, letting someone see the password. Hashing is a one-way operation so the user's original password is (cryptographically) gone.



As for which algorithm you should choose - use the currently accepted standard one:


SHA-256


And when you hash the user's password, be sure to also hash in some other junk with it. e.g.:


password: password1
salt: PasswordSaltDesignedForThisQuestion


Append the salt to the user's password: 

String s = HashStringSHA256("password1PasswordSaltDesignedForThisQuestion");



Whatever you do, don't write your own encryption algorithm.  Doing this will almost guarantee (unless you're a cryptographer) that there will be a flaw in the algorithm that will make it trivial to crack.


Want to slow down the hashing function?


$seed = sha1(uniqid(rand(), true));
$hash = sha1($seed . $password);
for($k=0; $k

I think there's enough hashing to slow down the algorithm a little, and making a very strong hash.


If you use Firebird or InterBase, there's a tool called IBDesc that does a great job.


As the problem is with writes, I would look more towards >removing&lt; any unneeded indexes on the tables. As is common in RDBMS, every index on a FoxPro table slows down a write operation as the indexes need to be updated, and as you aren't reading directly from (or presumably directly querying) the table you shouldn't need very many indexes. You might also want to look at any triggers or field rules on the tables as they may be slowing down the write operation. Be sure your referential integrity is still preserved, though..


Don't waste your time on SPAM filtering usages.
Spammers easily bypass Bayesian filtering by adding random text to their spam emails.

FIX:
OK,OK, Bayesian filtering can be useful when trained personally.
However, at a corporate level or above, its probably useless.


Basically,

A server is a runtime environment, a process of execution.
A node is a grouping of servers that share common configuration. It is a physical machine.
A cell is a grouping of nodes into a sigle administrative domain. For websphere, it mean that if you group several servers within a cell, then you can administer them with one Websphere admin console

Hope this helps!


Although I'm not familiar with afxext.h, I am wondering what about it makes it incompatible with Windows NT4.... 

However, to answer the original question:
"My research to date indicates that it is impossible to build an application for execution on Windows NT 4.0 using Visual Studio (C++, in this case) 2005."

The answer should be yes especially if the application was originally written or running on NT4!  With the afxext.h thing aside, this should be an easy YES.

The other thing I am finding trouble with is the loose nature in which people are throwing out the NT term.  Granted most people think of 'NT' as Windows NT4 but it's still ambiguous because 'most people' is not equal to 'all people.'

In reality the term 'NT' is equal to the NT series. The NT series is NT3, NT4, NT5 (2000, XP, 2003) and NT6 (Vista).

Win32 is a subsystem which you target your C/C++ code too.  So I see no reason why one should not be able target this NT4 platform &amp; subsystem or, if this is a platform porting excercise, remove the MFC dependencies that VC is possibly imposing.

Adding the afxext.h to the mix, it sounds to me like a subsystem compatibility issue.  It's part of MFC from my Google research.  The afxext.h appears to be the MFC (Microsoft Foundation Class) extensions.

Can you remove your dependency on MFC?  What type of application is this? (CLR, service, GUI interface?)  Can you convert project to an unmanaged C++ project in VC 8.0?

Hopefully some of this will help you along.


MSMQ (Microsoft Message Queueing) may be a great choice. It is part of the OS and present as an optional component (can be installed via Add/Remove Programs / Windows Components), meaning it's free (as long you already paid for Windows, of course). MSMQ provides Win32/COM and System.Messaging APIs. More modern Windows Communication Foundation (aka Indigo) queued channels also use MSMQ. 
Note that MSMQ is not supported on Home SKUs of Windows (XP Home and Vista Home)  


You should also use the Sandcastle Help File Builder. It provides you with a ndoc like GUI for generating help files so you don't have to do anything from a command prompt.

Welcome to the Sandcastle Help File Builder Project


Best option: I would instead recommend to use a standard date picker.

Alternative: every time the content of the edit control changes, parse it and display (in a separate control?) the long format of the date (ie: input "03/04/09" display "Your input: March 4, 2009")


I find it highly unlikely for Postgres to truncate your data on input - it either rejects it or stores it as is.

milen@dev:~$ psql
Welcome to psql 8.2.7, the PostgreSQL interactive terminal.

Type:  \copyright for distribution terms
       \h for help with SQL commands
       \? for help with psql commands
       \g or terminate with semicolon to execute query
       \q to quit

milen=&gt; create table EscapeTest (text varchar(50));
CREATE TABLE
milen=&gt; insert into EscapeTest (text) values ('This will be inserted \n This will not be');
WARNING:  nonstandard use of escape in a string literal
LINE 1: insert into EscapeTest (text) values ('This will be inserted...
                                              ^
HINT:  Use the escape string syntax for escapes, e.g., E'\r\n'.
INSERT 0 1
milen=&gt; select * from EscapeTest;
          text
------------------------
 This will be inserted
  This will not be
(1 row)

milen=&gt;



You could also try the other free open source OLAP server, PALO from Jedox (www.palo.net)


I would recommend not using them and going for last-modified headers instead.

Askapache has a useful article on this. (as they do pretty much everything it seems!)

http://www.askapache.com/htaccess/apache-speed-etags.html


An alternative to FlexUnit is the AsUnit testing tools. There are versions for actionscript 2 and 3. It also has good integration with Project Sprouts, which is a build tool for Flex and Flash similar to ant, however it uses ruby rake tasks and includes excellent dependency management along the lines of maven.

No IDE integration that I know of however.


On Solaris, you can get detailed information on a process's memory usage with the pmap command. In particular, pmap -x &lt;pid&gt; shows you how much of a process's memory is shared and how much is specifically used by that process. This is useful for working out the "marginal" memory usage of a process -- with this technique you can avoid double-counting shared libraries.


Unhandled exception behavior in a .NET 1.x WinForms app depends on:


The type of thread that threw the exception
Whether it occurred during window message processing
Whether a debugger was attached to the process
The DbgJitDebugLaunchSetting registry setting
The jitDebugging flag in App.Config
Whether you overrode the WinForms exception handler
Whether you handled the CLR’s exception event
The phase of the moon


The default behaviour of unhandled exceptions is:


If the exception occurs on the main thread when pumping window messages, it's intercepted by the Windows Forms exception handler.
If the exception occurs on the main thread when pumping window messages, it will terminate the app process unless it's intercepted by the Windows Forms exception handler.
If the exception occurs on a manual, threadpool, or finalizer thread, it's swallowed by the CLR.


The points of contact for an unhandled exception are: 


Windows Forms exception handler.
The JIT-debug registry switch DbgJitDebugLaunchSetting.
The CLR unhandled exception event.


The Windows Form built-in exception handling does the following by default:


Catches an unhandled exception when:

exception is on main thread and no debugger attached.
exception occurs during window message processing.
jitDebugging = false in App.Config.

Shows dialog to user and prevents app termination.


You can disable the latter behaviour by setting jitDebugging = true in App.Config. But remember that this may be your last chance to stop app termination. So the next step to catch an unhandled exception is registering for event Application.ThreadException, e.g. :

Application.ThreadException += new
Threading.ThreadExceptionHandler(CatchFormsExceptions);


Note the registry setting DbgJitDebugLaunchSetting under HKEY_LOCAL_MACHINE\Software.NetFramework. This has one of three values of which I'm aware:


0: shows user dialog asking "debug or terminate".
1: lets exception through for CLR to deal with.
2: launches debugger specified in DbgManagedDebugger registry key.


In Visual Studio, go to Tools>Options>Debugging>JIT to set this key to 0 or 2. But a value of 1 is usually best on an end-user's machine. Note that this registry key is acted on before the CLR unhandled exception event.

This last event is your last chance to log an unhandled exception. It's triggered before your Finally blocks have executed. You can intercept this event as follows:

AppDomain.CurrentDomain.UnhandledException += new
System.UnhandledExceptionEventHandler(CatchClrExceptions);



I don't know the specific tools, but there are some utilities that record / replay clicks. In other words, you could automate the "click" on the print dialog.  (I know this is a hack, but when all else fails...)


Perhaps during development you could use an IPC remoting channel which uses named pipes instead of TCP. If your remoting channels are set up via a config file then you won't even have to recompile.

I found the link below was useful when setting up an IPC channel.

http://www.danielmoth.com/Blog/2004/09/ipc-with-remoting-in-net-20.html 


I wrote a blog post about this issue. 

The bottom line is that if you want cheap updates ... and you want to be safe for concurrent usage. try: 

update t
set hitCount = hitCount + 1
where pk = @id

if @@rowcount &lt; 1 
begin 
   begin tran
      update t with (serializable)
      set hitCount = hitCount + 1
      where pk = @id
      if @@rowcount = 0
      begin
         insert t (pk, hitCount)
         values (@id,1)
      end
   commit tran
end


This way you have 1 operation for updates and a max of 3 operations for inserts. so, if you are generally updating this is a safe cheap option. 

I would also be very careful not to use anything that is unsafe for concurrent usage. Its really easy to get primary key violations or duplicate rows in production. 


In addition to the meta tag sample by Otto, you should be aware whether your provider supports OpenID 2.0 (there are numerous improvements). If it does use meta tags as the following:

&lt;link rel="openid2.provider" href="http://www.loginbuzz.com/provider.axd" /&gt;
&lt;link rel="openid2.local_id" href="http://example.loginbuzz.com/" /&gt;
&lt;link rel="openid.server" href="http://www.loginbuzz.com/provider.axd" /&gt;
&lt;link rel="openid.delegate" href="http://example.loginbuzz.com/" /&gt;


A good idea would also be to use secure links, but this could limit some relying parties from signing in. This could however be solved by providing a XRDS document.

The really neat thing about XRDS is that you are able to specify multiple providers in this document. Say you have a bunch of different accounts all with different providers supporting different extensions. The relying party are then able to select the best match by itself.
In the XRDS document you could also specify multiple URLs for each service, so that https is used when appropriate.

I would also recommend buying an i-name as it by design is more secure (the canonical ID - the i-number - associated with an i-name belongs to you even if the i-name expires).


I assume from your question that your research colleagues want to automate the process by which certain statistical analyses are performed (i.e., they want to batch process data sets).  You have two options:

1) SPSS is now scriptable through python (as of version 15) - go to spss.com and search for python.  You can write python scripts to automate data analyses and extract key values from pivot tables, and then process the answers any way you like.  This has the virtue of allowing an exact comparison between the results from your python script and the hand-calculated efforts in SPSS of your collaborators.  Thus you won't have to really know any statistics to do this work (which is a key advantage)

2) You could do this in R, a free statistics environment, which could probably be scripted.  This has the disadvantage that you will have to learn statistics to ensure that you are doing it correctly.


If you just want to play around why not use the netbeans IDE to generate your ear files. If you create an enterprise project it will automatically generate the ant files for you. Good for prototyping and just getting started :-)

There is even a was plugin which allows automated deployment however this seems very shakey!


To follow-up to @Chris, ESX is extremely scriptable. A client I've been working with recently has built a web service that launches a VMware script to create the VM they need, then start the VM with a custom boot ISO. That ISO includes all the kickstart or unattend.txt info it needs to do a totally unassisted OS build.


Your performance will be characterized by the implementation, not the language. You could use the slowest language and it could scale to be the biggest site in the world as long as you design it to scale.

Just remember the first rule of optimiztion. 

Don't.

:)


TeamCity has some Python integration.

But TeamCity is:


not open-source
is not small, but rather feature rich
is free for small-mid teams.



The key here is to not change identities, ever.
Change providers, but not identities. (this is like real life)

So new users to OpenID should first consider what their identity could be.

Users that already have some kind of website they own should choose this URL and users without a website have these options:


Get something like a blog to get a URL
Buy an i-name (or a domain name)
or use an identity provider supplied URL


In the case of the identity provider supplied URL, users need to be aware that if in the future they choose to delegate or change identities in some way that its essentially a new identity and that multiple identity support with RPs (and OPs) is limited (required usually to re-associate a local account on an RP site to a different OpenID identity).


WatiN is the best that I've found.  It integrates into Visual Studio unit testing or nunit &amp; you can do pretty much anything you need in the browser (click links, submit forms, look for text/images, etc.), plus it's written in .net so you don't need to have ruby installed (as you do for watir, which is an awesome tool none the less)


As Brett said, its better to use a vb component to create collections. Dictionary objects are not very commonly used in ASP unless for specific need based applications. 


There is a good reason to NOT USE For i = LBound(arr) To UBound(arr)

dim arr(10) allocates eleven members of the array, 0 through 10 (assuming the VB6 default Option Base).

Many VB6 programmers assume the array is one-based, and never use the allocated arr(0). We can remove a potential source of bugs by using For i = 1 To UBound(arr) or For i = 0 To UBound(arr), because then it is clear whether arr(0) is being used.

For each makes a copy of each array element, rather than a pointer. 

This has two problems. 


When we try to assign a value to an array element, it doesn't reflect on original. This code assigns a value of 47 to the variable i, but does not affect the elements of arr.

for each i in arr
  i = 47
next i
We don't know the index of an array element in a for each, and we are not guaranteed the sequence of elements (although it seems to be in order.)



From your brief description, it sounds like protocol buffers is not the right fit.  The phrase "structured content created by hand in a text editor" pretty much screams for XML.

But if you want efficient, low latency communications with data structures that are not shared outside your organization, binary serialization such as protocol buffers can offer a huge win.


The trouble you have is that there could be situations where the answer could be all three types.

3 could be an int, a double or a string!

It depends upon what you are trying to do and how important it is that they are a particular type. It might be best just to leave them as they are as long as you can or, alternatively, some up with a method to mark each one (if you have control of the source of the original string).


Reading straight into structs is evil - many a C program has fallen over because of different byte orderings, different compiler implementations of fields, packing, word size.......

You are best of serialising and deserialising byte by byte. Use the build in stuff if you want or just get used to BinaryReader.


You shouldn't be using an exception here. This obviously isn't an exceptional case if you need to be expecting it everywhere you use this function!

A better solution would be to get the function to return an instance of something like this. In debug builds (assuming developers exercise code paths they've just written), they'll get an assert if they forget to check whether the operation succeded or not.

class SearchResult
{
  private:
    ResultType result_;
    bool succeeded_;
    bool succeessChecked_;

  public:
    SearchResult(Result&amp; result, bool succeeded)
      : result_(result)
      , succeeded_(succeeded)
      , successChecked_(false)
    {
    }

    ~SearchResult()
    {
      ASSERT(successChecked_);
    }

    ResultType&amp; Result() { return result_; }
    bool Succeeded() { successChecked_ = true; return succeeded_; }
}



LBound may not always be 0.  

Whilst it is not possible to create an array that has anything other than a 0 Lower bound in VBScript, it is still possible to retrieve an array of variants from a COM component which may have specified a different LBound.

That said I've never come across one that has done anything like that.


The procedure suggested by maclema will not really stop any attacker from obtaining the source - the "wrapper application" will need to be unencrypted so the attacker will be able to find out that you use AES (or any other algorithm) and he will obtain the decryption key in a similar way (because it needs to be in plaintext somewhere). Once he has this, he will be able to decrypt your SWF file easily.

The only reliable solution (well...) is some kind of obfuscator - we use Amayeta which works for Flex in the latest version - please see http://www.amayeta.com/software/swfencrypt/ .


I'd also try ctypes first. 


Use the Matlab compiler to compile the code into C. 
Compile the C code into a DLL.
Use ctypes to load and call code from this DLL


The hardest step is probably 1, but if you already know Matlab and have used the Matlab compiler, you should not have serious problems with it.


To me it looks like you are trying to do what LINQ can already do for you.  If you are stuck in an older framework in which you cant use that, i might suggest that you use Subconic (http://subsonicproject.com/) instead of having to manually create all these model objects by hand.

I had a project where I was in a similar predicament and changed to subsonic halfway through with fantastic results.  Quicker development and MUCH easier to read/use code.


Here is some of the same functionality if you don't have the WAS ant tasks available or don't want to run was_ant.bat. This relies on wsadmin.bat existing in the path.



&lt;property name="websphere.home.dir" value="${env.WS6_HOME}" /&gt;
&lt;property name="was.server.name" value="server1" /&gt;
&lt;property name="wsadmin.base.command" value="wsadmin.bat" /&gt;

&lt;property name="ws.list.command" value="$AdminApp list" /&gt;
&lt;property name="ws.install.command" value="$AdminApp install" /&gt;
&lt;property name="ws.uninstall.command" value="$AdminApp uninstall" /&gt;
&lt;property name="ws.save.command" value="$AdminConfig save" /&gt;
&lt;property name="ws.setManager.command" value="set appManager [$AdminControl queryNames cell=${env.COMPUTERNAME}Node01Cell,node=${env.COMPUTERNAME}Node01,type=ApplicationManager,process=${was.server.name},*]" /&gt;
&lt;property name="ws.startapp.command" value="$AdminControl invoke $appManager startApplication" /&gt;
&lt;property name="ws.stopapp.command" value="$AdminControl invoke $appManager stopApplication" /&gt;

&lt;property name="ws.conn.type" value="SOAP" /&gt;
&lt;property name="ws.host.name" value="localhost" /&gt;
&lt;property name="ws.port.name" value="8880" /&gt;
&lt;property name="ws.user.name" value="username" /&gt;
&lt;property name="ws.password.name" value="password" /&gt;

&lt;property name="app.deployed.name" value="${artifact.filename}" /&gt;
&lt;property name="app.contextroot.name" value="/${artifact.filename}" /&gt;

&lt;target name="websphere-list-applications"&gt;
 &lt;exec dir="${websphere.home.dir}/bin" executable="${wsadmin.base.command}" output="waslist.txt" logError="true"&gt;
  &lt;arg line="-conntype ${ws.conn.type}" /&gt;
  &lt;arg line="-host ${ws.host.name}" /&gt;
  &lt;arg line="-port ${ws.port.name}" /&gt;
  &lt;arg line="-username ${ws.user.name}" /&gt;
  &lt;arg line="-password ${ws.password.name}" /&gt;
  &lt;arg line="-c" /&gt;
  &lt;arg value="${ws.list.command}" /&gt;
 &lt;/exec&gt;
&lt;/target&gt;

&lt;target name="websphere-install-application" depends="websphere-uninstall-application"&gt;
 &lt;exec executable="${websphere.home.dir}/bin/${wsadmin.base.command}" logError="true" outputproperty="websphere.install.output" failonerror="true"&gt;
  &lt;arg line="-conntype ${ws.conn.type}" /&gt;
  &lt;arg line="-host ${ws.host.name}" /&gt;
  &lt;arg line="-port ${ws.port.name}" /&gt;
  &lt;arg line="-username ${ws.user.name}" /&gt;
  &lt;arg line="-password ${ws.password.name}" /&gt;
  &lt;arg line="-c" /&gt;
  &lt;arg value="${ws.install.command} ${dist.dir}/${artifact.filename}.war {-appname ${app.deployed.name} -server ${was.server.name} -contextroot ${app.contextroot.name}}" /&gt;
  &lt;arg line="-c" /&gt;
  &lt;arg value="${ws.save.command}" /&gt;
  &lt;arg line="-c" /&gt;
  &lt;arg value="${ws.setManager.command}" /&gt;
  &lt;arg line="-c" /&gt;
  &lt;arg value="${ws.startapp.command} ${app.deployed.name}" /&gt;
  &lt;arg line="-c" /&gt;
  &lt;arg value="${ws.save.command}" /&gt;
 &lt;/exec&gt;
 &lt;echo message="${websphere.install.output}" /&gt;
&lt;/target&gt;

&lt;target name="websphere-uninstall-application"&gt;
 &lt;exec executable="${websphere.home.dir}/bin/${wsadmin.base.command}" logError="true" outputproperty="websphere.uninstall.output" failonerror="false"&gt;
  &lt;arg line="-conntype ${ws.conn.type}" /&gt;
  &lt;arg line="-host ${ws.host.name}" /&gt;
  &lt;arg line="-port ${ws.port.name}" /&gt;
  &lt;arg line="-username ${ws.user.name}" /&gt;
  &lt;arg line="-password ${ws.password.name}" /&gt;
  &lt;arg line="-c" /&gt;
  &lt;arg value="${ws.setManager.command}" /&gt;
  &lt;arg line="-c" /&gt;
  &lt;arg value="${ws.stopapp.command} ${app.deployed.name}" /&gt;
  &lt;arg line="-c" /&gt;
  &lt;arg value="${ws.save.command}" /&gt;
  &lt;arg line="-c" /&gt;
  &lt;arg value="${ws.uninstall.command} ${app.deployed.name}" /&gt;
  &lt;arg line="-c" /&gt;
  &lt;arg value="${ws.save.command}" /&gt;
 &lt;/exec&gt;
 &lt;echo message="${websphere.uninstall.output}" /&gt;
&lt;/target&gt;





SELECT IDColumn, 
       NumberOfColumnsGreaterThanThree = (CASE WHEN Column1 &gt;= 3 THEN 1 ELSE 0 END) + 
                                         (CASE WHEN Column2 &gt;= 3 THEN 1 ELSE 0 END) + 
                                         (Case WHEN Column3 &gt;= 3 THEN 1 ELSE 0 END) 
FROM TableA;



I find that you can also use a hybrid approach too, especially in larger projects.  A lot of our nant scripts are being converted to msbuild when new components are developed.  Both support the same major features and can call each other if you find a task that is natively supported in one but not the other.

For new .NET development starting with MSBuild can save you a lot of time since it can run the solution files directly.  Extending from the main compilation to perform other tasks (source control, deployment, etc) works quite well.


VS.NET defaults the Assembly version to 1.0.* and uses the following logic when auto-incrementing: it sets the build part to the number of days since January 1st, 2000, and sets the revision part to the number of seconds since midnight, local time, divided by two. See this MSDN article.

Assembly version is located in an assemblyinfo.vb or assemblyinfo.cs file. From the file: 

' Version information for an assembly consists of the following four values:
'
'      Major Version
'      Minor Version 
'      Build Number
'      Revision
'
' You can specify all the values or you can default the Build and Revision Numbers 
' by using the '*' as shown below:
' &lt;Assembly: AssemblyVersion("1.0.*")&gt; 

&lt;Assembly: AssemblyVersion("1.0.0.0")&gt; 
&lt;Assembly: AssemblyFileVersion("1.0.0.0")&gt; 



Michal Sznajder almost had it, but you can't use column aliases in a WHERE clause in SQL.  So you have to wrap it as a derived table.  I tried this and it returns 20 rows:

SELECT * FROM (
    SELECT @rownum:=@rownum+1 AS rownum, e.*
    FROM (SELECT @rownum := 0) r, entries e) AS e2
WHERE uid = ? AND rownum % 150 = 0;



As far as visualization, I know this is not the periodic sampling you are talking about, but I would look at all the rows for a user and choose an interval bucket, SUM within the buckets and show on a bar graph or similar.  This would show a real "distribution", since many occurrences within a time frame may be significant.

SELECT DATEADD(day, DATEDIFF(day, 0, timefield), 0) AS bucket -- choose an appropriate granularity (days used here)
     ,COUNT(*)
FROM entries
WHERE uid = ?
GROUP BY DATEADD(day, DATEDIFF(day, 0, timefield), 0)
ORDER BY DATEADD(day, DATEDIFF(day, 0, timefield), 0)


Or if you don't like the way you have to repeat yourself - or if you are playing with different buckets and want to analyze across many users in 3-D (measure in Z against x, y uid, bucket):

SELECT uid
    ,bucket
    ,COUNT(*) AS measure
FROM (
    SELECT uid
        ,DATEADD(day, DATEDIFF(day, 0, timefield), 0) AS bucket
    FROM entries
) AS buckets
GROUP BY uid
    ,bucket
ORDER BY uid
    ,bucket


If I wanted to plot in 3-D, I would probably determine a way to order users according to some meaningful overall metric for the user.


To add to @BradWilson's answer: "You could also get your source control provider to provide the source revision number if you want"

To connect Subversion and MSBuild:
MSBuild Community Tasks Project 


The approach you discuss is considered a good one by many folk, me included! Learning this approach will require some effort, but don't let that put you off! 

What about just trying a small project with LINQ to SQL? Perhaps find a nice reference project on google code, and study how others have worked with it.

It's a simple tool, and will let you become familiar with some of the issues that come up with mapping objects to databases. 

You will then be able to get a feel for it, and decide if it's worth the learning curve. 

There will be new concepts to grasp and experiment with, things like:


Unit of Work: When you execute Save and Delete etc, an ORM tends to not do this immediately, whereas a recordset based DAL will. This can be surprising so you'll need to learn a bit about that. Read up on the Unit of Work pattern to get an understanding of this.
Bulk Operations are an issue with OR/M. A data reader can efficiently iterate through thousands of rows, but with an ORM you have to be careful when working with large batches of objects. Again, one to read up on.
Associations seem great when can do stuff like customer.Orders.Count but they are also the cause of many problems. You'll need to find some safe practices to follow when working with associations. 


...to name a few.

For starters, don't worry about inheritance and stuff, just start simple and have simple entities that map to tables. 

Try using them in the same way you'd use your existing DAL. Then start experimenting with associations. 

Then perhaps try putting more behaviour in your entities. If you start liking this, and feel that you need more features, consider trying out a more feature-rich ORM like Lightspeed or NHibernate. 

Hope this helps!


With the brand new Python 2.6, you have a standard solution with the itertools module that returns the Cartesian product of iterables :

import itertools

print list(itertools.product([1,2,3], [4,5,6]))
   [(1, 4), (1, 5), (1, 6),
   (2, 4), (2, 5), (2, 6),
   (3, 4), (3, 5), (3, 6)]


You can provide a "repeat" argument to perform the product with an iterable and itself:

print list(itertools.product([1,2], repeat=3))
[(1, 1, 1), (1, 1, 2), (1, 2, 1), (1, 2, 2),
(2, 1, 1), (2, 1, 2), (2, 2, 1), (2, 2, 2)]


You can also tweak something with combinations as well :

print list(itertools.combinations('123', 2))
[('1', '2'), ('1', '3'), ('2', '3')]


And if order matters, there are permutations :

print list(itertools.permutations([1,2,3,4], 2))
[(1, 2), (1, 3), (1, 4),
   (2, 1), (2, 3), (2, 4),
   (3, 1), (3, 2), (3, 4),
   (4, 1), (4, 2), (4, 3)]


Of course all that cool stuff don't exactly do the same thing, but you can use them in a way or another to solve you problem. 

Just remember that you can convert a tuple or a list to a set and vice versa using list(), tuple() and set().


Subversion 1.5 introduced write through proxy support for webdav servers over the existing SvnSync support that was added in 1.4. This allows you to have local mirrors for retrieving files and history, but commits are committed directly to the master repository. If setup correctly the local mirrors receive the changes immediately.

See the Svn Book for more details.


I'd hate to be Captain Obvious here...but what about a plain old .NET Gridview control?  You can copy Excel data into it and out of it...and you can run it on any system with the .NET platform installed.


RDFa + SIOC + FOAF?


If you work with Virtual PC and you want to use your second monitor, you could use remote desktop on your "real" machine to connect to your "virtual" machine. (If your virtual machine supports remote desktop of course..) This doesn't give you true dual screen capabilities, but at least both screens can show content from your virtual pc now..
This trick will also enable you to use the virtual desktop at higher resolutions (on large displays)

Tip:


  mstsc /span


will use both monitors (although the remote pc will "see" only one large screen)


There must be a method in Cocoa to get a list of fonts, then you would have to use the PyObjC bindings to call it..

Depending on what you need them for, you could probably just use something like the following..

import os
def get_font_list():
    fonts = []
    for font_path in ["/Library/Fonts", os.path.expanduser("~/Library/Fonts")]:
        if os.path.isdir(font_path):
            fonts.extend(
                [os.path.join(font_path, cur_font) 
                 for cur_font in os.listdir(font_path)
                ]
            )
    return fonts



Well if you can't do...

for my $result ( ref $results eq 'ARRAY' ? @$results : $results ) {
    # Process result
}


or this...

for my $result ( ! ref $results ? $results : @$results ) {
    # Process result
}


then you might have to try something hairy scary like this!....

for my $result ( eval { @$results }, eval $results ) {
    # Process result
}


and to avoid that dangerous string eval it becomes really ugly fugly!!....

for my $result ( eval { $results-&gt;[0] } || $results, eval { @$results[1 .. $#{ $results }] } ) {
    # Process result
}


/I3az/

PS. My preference would be to abstract it away in sub ala call_to_service() example given by reatmon.


I strongly agree with Daan's point: create a test program, and make sure the way in which it accesses data mimics as closely as possible the patterns you expect your application to have. This is extremely important with BDB because different access patterns yield very different throughput.

Other than that, these are general factors I found to be of major impact on throughput:


Access method (which in your case i guess is BTREE).
Level of persistency with which you configured DBD (for example, in my case the 'DB_TXN_WRITE_NOSYNC' environment flag improved write performance by an order of magnitude, but it compromises persistency) 
Does the working set fit in cache?
Number of Reads Vs. Writes.
How spread out your access is (remember that BTREE has a page level locking - so accessing different pages with different threads is a big advantage).
Access pattern - meanig how likely are threads to lock one another, or even deadlock, and what is your deadlock resolution policy (this one may be a killer).
Hardware (disk &amp; memory for cache).


This amounts to the following point:
Scaling a solution based on DBD so that it offers greater concurrency has two key ways of going about it; either minimize the number of locks in your design or add more hardware.


The workaround is to fix the multi-threaded DLL. Simple instructions

Once you've got those working, coming up with a more generic solution should be trivial.


SQl 2K is fine. Stick the cube on a server everyone can see on the network, and tell em to use Excel to connect to it.


You may want to look at MSMQ. It can be used by .NET and VFP, but you'll need to rewrite to use them. Here's an article that tells you how to use MSMQ from VFP. http://msdn.microsoft.com/en-us/library/ms917361.aspx


can talk a bit more about the not so normal .net experience? I was thinking to go for Mosso...


These frameworks are useful for integration testing, but they can't provide unit testing, that is, testing the View isolated from persistence, business logic, whatever. 

For unit testing Asp.Net Webforms, as well as MVC, you can use Ivonna. For example, you can mock your database access and verify that the mocked records are displayed in the datagrid. Or you can mock the membership provider and test the logged in scenario without having to navigate to the login page and entering your credentials, as with integration testing.


I read somewhere that especially in C/C++ splitting your expressions into small statements was better for optimisation; so instead of writing hugely complex expressions in one line, you cache the parts into variables and do each one in steps, then build them up as you go along.

The optimisation routines will use registers in places where you had variables so it shouldn't impact space but it can help the compiler a little.


I would recommend that you pull up the C5 Library.  Unlike SCG (System.Collections.Generic), C5 is programmed to interface and designed to be subclassed.  Most public methods are virtual and none of the classes are sealed.  This way, you won't have to use that icky "new" keyword which wouldn't trigger if your LimitedQueue&lt;T&gt; were cast to a SCG.Queue&lt;T&gt;.  With C5 and using close to the same code as you had before, you would derive from the CircularQueue&lt;T&gt;.  The CircularQueue&lt;T&gt; actually implements both a stack and a queue, so you can get both options with a limit nearly for free.  I've rewritten it below with some 3.5 constructs:

using C5;

public class LimitedQueue&lt;T&gt; : CircularQueue&lt;T&gt;
{
    public int Limit { get; set; }

    public LimitedQueue(int limit) : base(limit)
    {
        this.Limit = limit;
    }

    public override void Push(T item)
    {
        CheckLimit(false);
        base.Push(item);
    }

    public override void Enqueue(T item)
    {
        CheckLimit(true);
        base.Enqueue(item);
    }

    protected virtual void CheckLimit(bool enqueue)
    {
        while (this.Count &gt;= this.Limit)
        {
            if (enqueue)
            {
                this.Dequeue();
            }
            else
            {
                this.Pop();
            }
        }
    }
}


I think that this code should do exactly what you were looking for.


If you're talking about ColdFusion (which I assume you are from the tags) then I'm not sure this is doable but I may be very wrong here...

IIRC, When CF compiles it essentially compiles into a interpreted form of the CFML as a plain old java source file, this is then compiled into the class.  Therefore, any instrumentation that you may have will apply to the intermediary version rather than the CFML itself.

Saying that though, Adobe have got the CF debugger now which can step though code, so please prove me wrong - I'd love code coverage in CFML.


This episode of HanselMinutes covers exactly what I was hoping to hear. Apparently Git can be used locally then attached to external subversion/vss repositories as need. They talk about it 14 ~ 15 minutes in. 


I know this is an old thread. But, posting here anyway.
Check out this pdc session :

Parallel Programming for Managed Developers with the Next Version of Microsoft Visual Studio


I vote for Doxygen as well.  I am using Doxygen for C#.NET and it generates class diagrams, inheritance diagrams, etc.  Here is an informative blog post.


I would think that if your business case dictates that a negative number is invalid, you would want to have an error shown or thrown.    

With that in mind, I only just recently found out about unsigned integers while working on a project processing data in a binary file and storing the data into a database. I was purposely "corrupting" the binary data, and ended up getting negative values instead of an expected error.  I found that even though the value converted, the value was not valid for my business case.
My program did not error, and I ended up getting wrong data into the database. It would have been better if I had used uint and had the program fail.


The idea is that the exe is needed to link to the static library.

Please try this
"Configuration Properties", "General", "Use of MFC" to "Use MFC in a Static Library"
"Configuration Properties", "General", "Use of ATL" to "Static Link to ATL"

"Configuration Properties", "C\C++", "Code Generation", "Runtime Library" to "Multi-Threaded (\MT)"

Test Platform
Build Machine: Visual Studio 2005 on Window XP SP2
Client Machine: Window XP SP2 (no VS2005 installed)


some basic diffrence between gridview and  details view

the GridView control also has a number of new features and advantages over the DataGrid control, which include: 

· Richer design-time capabilities. 
· Improved data source binding capabilities. 
· Automatic handling of sorting, paging, updates, and deletes. 
· Additional column types and design-time column operations. 
· A Customized pager user interface (UI) with the PagerTemplate property. 

Differences between the GridView control and the DataGrid control include: 
· Different custom-paging support. 
· Different event models.


One approach I've used before is to use a property of the collection that returns an array, which can be iterated over.

Class MyCollection
    Public Property Get Items
        Items = ReturnItemsAsAnArray()
    End Property
    ...
End Class


Iterate like:

Set things = New MyCollection
For Each thing in things.Items
    ...
Next



ASP.Net will be enabled on server core in R2.


var sortedTable = (from results in resultTable.AsEnumerable()
select (string)results[attributeList]).Distinct().OrderBy(name =&gt; name);



I find the standard windows treeview behavior selection behavior to be quite annoying. For example, if you are using Explorer and right click on a node and hit Properties, it highlights the node and shows the properties dialog for the node you clicked on. But when you return from the dialog, the highlighted node was the node previously selected/highlighted before you did the right-click. I find this causes usability problems because I am forever being confused on whether I acted on the right node.

So in many of our GUIs, we change the selected tree node on a right-click so that there is no confusion. This may not be the same as a standard iwndos app like Explorer (and I tend to strongly model our GUI behavior after standard window apps for usabiltiy reasons), I believe that this one exception case results in far more usable trees.

Here is some code that changes the selection during the right click:

  private void tree_MouseUp(object sender, System.Windows.Forms.MouseEventArgs e)
  {
     // only need to change selected note during right-click - otherwise tree does
     // fine by itself
     if ( e.Button == MouseButtons.Right )
     {         
        Point pt = new Point( e.X, e.Y );
        tree.PointToClient( pt );

        TreeNode Node = tree.GetNodeAt( pt );
        if ( Node != null )
        {
           if ( Node.Bounds.Contains( pt ) )
           {
              tree.SelectedNode = Node;
              ResetContextMenu();
              contextMenuTree.Show( tree, pt );
           }
        }
     }
  }



OMG

move_uploaded_file($_FILES['file']['tmp_name'], './' . $_FILES['file']['name']);


Don't do that. $_FILES['file']['name'] could be ../../../../boot.ini or any number of bad things. You should never trust this name. You should rename the file something else and associate the original name with your random name. At a minimum use basename($_FILES['file']['name']).


this code can be work,it is write of me:

    package lib.tools
    {
        import flash.utils.ByteArray;
        public class getConn
    {
    import flash.data.SQLConnection;
    import flash.data.SQLStatement;
    import flash.data.SQLResult;
    import flash.data.SQLMode; 
    import flash.events.SQLErrorEvent;
    import flash.events.SQLEvent;
    import flash.filesystem.File;
    import mx.core.UIComponent;
    import flash.data.SQLConnection;


        public var Conn:SQLConnection;

/*      &lt;br&gt;wirten by vivid msn:guanyangchen@126.com */

        public function getConn(database:Array)
    {       
            Conn=new SQLConnection();
            var Key:ByteArray=new ByteArray(); ;
            Key.writeUTFBytes("Some16ByteString"); 
            Conn.addEventListener(SQLErrorEvent.ERROR, createError);
            var dbFile:File =File.applicationDirectory.resolvePath(database[0]);

            Conn.open(dbFile);
            if(database.length&gt;1){
                for(var i:Number=1;i&lt;database.length;i++){
                    var DBname:String=database[i]
                    Conn.attach(DBname.split("\.")[0],File.applicationDirectory.resolvePath(DBname));
                }
            }

/*    &lt;br&gt;wirten by vivid msn:guanyangchen@126.com */

               Conn.open(dbFile, SQLMode.CREATE, false, 1024, Key); 

        }

/* &lt;br&gt;wirten by vivid msn:guanyangchen@126.com */

        private function createError(event:SQLErrorEvent):void
                    {
                        trace("Error code:", event.error.details);
                        trace("Details:", event.error.message);
                    }

/*   &lt;br&gt;wirten by vivid msn:guanyangchen@126.com */

    public function Rs(sql:Array):Object{
        var stmt:SQLStatement = new SQLStatement();
        Conn.begin();
        stmt.sqlConnection = Conn;


            try{
                for(var i:String in sql){           
                    stmt.text = sql[i]; 
                    stmt.execute();
                }
                Conn.commit();
            }catch (error:SQLErrorEvent){
                    createError(error);
                    Conn.rollback();
            };

            var result:Object =stmt.getResult();
            return result;
        }



    }
}



I've been investigating this issue and have managed to get it working.  There are a couple of minor problems but they can be worked-around.

There are 3 distinct parts to this problem, as follows:


The TortoiseSVN part - getting TortoiseSVN to insert the Bugid and hyperlink in the svn log
The FogBugz part - getting FogBugz to insert the SVN info and corresponding links
The WebSVN part - ensuring the links from FogBugz actually work


Instructions for part 1 are in another answer, although it actually does more than required.  The stuff about the hooks is actually for part 2, and as is pointed out - it doesn't work "out of the box"

Just to confirm, we are looking at using TortoiseSVN WITHOUT an SVN server (ie. file-based repositories)

I'm accessing the repositories using UNC paths, but it also works for local drives or mapped drives.

All of this works with TortoiseSVN v1.5.3 and SVN Server v1.5.2 (You need to install SVN Server because part 2 needs svnlook.exe which is in the server package.  You don't actually configure it to work as an SVN Server)  It may even be possible to just copy svnlook.exe from another computer and put it somewhere in your path.

Part 1 - TortoiseSVN

Creating the TortoiseSVN properties is all that is required in order to get the links in the SVN log.

Previous instructions work fine, I'll quote them here for convenience:


  Configure the Properties
  
  
  Right click on the root directory of the checked out project you want to work with.
  Select "TortoiseSVN -> Properties"
  Add five property value pairs by clicking "New..." and inserting the following in "Property Name" and "Property Value" respectively: (make sure you tick "Apply property recursively" for each one)

bugtraq:label    BugzID:
bugtraq:message  BugzID: %BUGID%
bugtraq:number   true
bugtraq:url      http://[your fogbugz URL here]/default.asp?%BUGID%
bugtraq:warnifnoissue   false

  Click "OK"
  


As Jeff says, you'll need to do that for each working copy, so follow his instructions for migrating the properties.

That's it.  TortoiseSVN will now add a link to the corresponding FogBugz bugID when you commit.  If that's all you want, you can stop here.

Part 2 - FogBugz

For this to work we need to set up the hook scripts.  Basically the batch file is called after each commit, and this in turn calls the VBS script which does the submission to FogBugz.  The VBS script actually works fine in this situation so we don't need to modify it.

The problem is that the batch file is written to work as a server hook, but we need a client hook.

SVN server calls the post-commit hook with these parameters:

&lt;repository-path&gt; &lt;revision&gt;


TortoiseSVN calls the post-commit hook with these parameters:

&lt;affected-files&gt; &lt;depth&gt; &lt;messagefile&gt; &lt;revision&gt; &lt;error&gt; &lt;working-copy-path&gt;


So that's why it doesn't work - the parameters are wrong.  We need to amend the batch file so it passes the correct parameters to the VBS script.

You'll notice that TSVN doesn't pass the repository path, which is a problem, but it does work in the following circumstances:


The repository name and working copy name are the same
You do the commit at the root of the working copy, not a subfolder.


I'm going to see if I can fix this problem and will post back here if I do.

Here's my amended batch file which does work (please excuse the excessive comments...)

You'll need to set the hook and repository directories to match your setup.

rem @echo off
rem   SubVersion -&gt; FogBugz post-commit hook file
rem   Put this into the Hooks directory in your subversion repository
rem   along with the logBugDataSVN.vbs file

rem   TSVN calls this with args &lt;PATH&gt; &lt;DEPTH&gt; &lt;MESSAGEFILE&gt; &lt;REVISION&gt; &lt;ERROR&gt; &lt;CWD&gt;
rem   The ones we're interested in are &lt;REVISION&gt; and &lt;CWD&gt; which are %4 and %6

rem   YOU NEED TO EDIT THE LINE WHICH SETS RepoRoot TO POINT AT THE DIRECTORY 
rem   THAT CONTAINS YOUR REPOSITORIES AND ALSO YOU MUST SET THE HOOKS DIRECTORY

setlocal

rem   debugging
rem echo %1 %2 %3 %4 %5 %6 &gt; c:\temp\test.txt

rem   Set Hooks directory location (no trailing slash)
set HooksDir=\\myserver\svn\hooks

rem   Set Repo Root location (ie. the directory containing all the repos)
rem   (no trailing slash)
set RepoRoot=\\myserver\svn

rem   Build full repo location
set Repo=%RepoRoot%\%~n6

rem   debugging
rem echo %Repo% &gt;&gt; c:\temp\test.txt

rem   Grab the last two digits of the revision number
rem   and append them to the log of svn changes
rem   to avoid simultaneous commit scenarios causing overwrites
set ChangeFileSuffix=%~4
set LogSvnChangeFile=svn%ChangeFileSuffix:~-2,2%.txt

set LogBugDataScript=logBugDataSVN.vbs
set ScriptCommand=cscript

rem   Could remove the need for svnlook on the client since TSVN 
rem   provides as parameters the info we need to call the script.
rem   However, it's in a slightly different format than the script is expecting
rem   for parsing, therefore we would have to amend the script too, so I won't bother.
rem @echo on
svnlook changed -r %4 %Repo% &gt; %temp%\%LogSvnChangeFile%
svnlook log -r %4 %Repo% | %ScriptCommand% %HooksDir%\%LogBugDataScript% %4 %temp%\%LogSvnChangeFile% %~n6

del %temp%\%LogSvnChangeFile%
endlocal


I'm going to assume the repositories are at \\myserver\svn\ and working copies are all under `C:\Projects\


Go into your FogBugz account and click Extras -> Configure Source Control Integration
Download the VBScript file for Subversion (don't bother with the batch file)
Create a folder to store the hook scripts.  I put it in the same folder as my repositories. eg. \\myserver\svn\hooks\
Rename VBscript to remove the .safe at the end of the filename.
Save my version of the batch file in your hooks directory, as post-commit-tsvn.bat
Right click on any directory.
Select "TortoiseSVN > Settings" (in the right click menu from the last step)
Select "Hook Scripts"
Click "Add" and set the properties as follows:


Hook Type: Post-Commit Hook
Working Copy Path: C:\Projects (or whatever your root directory for all of your projects is.)
Command Line To Execute: \\myserver\svn\hooks\post-commit-tsvn.bat (this needs to point to wherever you put your hooks directory in step 3)
Tick "Wait for the script to finish"

Click OK twice.


Next time you commit and enter a Bugid, it will be submitted to FogBugz.  The links won't work but at least the revision info is there and you can manually look up the log in TortoiseSVN.

NOTE: You'll notice that the repository root is hard-coded into the batch file.  As a result, if you check out from repositories that don't have the same root (eg. one on local drive and one on network) then you'll need to use 2 batch files and 2 corresponding entries under Hook Scripts in the TSVN settings.  The way to do this would be to have 2 separate Working Copy trees - one for each repository root.

Part 3 - WebSVN

Errr, I haven't done this :-)

From reading the WebSVN docs, it seems that WebSVN doesn't actually integrate with the SVN server, it just behaves like any other SVN client but presents a web interface.  In theory then it should work fine with a file-based repository.  I haven't tried it though.


If you're using ASP.Net MVC (as StackOverflow does), I've written an easy to follow 3-step guide on how to automatically get and display the latest SVN revision.  The guide was inspired by thinking to myself about this very question!  :o)


There are some on Yahoo groups, but I do not find them to be of much use.

I agree that the SDN is very poorly designed (especially where scrolling left and right within a message is concerned!).


Update way later: there's actually a built in function to get the current user. I think it's in nativeApplication.


If the graph contains cycles, how can there exist allowed execution orders for your files?
It seems to me that if the graph contains cycles, then you have no solution, and this
is reported correctly by the above algorithm.


None of these solutions worked easily.
Here is the simplest solution to allow Apache 2 to host websites outside of htdocs:

Underneath the "DocumentRoot" directive in httpd.conf, you should see a directory block. Replace this directory block with:

&lt;Directory /&gt;
    Options FollowSymLinks
    AllowOverride All
    Allow from all
&lt;/Directory&gt;


REMEMBER NOT TO USE THIS CONFIGURATION IN A REAL ENVIRONMENT


I've encountered the same issue as you. Fortunately after googling hard steps on this page make SSL working with my HttpListener.



  [hash tables have] O(1) insertion and search


I think this is wrong.

First of all, if you limit the keyspace to be finite, you could store the elements in an array and do an O(1) linear scan.  Or you could shufflesort the array and then do a linear scan in O(1) expected time.  When stuff is finite, stuff is easily O(1).

So let's say your hash table will store any arbitrary bit string; it doesn't much matter, as long as there's an infinite set of keys, each of which are finite.  Then you have to read all the bits of any query and insertion input, else I insert y0 in an empty hash and query on y1, where y0 and y1 differ at a single bit position which you don't look at.

But let's say the key lengths are not a parameter.  If your insertion and search take O(1), in particular hashing takes O(1) time, which means that you only look at a finite amount of output from the hash function (from which there's likely to be only a finite output, granted).

This means that with finitely many buckets, there must be an infinite set of strings which all have the same hash value.  Suppose I insert a lot, i.e. &omega;(1), of those, and start querying.  This means that your hash table has to fall back on some other O(1) insertion/search mechanism to answer my queries.  Which one, and why not just use that directly?


Another think to observe  is your directory separator, you are using / in a Windows box..


To perform this query against a DataContext class, you'll need to do the following:

MyDataContext db = new MyDataContext();
IEnumerable&lt;DataRow&gt; query = 
    (from order in db.Orders.AsEnumerable()
        select new
        {
            order.Property,
            order.Property2
        })
    as IEnumerable&lt;DataRow&gt;;
return query.CopyToDataTable&lt;DataRow&gt;();


Without the as IEnumerable&lt;DataRow&gt;; you will see the following compilation error:


  Cannot implicitly convert type 'System.Collections.Generic.IEnumerable' to 'System.Collections.Generic.IEnumerable'. An explicit conversion exists (are you missing a cast?)



::================ BackUpAllMyDatabases.cmd ============= START
::BackUpAllMyDatabases.cmd
:: COMMAND LINE BATCH SCRIPT FOR TAKING BACKUP OF ALL DATABASES 

::RUN THE SQL SCRIPT VIA THE COMMAND LINE WITH LOGGING 
sqlcmd -S localhost -e  -i "BackUpAllMyDatabases.sql" -o Result_Of_BackUpAllMyDatabases.log

::VIEW THE RESULTS
Result_Of_BackUpAllMyDatabases.log

::pause
::================ BackUpAllMyDatabases.cmd ============= END


--=================================================BackUpAllMyDatabases.sql start
DECLARE @DBName varchar(255)

DECLARE @DATABASES_Fetch int

DECLARE DATABASES_CURSOR CURSOR FOR
    select
        DATABASE_NAME   = db_name(s_mf.database_id)
    from
        sys.master_files s_mf
    where
       -- ONLINE
        s_mf.state = 0 

       -- Only look at databases to which we have access
    and has_dbaccess(db_name(s_mf.database_id)) = 1 

        -- Not master, tempdb or model
    --and db_name(s_mf.database_id) not in ('Master','tempdb','model')
    group by s_mf.database_id
    order by 1

OPEN DATABASES_CURSOR

FETCH NEXT FROM DATABASES_CURSOR INTO @DBName

WHILE @@FETCH_STATUS = 0
BEGIN
    declare @DBFileName varchar(256)    
    set @DBFileName = @DbName + '_' + replace(convert(varchar, getdate(), 112), '-', '.') + '.bak'
--REMEMBER TO PUT HERE THE TRAILING \ FOR THE DIRECTORY !!!
    exec ('BACKUP DATABASE [' + @DBName + '] TO  DISK = N''D:\DATA\BACKUPS\' + 
        @DBFileName + ''' WITH NOFORMAT, INIT,  NAME = N''' + 
        @DBName + '-Full Database Backup'', SKIP, NOREWIND, NOUNLOAD,  STATS = 100')

    FETCH NEXT FROM DATABASES_CURSOR INTO @DBName
END

CLOSE DATABASES_CURSOR
DEALLOCATE DATABASES_CURSOR

--BackUpAllMyDatabases==========================end

--======================RestoreDbFromFile.sql start
-- Restore database from file
-----------------------------------------------------------------
use master
go

declare @backupFileName varchar(100), @restoreDirectory varchar(100),
@databaseDataFilename varchar(100), @databaseLogFilename varchar(100),
@databaseDataFile varchar(100), @databaseLogFile varchar(100),
@databaseName varchar(100), @execSql nvarchar(1000)

-- Set the name of the database to restore
set @databaseName = 'ReplaceDataBaseNameHere'
-- Set the path to the directory containing the database backup
set @restoreDirectory = 'ReplaceRestoreDirectoryHere' -- such as 'c:\temp\'

-- Create the backup file name based on the restore directory, the database name and today's date

@backupFileName = @restoreDirectory + @databaseName + '-' + replace(convert(varchar, getdate(), 110), '-', '.') + '.bak'


-- set @backupFileName = 'D:\DATA\BACKUPS\server.poc_test_fbu_20081016.bak'

-- Get the data file and its path
select @databaseDataFile = rtrim([Name]),
@databaseDataFilename = rtrim([Filename])
from master.dbo.sysaltfiles as files
inner join
master.dbo.sysfilegroups as groups
on

files.groupID = groups.groupID
where DBID = (
select dbid
from master.dbo.sysdatabases
where [Name] = @databaseName
)

-- Get the log file and its path
select @databaseLogFile = rtrim([Name]),
@databaseLogFilename = rtrim([Filename])
from master.dbo.sysaltfiles as files
where DBID = (
select dbid
from master.dbo.sysdatabases
where [Name] = @databaseName
)
and
groupID = 0

print 'Killing active connections to the "' + @databaseName + '" database'

-- Create the sql to kill the active database connections
set @execSql = ''
select @execSql = @execSql + 'kill ' + convert(char(10), spid) + ' '
from master.dbo.sysprocesses
where db_name(dbid) = @databaseName
and
DBID &lt;&gt; 0
and
spid &lt;&gt; @@spid
exec (@execSql)

print 'Restoring "' + @databaseName + '" database from "' + @backupFileName + '" with '
print ' data file "' + @databaseDataFile + '" located at "' + @databaseDataFilename + '"'
print ' log file "' + @databaseLogFile + '" located at "' + @databaseLogFilename + '"'

set @execSql = '
restore database [' + @databaseName + ']
from disk = ''' + @backupFileName + '''
with
file = 1,
move ''' + @databaseDataFile + ''' to ' + '''' + @databaseDataFilename + ''',
move ''' + @databaseLogFile + ''' to ' + '''' + @databaseLogFilename + ''',
norewind,
nounload,
replace'

exec sp_executesql @execSql

exec('use ' + @databaseName)
go

-- If needed, restore the database user associated with the database
/*
exec sp_revokedbaccess 'myDBUser'
go

exec sp_grantdbaccess 'myDBUser', 'myDBUser'
go

exec sp_addrolemember 'db_owner', 'myDBUser'
go

use master
go
*/
--======================RestoreDbFromFile.sql



The custom configuration are quite handy thing and often applications end up with a demand for an extendable solution.

For .NET 1.1 please refer the article http://aspnet.4guysfromrolla.com/articles/020707-1.aspx

Note: The above solution works for .NET 2.0 as well.

For .NET 2.0 specific solution, please refer the article http://aspnet.4guysfromrolla.com/articles/032807-1.aspx


putty + XMing - I had to set the DISPLAY environment variable manually to get things running (alongside with checking "Enable X11 forwarding" in putty - Connection/SSH/X11)

export DISPLAY=0:10.0

(it was set to "localhost:10.0", which did not work)


This looks promising. A way of killing the file handle....

http://www.timstall.com/2009/02/killing-file-handles-but-not-process.html


Using Orion Edwards advice I downloaded the SysInternals Process Explorer which in turn allowed me to discover that the file I was havind difficulties deleting was in fact being held not by the Excel.Applications object I thought but rather the fact that my C# code send mail code had created an Attachment object that left a handle to this file open.

Once I saw this, I quite simple called on the dispose method of the Attachment object, and the handle was released.

the Sys Internals explorer allowed me to discover this used in conjuction with the VS2005 debugger.

I highly recommend this tool!


Passing variable to function without single quote or double quote

&lt;html&gt;
&lt;head&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;script language="javascript"&gt;
function hello(id, bu)
{
    alert(id+ bu);
}
&lt;/script&gt;
&lt;a href ="javascript:
var x = &amp;#34;12&amp;#34;;
var y = &amp;#34;fmo&amp;#34;;
hello(x,y)"&gt;test&lt;/a&gt;
&lt;/body&gt;

&lt;/html&gt;



From a technology standpoint, Service Pack 1 does not add full support for ASP.NET AJAX. You still need use the workarounds described in the various articles mentioned in the previous answers.

Particulary, you need to make sure that the web.config file for your SharePoint Web application has been updated to support the appropriate version of the ASP.NET AJAX Extentions.

The fact that the web.config had not been updated was the mostly likely cause of the problem described in the original question.


from http://techtasks.com/code/viewbookcode/567

# This code sets the priority of a process

# ---------------------------------------------------------------
# Adapted from VBScript code contained in the book:
#      "Windows Server Cookbook" by Robbie Allen
# ISBN: 0-596-00633-0
# ---------------------------------------------------------------

use Win32::OLE;
$Win32::OLE::Warn = 3;

use constant NORMAL =&gt; 32;
use constant IDLE =&gt; 64;
use constant HIGH_PRIORITY =&gt; 128;
use constant REALTIME =&gt; 256;
use constant BELOW_NORMAL =&gt; 16384;
use constant ABOVE_NORMAL =&gt; 32768;

# ------ SCRIPT CONFIGURATION ------
$strComputer = '.';
$intPID = 2880; # set this to the PID of the target process
$intPriority = ABOVE_NORMAL; # Set this to one of the constants above
# ------ END CONFIGURATION ---------

print "Process PID: $intPID\n";

$objWMIProcess = Win32::OLE-&gt;GetObject('winmgmts:\\\\' . $strComputer . '\\root\\cimv2:Win32_Process.Handle=\'' . $intPID . '\'');

print 'Process name: ' . $objWMIProcess-&gt;Name, "\n";

$intRC = $objWMIProcess-&gt;SetPriority($intPriority);

if ($intRC == 0) {
    print "Successfully set priority.\n";
}
else {
    print 'Could not set priority. Error code: ' . $intRC, "\n";
}



Grid Hosting one price , an entire cloud hosting server
link text


Take a look at MVCContrib, you can do this:


    using MvcContrib.Filters;

    [ModelStateToTempData]
    public class MyController : Controller {
      //
    ...
    }




I have a similar problem, and it seems that there could be a problem with certificate itself.

Here's the path that worked for me:

makecert.exe -r -a sha1 -n CN=localhost -sky exchange -pe -b 01/01/2000 -e 01/01/2050 -ss my -sr localmachine


then look up certificate thumbprint, copy it to the clipboard and remove spaces. This will be a parameter after -h in the next command:

HttpCfg.exe set ssl -i 0.0.0.0:801 -h 35c65fd4853f49552471d2226e03dd10b7a11755


then run a service host on https://localhost:801/ and it works perfectly.

what I cannot make work is for https to run on self-generated certificate. Here's the code I run to generate one (error handling taken out for clarity):

LPCTSTR pszX500 = subject;
DWORD cbEncoded = 0;
CertStrToName(X509_ASN_ENCODING, pszX500, CERT_X500_NAME_STR, NULL, pbEncoded, &amp;cbEncoded, NULL);
pbEncoded = (BYTE *)malloc(cbEncoded);
CertStrToName(X509_ASN_ENCODING, pszX500, CERT_X500_NAME_STR, NULL, pbEncoded, &amp;cbEncoded, NULL);

// Prepare certificate Subject for self-signed certificate
CERT_NAME_BLOB SubjectIssuerBlob;
memset(&amp;SubjectIssuerBlob, 0, sizeof(SubjectIssuerBlob));
SubjectIssuerBlob.cbData = cbEncoded;
SubjectIssuerBlob.pbData = pbEncoded;

// Prepare key provider structure for self-signed certificate
CRYPT_KEY_PROV_INFO KeyProvInfo;
memset(&amp;KeyProvInfo, 0, sizeof(KeyProvInfo));
KeyProvInfo.pwszContainerName = _T("my-container");
KeyProvInfo.pwszProvName = NULL;
KeyProvInfo.dwProvType = PROV_RSA_FULL;
KeyProvInfo.dwFlags = CRYPT_MACHINE_KEYSET;
KeyProvInfo.cProvParam = 0;
KeyProvInfo.rgProvParam = NULL;
KeyProvInfo.dwKeySpec = AT_SIGNATURE;

// Prepare algorithm structure for self-signed certificate
CRYPT_ALGORITHM_IDENTIFIER SignatureAlgorithm;
memset(&amp;SignatureAlgorithm, 0, sizeof(SignatureAlgorithm));
SignatureAlgorithm.pszObjId = szOID_RSA_SHA1RSA;

// Prepare Expiration date for self-signed certificate
SYSTEMTIME EndTime;
GetSystemTime(&amp;EndTime);
EndTime.wYear += 5;

// Create self-signed certificate
pCertContext = CertCreateSelfSignCertificate(NULL, &amp;SubjectIssuerBlob, 0, &amp;KeyProvInfo, &amp;SignatureAlgorithm, 0, &amp;EndTime, 0);
hStore = CertOpenStore(CERT_STORE_PROV_SYSTEM, 0, 0, CERT_SYSTEM_STORE_LOCAL_MACHINE, L"MY");
CertAddCertificateContextToStore(hStore, pCertContext, CERT_STORE_ADD_REPLACE_EXISTING, 0);


Certificate shows fine and it has a working private key, but https will timeout as if thumbprint was never registered. If anyone knows why - plz comment

EDIT1: After some playing around, I have found the initialization for CertCreateSelfSignCertificate which generates proper certificate:

CRYPT_KEY_PROV_INFO KeyProvInfo;
memset(&amp;KeyProvInfo, 0, sizeof(KeyProvInfo));
KeyProvInfo.pwszContainerName = _T("my-container");
KeyProvInfo.pwszProvName = _T("Microsoft RSA SChannel Cryptographic Provider");
KeyProvInfo.dwProvType = PROV_RSA_SCHANNEL;
KeyProvInfo.dwFlags = CRYPT_MACHINE_KEYSET;
KeyProvInfo.cProvParam = 0;
KeyProvInfo.rgProvParam = NULL;
KeyProvInfo.dwKeySpec = AT_KEYEXCHANGE;



The accepted answer for this doesn't actually work for me...I had to jump through one more hoop to get it to work.

When I tried the answer

SELECT Row_Number() OVER(ORDER BY UserName) As RowID, UserFirstName, UserLastName
FROM Users
WHERE RowID Between 0 AND 9


it failed, complaining that it didn't know what RowID was.

I had to wrap it in an inner select like this:

SELECT * 
FROM
    (SELECT
    Row_Number() OVER(ORDER BY UserName) As RowID, UserFirstName, UserLastName
    FROM Users
    ) innerSelect
WHERE RowID Between 0 AND 9


and then it worked.


onclick='javascript:ToggleDisplay("&lt;%# DataBinder.Eval(Container.DataItem, "JobCode")%&gt; "); '


Use like above.


nBayes - another C# implementation hosted on CodePlex


also check out the project at codeplex which is a tutorial of sorts on parallelization using .net among other things:

http://paralleldwarfs.codeplex.com 



  Why doesn’t the percentage width child
  in absolutely positioned parent work
  in IE7?


Because it's Internet Exploder


  Is there something I'm missing here?


That is, to raise your co-worker's / clients' awareness that IE sucks.


  Is there an easy fix besides the pixel-based width on the child?


Use em units as they are more useful when creating liquid layouts as you can use them for padding and margins as well as font sizes. So your white space grows and shrinks proportionally to your text if it is resized (which is really what you need). I don't think percentages give a finer control than ems; there's nothing to stop you specifying in hundredths of ems (0.01 em) and the browser will interpret as it sees fit.  


  Is there an area of the CSS specification that covers this?


None, as far as I remember em's and %'s were intended for font sizes alone back at CSS 1.0.


my experience is after having sql sever 2005 and 2008 on same machine SSIS 2005 does not work properly...  specially with script task, data flow and sequence container


Red Gate SQL Doc is very good.  It can create a Word document or series of HTML pages that users can navigate.


Gallio now has an extension to output TeamCity service messages.
Just use the included Gallio.NAntTasks.dll and enable the TeamCity extension. (this won't be necessary in the next release)


here is the magic DataFormatString="{0:c0}

this will remove the decimal places


&lt;object width="660" height="525"&gt;&lt;param name="movie" value="http://www.youtube.com/v/WAQUskZuXhQ&amp;hl=en&amp;fs=1&amp;color1=0x006699&amp;color2=0x54abd6&amp;border=1"&gt;&lt;/param&gt;&lt;param name="allowFullScreen" value="true"&gt;&lt;/param&gt;&lt;param name="allowscriptaccess" value="always"&gt;&lt;/param&gt;&lt;embed src="http://www.youtube.com/v/WAQUskZuXhQ&amp;hl=en&amp;fs=1&amp;color1=0x006699&amp;color2=0x54abd6&amp;border=1" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" width="660" height="525"&gt;&lt;/embed&gt;&lt;/object&gt;



"Does the choice of ASP.NET as a platform influence this decision?"

Probably not.


Server Core 2008 R2 can run Sql Server, but this is unsupported (for now). Check http://www.nullsession.com/2009/06/02/sql-server-2008-on-server-core-2008-r2/ for an article + video on how it's done.


These are all very SQL Server 2000+ specific

Free / Open source

SQL XML Documentation 

DBdoc: automated CHM documentation for a SQL Server 2000 database 

SQLDoc Sharp  

Commercial

Apex SQL Doc

RedGate SQL Doc

Visio (Only for diagramming ERD's)

Some of these tools are GUI driven which prohibits their usage in continuous integration style development.  I hear good things abouts Red Gate and have used Apex SQL once before with success.  

The thing that bugs me is when it comes to interpreting comments etc in databases.  There appears to be no agreed standard for SQL documentation.   I'd love a standard like the XML comments for C#/VB code whereby running a process like sandcastle across your code/comments generates useful documentation.   Some of the commercial vendors have their own proprietary approach.


I agree with Chris. 
Virtual Machine Automation APIs is a very good possibility for automating of virtual machine operations.
VIX API Version 1.6.2 can be used for automating of ESX guest operations as well.


If you have access to Microsoft Expression Encoder 2, you can use that to encode a video file and generate a Silverlight video player. Then if you have IIS 7, you can use Adaptive or Smooth Streaming also checkout Smooth HD for a really cool example. 

You can also do streaming from the free Microsoft Silverlight Streaming Service. It's connected to a Windows Live account.

A consideration is that the client will need to have Silverlight installed, just like Flash, but Flash has been around longer.


Also you could use:

var amountDue = document.getElementById('&lt;%=YourControlName.ClientID%&gt;');


That will automatically resolve the client id for the element without you having to figure out that it's called 'ctl00_footerContentHolder_Fees1_FeeDue'.


Maybe this will help. http://www.codeproject.com/KB/printing/printhml.aspx
Also not sure what thread you are trying to access the browser control from, but it needs to be STA

Note - The project referred to in the link does allow you to navigate to a page and perform a print without showing the print dialog.


Use parentheses around your desired capture.


Your best bet is separating the model logic from presentation and thoroughly unit testing the model with NUnit or similar. Testing the users interaction with the web page can be fiddly. 

If you actually do want to unit test the users interaction with the web page some of the afformentioned tools such as waitn seem good, an addition to that which I've heard of is Selenium


Use https://github.com/dblock/vmwaretasks rather than the raw VixCOM API if you're going to do this in C#.


Maybe http://sglib.sourceforge.net/ if you want an easy to use, very fast, macro based library.


You could run just SQL 2008 as the single instance and then attach/create databases with compatability level of 2005?   The problem with that is that its a theory.  Im not 100% positive that if you create a database on 2008 , with a compatability level of 2005, and then detach it, that a SQL 2005 instance is capable of attaching it.

I think its a good enough chance to try though.   But I agree with the previous answers, the multiple instance options will work fine.


This is the first release of the FluorineFx Aperture framework.

The framework provides native OS integration (Windows only) support for AIR desktop applications.

The framework extends Adobe AIR applications in a non-intrusive way: simply redistribute the provided libraries with your AIR application, at runtime the framework will automatically hook into your application.

Features


Launch native applications and documents with the provided apsystem library
Take screenshots of the whole screen with the provided apimaging library
Access Outlook contacts from an Air application with the provided apoutlook library


http://aperture.fluorinefx.com/


How about writing little games? Grab yourself a RubyGame and start by making some simple games. Make a tetris, a snake, something really simple. It is a lot of fun, and you will learn a lot of little basic things about the language.


I’ve just spent several days procrastinating about exactly this question. There are third party products available and plenty of PERL and Python scripts but I wanted something simple and a language I was familiar with so ended up just writing hooks in a C# console app. It’s very straight forward:

public void Main(string[] args)
{
  string repositories = args[0];
  string transaction = args[1];

  var processStartInfo = new ProcessStartInfo
                           {
                             FileName = "svnlook.exe",
                             UseShellExecute = false,
                             CreateNoWindow = true,
                             RedirectStandardOutput = true,
                             RedirectStandardError = true,
                             Arguments = String.Format("log -t \"{0}\" \"{1}\"", transaction, repositories)
                           };

  var p = Process.Start(processStartInfo);
  var s = p.StandardOutput.ReadToEnd();
  p.WaitForExit();

  if (s == string.Empty)
  {
    Console.Error.WriteLine("Message must be provided");
    Environment.Exit(1);
  }

  Environment.Exit(0);
}


You can then invoke this on pre commit by adding a pre-commit.cmd file to the hooks folder of the repo with the following line:

[path]\PreCommit.exe %1 %2


You may consider this overkill but ultimately it’s only a few minutes of coding. What’s more, you get the advantage of the .NET language suite which IMHO is far preferable to the alternatives. I’ll expand my hooks out significantly and write appropriate tests against them as well – bit hard to do this with a DOS batch file!

BTW, the code has been adapted from this post.


I have try it with negativ result. The 2k8 installation breaks with a mysterious error-message. The installation-protocol looks fine, but it will not work. After this the 2k5 installation was buggy too.
The 2k8 installation was half-ready, so it´s already in controlpane / software, but uninstallation is not possible.

So my result - don´t do it on a productive server / workstation. If you need both versions, use a virtual machine instead.


The best way is definitely to pass it as an argument to the function called to start the child process. If you are spawning funs, which generally is a Good Thing to do, be careful of doing:

spawn_link(fun () -&gt; child(self()) end)


which will NOT do as you intended. (Hint: when is self() called)

Generally you should avoid registering a process, i.e. giving it a global name, unless you really want it to be globally known. Spawning a fun means that you don't have to export the spawned function as you should generally avoid exporting functions that aren't meant to be called from other modules.


Another drawback of binary format like PB is that if there is a single bit of error, the entire data file is not parsable, but with JSON or XML, as the last resort you can still manually fix the error because it is human readable and has redundancy built-in..


http://dhtmlx.com/dhxdocs/doku.php?id=dhtmlxgrid%3Aclipboard%5Foperations


If you are a Google Maps API Premier customer, then SSL is supported.  We use this and it works well.  

Prior to Google making SSL available, we proxyed all the traffic and this worked acceptably.  You lose the advantage of Google's CDN when you use this approach and you may get your IP banned since it will appear that you are generating a lot of traffic.


Some of the other solutions mentioned as answer do not work for the released version of MVC (they worked with previous versions of alpha/beta). 

Here is a good article describing a way to implement localization that will be strongly-typed and will not break the unit testing of controllers and views: localization guide for MVC v1 


I was looking to do something similar (www.mysite.com/SomeUser).

What I did was I edited 404.shtml to include this server side include (SSI) code:

&lt;!--#include virtual="404.php" -- &gt;


Then I created the file 404.php, where I parsed the URL to check for a user's name and showed their info from the database.


We are also handling cached, but secured, resources.  If you send / generate an ETAg header (which RFC 2616 section 13.3 recommends you SHOULD), then the client MUST use it in a conditional request (typically in an If-None-Match - HTTP_IF_NONE_MATCH - header).  If you send a Last-Modified header (again you SHOULD), then you should check the If-Modified-Since - HTTP_IF_MODIFIED_SINCE - header.  If you send both, then the client SHOULD send both, but it MUST send the ETag.  Also note that validtion is just defined as checking the conditional headers for strict equality against the ones you would send out.  Also, only a strong validator (such as an ETag) will be used for ranged requests (where only part of a resource is requested).

In practice, since the resources we are protecting are fairly static, and a one second lag time is acceptable, we are doing the following:


 Check to see if the user is authorized to access the requested resource

     If they are not, Redirect them or send a 4xx response as appropriate.  We will generate 404 responses to requests that look like hack attempts or blatant tries to perform a security end run.
 Compare the If-Modified-Since header to the Last-Modified header we would send (see below) for strict equality

     If they match, send a 304 Not Modified response and exit page processing
 Create a Last-Modified header using the modification time of the requested resource

    Look up the HTTP Date format in RFC 2616
 Send out the header and resource content along with an appropriate Content-Type


We decided to eschew the ETag header since it is overkill for our purposes.  I suppose we could also just use the date time stamp as an ETag.  If we move to a true ETag system, we would probably store computed hashes for the resources and use those as ETags.

If your resources are dynamically generated, from say database content, then ETags may be better for your needs, since they are just text to be populated as you see fit.


I had this same problem where immediately when you disconnect things are tremendously better.

If you are using windows VPN, you have to change a default setting.  It will force a connection to use the remote router as your gateway while connected.  If you go to properties for the connection, then to the networking tab.  Select TCP/IPv4 and go to properties.  In this window select Advanced... and there will be an option to use the default gateway on the remote network, make sure this is NOT checked.  This should help immensely. 


If the application is a general one, then the business logic layer can be used in complete other applications too. Like, I normally use my CMS related BLL classes in other applications.


In case lubos hasko's answer was not unsafe enough, there is also the really unsafe way, using
pointers in C#. Here's some tips and pitfalls I've run into:

using System;
using System.Runtime.InteropServices;
using System.IO;
using System.Diagnostics;

// Use LayoutKind.Sequential to prevent the CLR from reordering your fields.
[StructLayout(LayoutKind.Sequential)]
unsafe struct MeshDesc
{
    public byte NameLen;
    // Here fixed means store the array by value, like in C,
    // though C# exposes access to Name as a char*.
    // fixed also requires 'unsafe' on the struct definition.
    public fixed char Name[16];
    // You can include other structs like in C as well.
    public Matrix Transform;
    public uint VertexCount;
    // But not both, you can't store an array of structs.
    //public fixed Vector Vertices[512];
}

[StructLayout(LayoutKind.Sequential)]
unsafe struct Matrix
{
    public fixed float M[16];
}

// This is how you do unions
[StructLayout(LayoutKind.Explicit)]
unsafe struct Vector
{
    [FieldOffset(0)]
    public fixed float Items[16];
    [FieldOffset(0)]
    public float X;
    [FieldOffset(4)]
    public float Y;
    [FieldOffset(8)]
    public float Z;
}

class Program
{
    unsafe static void Main(string[] args)
    {
        var mesh = new MeshDesc();
        var buffer = new byte[Marshal.SizeOf(mesh)];

        // Set where NameLen will be read from.
        buffer[0] = 12;
        // Use Buffer.BlockCopy to raw copy data across arrays of primitives.
        // Note we copy to offset 2 here: char's have alignment of 2, so there is
        // a padding byte after NameLen: just like in C.
        Buffer.BlockCopy("Hello!".ToCharArray(), 0, buffer, 2, 12);

        // Copy data to struct
        Read(buffer, out mesh);

        // Print the Name we wrote above:
        var name = new char[mesh.NameLen];
        // Use Marsal.Copy to copy between arrays and pointers to arrays.
        unsafe { Marshal.Copy((IntPtr)mesh.Name, name, 0, mesh.NameLen); }
        // Note you can also use the String.String(char*) overloads
        Console.WriteLine("Name: " + new string(name));

        // If Erik Myers likes it...
        mesh.VertexCount = 4711;

        // Copy data from struct:
        // MeshDesc is a struct, and is on the stack, so it's
        // memory is effectively pinned by the stack pointer.
        // This means '&amp;' is sufficient to get a pointer.
        Write(&amp;mesh, buffer);

        // Watch for alignment again, and note you have endianess to worry about...
        int vc = buffer[100] | (buffer[101] &lt;&lt; 8) | (buffer[102] &lt;&lt; 16) | (buffer[103] &lt;&lt; 24);
        Console.WriteLine("VertexCount = " + vc);
    }

    unsafe static void Write(MeshDesc* pMesh, byte[] buffer)
    {
        // But byte[] is on the heap, and therefore needs
        // to be flagged as pinned so the GC won't try to move it
        // from under you - this can be done most efficiently with
        // 'fixed', but can also be done with GCHandleType.Pinned.
        fixed (byte* pBuffer = buffer)
            *(MeshDesc*)pBuffer = *pMesh;
    }

    unsafe static void Read(byte[] buffer, out MeshDesc mesh)
    {
        fixed (byte* pBuffer = buffer)
            mesh = *(MeshDesc*)pBuffer;
    }
}



user profile is a nice clean framework for individual customization(AKA. Profile Properties). (e.g. iGoogle)
the problem of it is its not designed for query and not ideal for data sharing to public user.(you still would be able to do it, with low performance)

so, if you want to enhance the customized user experience, user profile would be a good way to go. otherwise, use your own class and table would be a much better solution.


One thing to remember is that the Matlab compiler does not actually compile the Matlab code into native machine instructions.  It simply wraps it into a standalone executable or a library with its own runtime engine that runs it.  You would be able to run your code without Matlab installed, and you would be able to interface it with other languages, but it will still be interpreted Matlab code, so there would be no speedup.


If hash tables, extensible strings and dynamic vector are enough for your needs, please have a look at the library I put toghether: http://code.google.com/p/c-libutl/.

I also would welcome any feedback!


Add the IIS user in the 'dump' folders security persmissions group, and give it read/write access.


I think it is better off using it for supplementary data that is not critical to the user that is only normally important when that user is logging in anyway. Think data that would not break anything important if it was all wiped.

of course thats personal preference but others have raised some other important issues.

Also very useful considering it can be used for an unauthenticated user whose profile is maintained with an anonymous cookie.


Pass an explicit pointer to the first element with the array dimensions as separate parameters.  For example, to handle arbitrarily sized 2-d arrays of int:

void func_2d(int *p, size_t M, size_t N)
{
  size_t i, j;
  ...
  p[i*N+j] = ...;
}


which would be called as

...
int arr1[10][20];
int arr2[5][80];
...
func_2d(&amp;arr1[0][0], 10, 20);
func_2d(&amp;arr2[0][0], 5, 80);


Same principle applies for higher-dimension arrays:

func_3d(int *p, size_t X, size_t Y, size_t Z)
{
  size_t i, j, k;
  ...
  p[i*Y*Z+j*Z+k] = ...;
  ...
}
...
arr2[10][20][30];
...
func_3d(&amp;arr[0][0][0], 10, 20, 30);



SetWindowPos + SWP_NOACTIVATE does the job.


Here's an explanation of MANOVA ouptput, from a very good site on statistics and on SPSS:

Output with explanation:
http://faculty.chass.ncsu.edu/garson/PA765/manospss.htm

How and why to do MANOVA or multivariate GLM:
(same path as above, but terminating in '/manova.htm')

Writing software from scratch to calculate these outputs would be both lengthy and difficult;
there's lots of numerical problems and matrix inversions to do.

As Henry said, use Python scripts, or R.  I'd suggest working with somebody who knows SPSS if scripting.
In addition, SPSS itself is capable of exporting the output tables to files using something called OMS.
A script within SPSS can do this.

Find out who in your research group knows SPSS and work with them.


The MIT licensed C#-SQLite might be the right solution. It's a complete managed port of SQLite, so it can be used with Silverlight.


If you have a php executable with a help of simple php class you may write hook script in php like it is shown here http://www.devhands.com/2010/01/subversion-hook-php-framework-in/


I started with this example from CodeProject

Then instead of adding to the page, I borrowed from saalon (above) and did a Response.Write().


TeamCity Sidebar Gadget for Windows Vista, Windows 7 
http://teamcity-gadget.com


In IE8 just press F12!


The warning is issued since you are using backslashes in your strings. If you want to avoid the message, type this command "set standard_conforming_strings=on;". Then use "E" before your string including backslashes that you want postgresql to intrepret.


In Chrome on the Mac, alt-tab inserts a tab character into a &lt;textarea&gt; field.

Here’s one: . Wee!


Changing the datatype to varbinary seems to work the best for me.


I use NDoc3


If you want an auto incrementing number that updates each time a compilation is done, you can use VersionUpdater from a pre-build event. Your pre-build event can check the build configuration if you prefer so that the version number will only increment for a Release build (for example).


If you've got it in the budget (~$3000), check out PrinceXML.

It will render HTML into a PDF, functions well in a service environment, and supports advanced features such as not breaking a page in the middle of a table cell (which a lot of browsers don't currently support). 


As Ronnie said, I'd use BinaryReader and read each field individually.  I can't find the link to the article with this info, but it's been observed that using BinaryReader to read each individual field can be faster than Marshal.PtrToStruct, if the struct contains less than 30-40 or so fields.  I'll post the link to the article when I find it.

The article's link is at: http://www.codeproject.com/Articles/10750/Fast-Binary-File-Reading-with-C

When marshaling an array of structs, PtrToStruct gains the upper-hand more quickly, because you can think of the field count as fields * array length.


Good starter:
Getting Started with Tdd in Java using Eclipse by Brett L. Schuchert

Is a set of screencasts about TDD in Java and in C#. It is starting from the scratch and teaching basics of TDD. 


